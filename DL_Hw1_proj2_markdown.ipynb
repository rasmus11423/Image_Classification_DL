{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc1a222",
   "metadata": {},
   "source": [
    "# Project 2 – Towards Foundation Models for Tabular Data\n",
    "\n",
    "We:\n",
    "1) Preprocess them **as in the papers** (cats vs. nums, standardization).\n",
    "2) Train strong **tree baselines** (CatBoost, LightGBM, XGBoost).\n",
    "3) Train **TabPFN** (classification) and **TabNet** (classification & regression).\n",
    "4) Build an **ensemble** on one dataset (default: `credit-g`).\n",
    "5) **Fine-tune the same model on another dataset** (we include a **TransTab transfer** example: pretrain on `mfeat-fourier` → fine-tune on `credit-g`), which supports differing schemas across tables.\n",
    "6) **Perform transfer-learning** on two different pairs selected from TabPFN paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87aab877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DEVICE = cuda\n"
     ]
    }
   ],
   "source": [
    "import os, warnings, logging, torch\n",
    "\n",
    "# Prefer GPU when available\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Silence common warning/progress bars (\"warning bars\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"lightgbm\").setLevel(logging.ERROR)\n",
    "\n",
    "def to_device(x):\n",
    "    try:\n",
    "        import torch\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return x.to(DEVICE, non_blocking=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return x\n",
    "\n",
    "print(f\"Using DEVICE = {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f7d364b-30c0-47e9-bee9-ee565be24491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Versions:\n",
      "numpy 2.2.6\n",
      "pandas 2.3.3\n",
      "sklearn 1.6.1\n",
      "openml 0.15.1\n",
      "xgboost 3.0.5\n",
      "lightgbm 4.6.0\n",
      "catboost 1.2.8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Environment setup (run once) ---\n",
    "%pip -q install --upgrade pip\n",
    "%pip -q install scikit-learn pandas numpy matplotlib openml\n",
    "%pip -q install xgboost lightgbm catboost\n",
    "# Deep models\n",
    "%pip -q install pytorch-tabnet\n",
    "%pip -q install tabpfn  # classification\n",
    "%pip -q install transtab  \n",
    "\n",
    "import numpy as np, pandas as pd, sklearn, openml  \n",
    "print('Versions:')\n",
    "import importlib, sys\n",
    "for m in ['numpy','pandas','sklearn','openml','xgboost','lightgbm','catboost']:\n",
    "    try:\n",
    "        print(m, importlib.import_module(m).__version__)\n",
    "    except Exception as e:\n",
    "        print(m, 'not found:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317a2444-7a0f-4d1c-84e6-f568f4a90f76",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf859271",
   "metadata": {
    "id": "cf859271"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Utilities: seed, metrics, helpers ---\n",
    "import numpy as np, pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import logging\n",
    "\n",
    "logging.getLogger('lightgbm').setLevel(logging.ERROR)  \n",
    "from pathlib import Path\n",
    "RESULTS_DIR = Path(\"./results\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    import random, os, numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, accuracy_score, mean_absolute_error, mean_squared_error\n",
    ")\n",
    "\n",
    "def cls_metrics(y_true, y_proba_or_pred, average='macro', is_multiclass=False):\n",
    "    \"\"\"\n",
    "    If probabilities are provided, compute ROC-AUC (OVR if multiclass).\n",
    "    Always compute macro-F1 from predicted labels.\n",
    "    \"\"\"\n",
    "    import numpy as _np\n",
    "    # decide if proba or labels\n",
    "    if isinstance(y_proba_or_pred, _np.ndarray) and y_proba_or_pred.ndim >= 2:\n",
    "        # proba given\n",
    "        y_pred = y_proba_or_pred.argmax(1)\n",
    "        try:\n",
    "            if is_multiclass:\n",
    "                auc = roc_auc_score(y_true, y_proba_or_pred, multi_class='ovr')\n",
    "            else:\n",
    "                # assume y_true is {0,1}\n",
    "                auc = roc_auc_score(y_true, y_proba_or_pred[:,1])\n",
    "        except Exception:\n",
    "            auc = _np.nan\n",
    "    else:\n",
    "        # labels\n",
    "        y_pred = y_proba_or_pred\n",
    "        auc = _np.nan\n",
    "    f1 = f1_score(y_true, y_pred, average=average)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return dict(roc_auc=auc, macro_f1=f1, acc=acc)\n",
    "\n",
    "def reg_metrics(y_true, y_pred):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    try:\n",
    "        rmse = mean_squared_error(y_true, y_pred, squared=False)  # newer sklearn\n",
    "    except TypeError:\n",
    "        rmse = mean_squared_error(y_true, y_pred) ** 0.5          # older sklearn\n",
    "    return dict(MAE=float(mae), RMSE=float(rmse))\n",
    "\n",
    "\n",
    "def cat_indices_from_df(df: pd.DataFrame, target: str) -> List[int]:\n",
    "    \"\"\"Return categorical column indices based on object/category dtype (excluding target).\"\"\"\n",
    "    cats = [i for i, c in enumerate(df.columns) if c != target and (df[c].dtype == 'object' or str(df[c].dtype).startswith('category'))]\n",
    "    return cats\n",
    "\n",
    "def train_val_test_split_indices(n, stratify=None, test_size=0.2, val_size=0.2, seed=42):\n",
    "    \"\"\"Return train/val/test indices, with optional stratification (labels array).\"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import numpy as _np\n",
    "    idx = _np.arange(n)\n",
    "    idx_train, idx_test = train_test_split(idx, test_size=test_size, random_state=seed, stratify=stratify)\n",
    "    strat2 = stratify[idx_train] if stratify is not None else None\n",
    "    val_rel = val_size / (1.0 - test_size)\n",
    "    idx_train, idx_val = train_test_split(idx_train, test_size=val_rel, random_state=seed, stratify=strat2)\n",
    "    return idx_train, idx_val, idx_test\n",
    "\n",
    "def save_csv(df, filename):\n",
    "    path = RESULTS_DIR / filename\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"Saved CSV -> {path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3014b08-654c-4554-81d5-826559f364df",
   "metadata": {},
   "source": [
    "\n",
    "# Datasets\n",
    "\n",
    "This notebook has been **modified** to use exactly **three datasets**:\n",
    "- **mfeat-fourier** (OpenML ID **14**) — **multi-class** (10 classes) classification — from the TabPFN paper's CC18 suite\n",
    "- **credit-g (German Credit)** (OpenML ID **31**) — **binary** classification — from the TabPFN paper's CC18 suite\n",
    "- **SARCOS** — **regression** (21 features, 7 targets y1..y7) - from TabNet paper\n",
    "\n",
    "> **Notes**    \n",
    "> • GPU is recommended for TabNet/TransTab; TabPFN runs on CPU reasonably fast for ≤10k rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba5acaa",
   "metadata": {
    "id": "2ba5acaa",
    "outputId": "0a94ce56-cf7a-4cda-e385-dc917b5c8536"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded datasets:\n",
      "mfeat-fourier: (2000, 76) classes: 10\n",
      "credit-g    : (1000, 20) (cats=13, nums=7)\n",
      "sarcos(y1)  : (48933, 27)\n"
     ]
    }
   ],
   "source": [
    "# --- Dataset loaders (OpenML) + SARCOS (GPML) ---\n",
    "from typing import Optional\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from pathlib import Path\n",
    "import openml\n",
    "import io, urllib.request\n",
    "\n",
    "def load_openml_dataset(dataset_id: int, target: Optional[str]=None, as_frame=True):\n",
    "    d = openml.datasets.get_dataset(dataset_id, download_all_files=True)\n",
    "    if target is None:\n",
    "        target = d.default_target_attribute\n",
    "    X, y, categorical_indicator, attribute_names = d.get_data(\n",
    "        dataset_format=\"dataframe\", target=target\n",
    "    )\n",
    "    df = pd.concat([X, y.rename(target)], axis=1)\n",
    "    return df, target, categorical_indicator, attribute_names\n",
    "\n",
    "def load_sarcos_single(target: str = \"y1\", sample: Optional[int] = None, seed: int = 42):\n",
    "    \"\"\"\n",
    "    SARCOS inverse dynamics (regression, 21 features, 7 targets y1..y7).\n",
    "    Loads train+test from GPML, concatenates, and returns (df, target).\n",
    "    Set `target` to one of 'y1'..'y7'. Optionally subsample with `sample`.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from scipy.io import loadmat\n",
    "    except Exception:\n",
    "        from scipy.io import loadmat  # will raise if not installed\n",
    "\n",
    "    def _fetch_mat(url: str):\n",
    "        with urllib.request.urlopen(url) as r:\n",
    "            return loadmat(io.BytesIO(r.read()))\n",
    "\n",
    "    tr = _fetch_mat(\"http://www.gaussianprocess.org/gpml/data/sarcos_inv.mat\")\n",
    "    te = _fetch_mat(\"http://www.gaussianprocess.org/gpml/data/sarcos_inv_test.mat\")\n",
    "\n",
    "    Xtr, Ytr = tr[\"sarcos_inv\"][:, :21], tr[\"sarcos_inv\"][:, 21:]\n",
    "    Xte, Yte = te[\"sarcos_inv_test\"][:, :21], te[\"sarcos_inv_test\"][:, 21:]\n",
    "    X = np.vstack([Xtr, Xte]); Y = np.vstack([Ytr, Yte])\n",
    "\n",
    "    cols_X = [f\"x{i}\" for i in range(21)]\n",
    "    cols_Y = [f\"y{i+1}\" for i in range(7)]\n",
    "    df = pd.DataFrame(np.hstack([X, Y]), columns=cols_X + cols_Y)\n",
    "\n",
    "    assert target in df.columns, f\"target must be one of {cols_Y}\"\n",
    "    if sample is not None and len(df) > sample:\n",
    "        df = df.sample(n=sample, random_state=seed).reset_index(drop=True)\n",
    "    return df.astype(np.float32), target\n",
    "\n",
    "# 1) mfeat-fourier (ID 14) — multi-class, numeric\n",
    "mfeat_fourier_df, mfeat_fourier_y, _, _ = load_openml_dataset(14)  # target = 'class'\n",
    "mf_X = mfeat_fourier_df.drop(columns=[mfeat_fourier_y]).astype(float)\n",
    "mf_y = mfeat_fourier_df[mfeat_fourier_y].astype('category').cat.codes.values  # 0..9\n",
    "\n",
    "# 2) credit-g (ID 31) — binary, cat-heavy\n",
    "creditg_df, creditg_y, _, _ = load_openml_dataset(31)\n",
    "cg_y = creditg_df[creditg_y].astype('category').cat.codes.values\n",
    "cg_X = creditg_df.drop(columns=[creditg_y])\n",
    "cg_cat_cols = [c for c in cg_X.columns if cg_X[c].dtype == 'object' or str(cg_X[c].dtype).startswith('category')]\n",
    "cg_num_cols = [c for c in cg_X.columns if c not in cg_cat_cols]\n",
    "\n",
    "# 3) SARCOS — regression; choose one torque target (y1..y7). No time-aware CV needed.\n",
    "sarcos_df, sarcos_target = load_sarcos_single(target=\"y1\", sample=None, seed=42)\n",
    "bh_y = sarcos_df[sarcos_target].values.astype(np.float32)         # keep names used later\n",
    "bh_X = sarcos_df.drop(columns=[sarcos_target]).astype(np.float32) # all numeric\n",
    "\n",
    "print('Loaded datasets:')\n",
    "print('mfeat-fourier:', mf_X.shape, 'classes:', len(np.unique(mf_y)))\n",
    "print('credit-g    :', cg_X.shape, f'(cats={len(cg_cat_cols)}, nums={len(cg_num_cols)})')\n",
    "print('sarcos(y1)  :', bh_X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a10a68-ecd7-4a6a-8d2c-0e15a7ad9f89",
   "metadata": {},
   "source": [
    "## Classical ML Baselines (XGBoost / LightGBM / CatBoost)\n",
    "\n",
    "**Goal.** Strong tree-based baselines across all three datasets with correct categorical handling.\n",
    "\n",
    "**Configuration**\n",
    "\n",
    "- **XGBoost**: tree-based booster (e.g., `gbtree`), early stopping via validation AUC/RMSE, tune `max_depth`, `eta`, `subsample`, `colsample_bytree`.\n",
    "- **LightGBM**: use `categorical_feature` for credit-g; gradient-based one-side sampling is fine; tune `num_leaves`, `feature_fraction`, `bagging_fraction`, `min_data_in_leaf`.\n",
    "- **CatBoost**: pass raw categoricals for credit-g; automatic target statistics; control `depth`, `learning_rate`, `l2_leaf_reg`.\n",
    "\n",
    "**Reporting**\n",
    "\n",
    "- **mfeat-fourier**: macro-F1, OvR/AUC (optionally accuracy).  \n",
    "- **credit-g**: ROC-AUC (primary), macro-F1.  \n",
    "- **SARCOS**: RMSE/MAE (per target or y1 primary).\n",
    "\n",
    "> Ensure consistent CV (e.g., 5-fold stratified for classification) and log the mean ± std for each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a233993",
   "metadata": {
    "id": "3a233993",
    "outputId": "85093e35-515d-4b50-989c-b5ef0fc78248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baselines ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Classical ML baselines: XGBoost, LightGBM, CatBoost ---\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\"X does not have valid feature names, but .* was fitted with feature names\",\n",
    "    category=UserWarning,\n",
    "    module=r\"sklearn\\.utils\\.validation\",\n",
    ")\n",
    "\n",
    "# Helper to build pipelines\n",
    "def build_cls_pipeline_xgb(cat_cols: List[str], num_cols: List[str], multiclass=False):\n",
    "    from xgboost import XGBClassifier\n",
    "    ohe = ColumnTransformer([('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)],\n",
    "                            remainder='passthrough')\n",
    "    obj = 'multi:softprob' if multiclass else 'binary:logistic'\n",
    "    clf = XGBClassifier(\n",
    "        objective=obj, n_estimators=600, max_depth=6, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8, tree_method='hist', eval_metric='logloss', n_jobs=-1\n",
    "    )\n",
    "    return Pipeline([('prep', ohe), ('clf', clf)])\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def build_cls_pipeline_lgbm(cat_cols, num_cols, multiclass=False):\n",
    "    objective = 'multiclass' if multiclass else 'binary'\n",
    "\n",
    "    if len(cat_cols) == 0:\n",
    "        clf = LGBMClassifier(\n",
    "            objective=objective,\n",
    "            n_estimators=1200,\n",
    "            learning_rate=0.03,\n",
    "            num_leaves=255,\n",
    "            min_data_in_leaf=5,\n",
    "            min_gain_to_split=0.0,\n",
    "            feature_fraction=1.0,\n",
    "            bagging_fraction=1.0,\n",
    "            max_depth=-1,\n",
    "            force_col_wise=True,\n",
    "            n_jobs=-1,\n",
    "            verbosity=-1,      # <- silence LightGBM logs\n",
    "        )\n",
    "        return Pipeline([('clf', clf)])\n",
    "\n",
    "    import pandas as pd\n",
    "    class LGBMNativeCats:\n",
    "        def __init__(self):\n",
    "            self.cat_cols = list(cat_cols)\n",
    "            self.num_cols = list(num_cols)\n",
    "            self.columns = self.cat_cols + self.num_cols\n",
    "            self.model = LGBMClassifier(\n",
    "                objective=objective,\n",
    "                n_estimators=1200,\n",
    "                learning_rate=0.03,\n",
    "                num_leaves=255,\n",
    "                min_data_in_leaf=5,\n",
    "                min_gain_to_split=0.0,\n",
    "                feature_fraction=1.0,\n",
    "                bagging_fraction=1.0,\n",
    "                max_depth=-1,\n",
    "                force_col_wise=True,\n",
    "                n_jobs=-1,\n",
    "                verbosity=-1,    # <- silence LightGBM logs\n",
    "            )\n",
    "        def _prep(self, X):\n",
    "            Xdf = X[self.columns].copy()\n",
    "            for c in self.cat_cols:\n",
    "                Xdf[c] = Xdf[c].astype('category')\n",
    "            return Xdf\n",
    "        def fit(self, X, y):\n",
    "            Xdf = self._prep(X)\n",
    "            cat_idx = [Xdf.columns.get_loc(c) for c in self.cat_cols]\n",
    "            self.model.fit(Xdf, y, categorical_feature=cat_idx)\n",
    "            return self\n",
    "        def predict_proba(self, X):\n",
    "            Xdf = self._prep(X)\n",
    "            return self.model.predict_proba(Xdf)\n",
    "        def predict(self, X):\n",
    "            Xdf = self._prep(X)\n",
    "            return self.model.predict(Xdf)\n",
    "    return LGBMNativeCats()\n",
    "\n",
    "def build_cls_pipeline_catboost(cat_cols: List[str], num_cols: List[str], multiclass=False):\n",
    "    from catboost import CatBoostClassifier\n",
    "    # CatBoost can ingest raw categoricals; we pass indices at fit time\n",
    "    class CatBoostWrapper:\n",
    "        def __init__(self):\n",
    "            self.cat_cols = cat_cols\n",
    "            self.num_cols = num_cols\n",
    "            self.columns = cat_cols + num_cols\n",
    "            self.model = CatBoostClassifier(\n",
    "                loss_function='MultiClass' if multiclass else 'Logloss',\n",
    "                eval_metric='MultiClass' if multiclass else 'AUC',\n",
    "                depth=8, learning_rate=0.05, iterations=1500, verbose=False\n",
    "            )\n",
    "        def fit(self, X, y):\n",
    "            Xdf = X[self.columns].copy()\n",
    "            cat_idx = [Xdf.columns.get_loc(c) for c in self.cat_cols]\n",
    "            self.model.fit(Xdf, y, cat_features=cat_idx)\n",
    "            return self\n",
    "        def predict_proba(self, X):\n",
    "            Xdf = X[self.columns].copy()\n",
    "            return self.model.predict_proba(Xdf)\n",
    "        def predict(self, X):\n",
    "            Xdf = X[self.columns].copy()\n",
    "            return self.model.predict(Xdf).astype(int)\n",
    "    return CatBoostWrapper()\n",
    "\n",
    "def build_reg_pipeline_xgb():\n",
    "    from xgboost import XGBRegressor\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    reg = XGBRegressor(\n",
    "        n_estimators=1200, learning_rate=0.03, max_depth=6,\n",
    "        subsample=0.8, colsample_bytree=0.8, tree_method='hist',\n",
    "        objective='reg:squarederror', n_jobs=-1\n",
    "    )\n",
    "    return Pipeline([('scaler', scaler), ('reg', reg)])\n",
    "\n",
    "def build_reg_pipeline_lgbm():\n",
    "    from lightgbm import LGBMRegressor\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    reg = LGBMRegressor(\n",
    "        n_estimators=1500, learning_rate=0.03, num_leaves=63,\n",
    "        subsample=0.8, colsample_bytree=0.8, n_jobs=-1\n",
    "    )\n",
    "    return Pipeline([('scaler', scaler), ('reg', reg)])\n",
    "\n",
    "def build_reg_pipeline_catboost():\n",
    "    from catboost import CatBoostRegressor\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    reg = CatBoostRegressor(\n",
    "        depth=8, learning_rate=0.05, iterations=2000, verbose=False,\n",
    "        loss_function='RMSE'\n",
    "    )\n",
    "    return Pipeline([('scaler', scaler), ('reg', reg)])\n",
    "\n",
    "print(\"Baselines ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5424a666",
   "metadata": {},
   "source": [
    "## Tabular Deep Learning (DL)\n",
    "\n",
    "We evaluate **TabPFN** and **TabNet** on all datasets. Training prefers **GPU** when available.\n",
    "\n",
    "### TabPFN\n",
    "- Treat as a strong out-of-the-box DL baseline for classification.  \n",
    "- Prepare inputs: numeric standardized, categoricals ordinal-encoded (embeddings on the DL side).  \n",
    "- Metrics and splits mirror baselines for comparability.\n",
    "\n",
    "### TabNet\n",
    "- Use `TabNetClassifier` / `TabNetRegressor` with `device_name=\"cuda\"` when available.  \n",
    "- Typical hyperparams: `n_d`, `n_a`, `n_steps`, `gamma`, `lambda_sparse`, early stopping on validation metric.  \n",
    "- For SARCOS, use multi-output regression or train per-target models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c1a4cdd",
   "metadata": {
    "id": "8c1a4cdd",
    "outputId": "1b2d4772-ca38-4d44-c741-391e39aec177"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFN block ready.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ----- helpers: robust constructors (GPU + ignore caps) -----\n",
    "def _make_tabpfn():\n",
    "    from tabpfn import TabPFNClassifier\n",
    "    # try common arg names across versions\n",
    "    ensemble_keys = (\"N_ensemble_configurations\", \"N_ensembles\", \"n_ensembles\")\n",
    "    base_kwargs_list = [\n",
    "        # some versions accept 'devices', others 'device'\n",
    "        dict(ignore_pretraining_limits=True, devices=[\"cuda\"]),\n",
    "        dict(ignore_pretraining_limits=True, device=\"cuda\"),\n",
    "        dict(ignore_pretraining_limits=True),  # fallback, will auto-pick device\n",
    "    ]\n",
    "    last_err = None\n",
    "    for ens_key in ensemble_keys:\n",
    "        for base_kwargs in base_kwargs_list:\n",
    "            try:\n",
    "                return TabPFNClassifier(**{ens_key: 32, **base_kwargs})\n",
    "            except TypeError as e:\n",
    "                last_err = e\n",
    "                continue\n",
    "    # final fallback: no ensemble kw, minimal args\n",
    "    try:\n",
    "        return TabPFNClassifier(ignore_pretraining_limits=True)\n",
    "    except TypeError:\n",
    "        # absolute minimal fallback\n",
    "        return TabPFNClassifier()\n",
    "\n",
    "def _make_tabpfn_reg():\n",
    "    from tabpfn import TabPFNRegressor\n",
    "    ensemble_keys = (\"N_ensemble_configurations\", \"N_ensembles\", \"n_ensembles\")\n",
    "    base_kwargs_list = [\n",
    "        dict(ignore_pretraining_limits=True, devices=[\"cuda\"]),\n",
    "        dict(ignore_pretraining_limits=True, device=\"cuda\"),\n",
    "        dict(ignore_pretraining_limits=True),\n",
    "    ]\n",
    "    for ens_key in ensemble_keys:\n",
    "        for base_kwargs in base_kwargs_list:\n",
    "            try:\n",
    "                return TabPFNRegressor(**{ens_key: 16, **base_kwargs})\n",
    "            except TypeError:\n",
    "                continue\n",
    "    try:\n",
    "        return TabPFNRegressor(ignore_pretraining_limits=True)\n",
    "    except TypeError:\n",
    "        return TabPFNRegressor()\n",
    "\n",
    "# ----- optional (recommended): cap only the TRAIN fold size to 10k, keep full validation -----\n",
    "def _cap_train_fold(tr_idx, y, max_train=10_000, random_state=42):\n",
    "    if tr_idx.size <= max_train:\n",
    "        return tr_idx\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    # stratified subsample for classification, uniform for regression (y can be float)\n",
    "    if np.issubdtype(y.dtype, np.integer) and y.ndim == 1:\n",
    "        # stratified: sample per class proportionally\n",
    "        capped = []\n",
    "        _, counts = np.unique(y[tr_idx], return_counts=True)\n",
    "        target_per_class = np.floor(counts / counts.sum() * max_train).astype(int)\n",
    "        # ensure at least 1 per present class\n",
    "        target_per_class[target_per_class == 0] = 1\n",
    "        # adjust rounding drift\n",
    "        diff = max_train - target_per_class.sum()\n",
    "        # add remainder to the largest classes\n",
    "        if diff > 0:\n",
    "            order = np.argsort(-counts)\n",
    "            for k in order[:diff]:\n",
    "                target_per_class[k] += 1\n",
    "        classes = np.unique(y[tr_idx])\n",
    "        for cls, take in zip(classes, target_per_class):\n",
    "            cls_idx = tr_idx[y[tr_idx] == cls]\n",
    "            sel = rng.choice(cls_idx, size=min(take, cls_idx.size), replace=False)\n",
    "            capped.append(sel)\n",
    "        return np.concatenate(capped)\n",
    "    else:\n",
    "        # regression: uniform sample\n",
    "        return rng.choice(tr_idx, size=max_train, replace=False)\n",
    "\n",
    "# ----- your CV loops (patched) -----\n",
    "def run_tabpfn_cv(X, y, n_splits=5, is_multiclass=False, random_state=42, cap_train_to_10k=True):\n",
    "    \"\"\"Stratified CV for classification with GPU + cap handling.\"\"\"\n",
    "    if not TABPFN_OK:\n",
    "        return None\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # ensure contiguous labels 0..C-1 and float32 features\n",
    "    y = np.asarray(y)\n",
    "    # map labels to 0..C-1 for safety\n",
    "    _, y_mapped = np.unique(y, return_inverse=True)\n",
    "    y = y_mapped.astype(int)\n",
    "    Xnp = X.to_numpy(dtype=np.float32, copy=False) if hasattr(X, \"to_numpy\") else np.asarray(X, dtype=np.float32)\n",
    "\n",
    "    metrics = []\n",
    "    for tr, te in skf.split(Xnp, y):\n",
    "        if cap_train_to_10k:\n",
    "            tr_eff = _cap_train_fold(tr, y, max_train=10_000, random_state=random_state)\n",
    "        else:\n",
    "            tr_eff = tr\n",
    "        clf = _make_tabpfn()\n",
    "        clf.fit(Xnp[tr_eff], y[tr_eff])\n",
    "        proba = clf.predict_proba(Xnp[te])\n",
    "        m = cls_metrics(y[te], proba, is_multiclass=is_multiclass)\n",
    "        metrics.append(m)\n",
    "\n",
    "    out = {k: float(np.nanmean([m[k] for m in metrics])) for k in metrics[0]}\n",
    "    out_std = {k + \"_std\": float(np.nanstd([m[k] for m in metrics])) for k in metrics[0]}\n",
    "    out.update(out_std)\n",
    "    return out\n",
    "\n",
    "def run_tabpfn_reg_cv(X, y, n_splits=5, random_state=42, cap_train_to_10k=True):\n",
    "    \"\"\"KFold CV for regression with GPU + cap handling.\"\"\"\n",
    "    if not TABPFN_REG_OK:\n",
    "        return None\n",
    "    from sklearn.model_selection import KFold\n",
    "    Xnp = X.to_numpy(dtype=np.float32, copy=False) if hasattr(X, \"to_numpy\") else np.asarray(X, dtype=np.float32)\n",
    "    ynp = np.asarray(y, dtype=np.float32).ravel()\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    mets = []\n",
    "    for tr, te in kf.split(Xnp):\n",
    "        if cap_train_to_10k:\n",
    "            tr_eff = _cap_train_fold(tr, ynp, max_train=10_000, random_state=random_state)\n",
    "        else:\n",
    "            tr_eff = tr\n",
    "        reg = _make_tabpfn_reg()\n",
    "        reg.fit(Xnp[tr_eff], ynp[tr_eff])\n",
    "        yhat = np.asarray(reg.predict(Xnp[te]), dtype=np.float32).ravel()\n",
    "        mets.append(reg_metrics(ynp[te], yhat))\n",
    "\n",
    "    out = {k: float(np.mean([m[k] for m in mets])) for k in mets[0]}\n",
    "    out_std = {k + \"_std\": float(np.std([m[k] for m in mets])) for k in mets[0]}\n",
    "    out.update(out_std)\n",
    "    return out\n",
    "    \n",
    "print(\"TabPFN block ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a87285a",
   "metadata": {
    "id": "3a87285a",
    "outputId": "a3af6402-2ca6-40fd-9d23-345f3eb7bd6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet block ready.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from tqdm.auto import tqdm  \n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"Best weights from best epoch are automatically used!\",\n",
    "    module=r\"pytorch_tabnet\\.callbacks\"\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=UserWarning,\n",
    "    module=r\"pytorch_tabnet\\.callbacks\"\n",
    ")\n",
    "\n",
    "mf_scaler = StandardScaler()\n",
    "mf_X_std = mf_scaler.fit_transform(mf_X.values)\n",
    "\n",
    "# --- TabNet: classification & regression ---\n",
    "try:\n",
    "    from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "    import torch\n",
    "    TABNET_OK = True\n",
    "except Exception as e:\n",
    "    TABNET_OK = False\n",
    "    print(\"TabNet not available:\", e)\n",
    "\n",
    "def _tabnet_cv_cls(X, y, n_splits=5, is_multiclass=False, seed=42):\n",
    "    if not TABNET_OK: return None\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    metrics = []\n",
    "    for tr, te in skf.split(X, y):\n",
    "        model = TabNetClassifier(\n",
    "            n_d=32, n_a=32, n_steps=5, gamma=1.3,\n",
    "            lambda_sparse=1e-4, optimizer_params=dict(lr=2e-2),\n",
    "            verbose=0\n",
    "        )\n",
    "        metric = ['auc'] if not is_multiclass else ['accuracy']\n",
    "        model.fit(\n",
    "            X_train=X[tr], y_train=y[tr],\n",
    "            eval_set=[(X[te], y[te])], eval_name=['val'],\n",
    "            eval_metric=metric,\n",
    "            max_epochs=150, patience=20, batch_size=1024, virtual_batch_size=128\n",
    "        )\n",
    "        proba = model.predict_proba(X[te])\n",
    "        m = cls_metrics(y[te], proba, is_multiclass=is_multiclass)\n",
    "        metrics.append(m)\n",
    "    import numpy as np\n",
    "    out = {k: float(np.nanmean([m[k] for m in metrics])) for k in metrics[0]}\n",
    "    out_std = {k+'_std': float(np.nanstd([m[k] for m in metrics])) for k in metrics[0]}\n",
    "    out.update(out_std)\n",
    "    return out\n",
    "\n",
    "# --- tqdm helpers for notebook/text ---\n",
    "import sys\n",
    "try:\n",
    "    from tqdm.notebook import tqdm as _tqdm\n",
    "    _WIDGETS = True\n",
    "except Exception:\n",
    "    from tqdm import tqdm as _tqdm\n",
    "    _WIDGETS = False\n",
    "\n",
    "def ptqdm(disable=True, *args, **kwargs):\n",
    "    kwargs.setdefault(\"dynamic_ncols\", True)\n",
    "    if not _WIDGETS:\n",
    "        kwargs.setdefault(\"file\", sys.stdout)  # avoid pink stderr spam\n",
    "    return _tqdm(disable=True, *args, **kwargs)\n",
    "\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "\n",
    "class TqdmEpochCallback(Callback):\n",
    "    def __init__(self, total_epochs:int, desc:str):\n",
    "        super().__init__()\n",
    "        self.total = total_epochs\n",
    "        self.desc = desc\n",
    "        self.pbar = None\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.pbar = ptqdm(disable=True, total=self.total, desc=self.desc, leave=False)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        show = {}\n",
    "        for k in (\"loss\",\"val_0_rmse\",\"val_0_mae\",\"lr\"):\n",
    "            if k in logs:\n",
    "                try: show[k] = f\"{logs[k]:.4f}\"\n",
    "                except Exception: show[k] = str(logs[k])\n",
    "        self.pbar.update(1)\n",
    "        if show: self.pbar.set_postfix(show)\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.pbar: self.pbar.close()\n",
    "\n",
    "def _tabnet_cv_reg(X, y, n_splits=5, seed=42):\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    try:\n",
    "        from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "        import torch\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    cfg = dict(\n",
    "        n_d=32, n_a=32, n_steps=5, gamma=1.5, lambda_sparse=1e-4,\n",
    "        lr=1e-3, batch=512, vbatch=128, max_epochs=300, patience=50\n",
    "    )\n",
    "\n",
    "    X = np.asarray(X, dtype=np.float32)\n",
    "    y = np.asarray(y, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    mets = []\n",
    "\n",
    "    splits = list(kf.split(X))\n",
    "    assert isinstance(splits, list) and len(splits) > 0, \"No CV splits produced.\"\n",
    "\n",
    "    for fold_idx, (tr, te) in enumerate(\n",
    "        tqdm(splits, desc=\"TabNet CV (folds)\", leave=False, disable=False)\n",
    "    ):\n",
    "        y_scaler = StandardScaler().fit(y[tr])\n",
    "        ytr_s = y_scaler.transform(y[tr])\n",
    "\n",
    "        model = TabNetRegressor(\n",
    "            n_d=cfg[\"n_d\"], n_a=cfg[\"n_a\"], n_steps=cfg[\"n_steps\"],\n",
    "            gamma=cfg[\"gamma\"], lambda_sparse=cfg[\"lambda_sparse\"],\n",
    "            seed=seed, verbose=0,  # keep TabNet quiet; tqdm shows progress\n",
    "            optimizer_fn=torch.optim.AdamW,\n",
    "            optimizer_params=dict(lr=cfg[\"lr\"], weight_decay=1e-5),\n",
    "            scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "            scheduler_params=dict(mode=\"min\", factor=0.5, patience=10, min_lr=1e-5),\n",
    "        )\n",
    "\n",
    "        # epoch bar callback\n",
    "        cb = TqdmEpochCallback(total_epochs=cfg[\"max_epochs\"], desc=f\"fold {fold_idx+1} (epochs)\")\n",
    "\n",
    "        model.fit(\n",
    "            X_train=X[tr], y_train=ytr_s,\n",
    "            eval_set=[(X[te], y_scaler.transform(y[te]))], eval_name=['val'],\n",
    "            eval_metric=['rmse'],\n",
    "            max_epochs=cfg[\"max_epochs\"], patience=cfg[\"patience\"],\n",
    "            batch_size=cfg[\"batch\"], virtual_batch_size=cfg[\"vbatch\"],\n",
    "            callbacks=[cb],\n",
    "        )\n",
    "\n",
    "        y_hat = y_scaler.inverse_transform(model.predict(X[te])).ravel()\n",
    "        mets.append(reg_metrics(y[te].ravel(), y_hat))\n",
    "\n",
    "    out = {k: float(np.mean([m[k] for m in mets])) for k in mets[0]}\n",
    "    out_std = {k+'_std': float(np.std([m[k] for m in mets], ddof=1)) for k in mets[0]}\n",
    "    out.update(out_std)\n",
    "    out[\"config\"] = cfg\n",
    "    return out\n",
    "\n",
    "print(\"TabNet block ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ffc302e-fdcc-42d9-bd4d-fb1818181eeb",
   "metadata": {
    "id": "4ffc302e-fdcc-42d9-bd4d-fb1818181eeb"
   },
   "outputs": [],
   "source": [
    "def cv_eval_cls_df(model, X, y, multiclass: bool = False, n_splits: int = 5, random_state=None):\n",
    "    \"\"\"\n",
    "    Cross-validated classification metrics for a given sklearn Pipeline/estimator.\n",
    "    Returns a dict like {'roc_auc': ..., 'acc': ..., 'macro_f1': ...}.\n",
    "    - Works for binary and multi-class (OVR) using predict_proba.\n",
    "    - Encodes non-numeric targets once to stable integer codes.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "    if random_state is None:\n",
    "        random_state = globals().get(\"SEED\", 42)\n",
    "\n",
    "    # Ensure X is a DataFrame for nice slicing; y as a 1D array of ints if not already\n",
    "    X_df = pd.DataFrame(X) if not hasattr(X, \"iloc\") else X\n",
    "    if not np.issubdtype(pd.Series(y).dtype, np.integer):\n",
    "        y_series = pd.Series(y).astype(\"category\")\n",
    "        y_enc = y_series.cat.codes.to_numpy()\n",
    "    else:\n",
    "        y_enc = np.asarray(y)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    aucs, accs, f1s = [], [], []\n",
    "\n",
    "    for tr_idx, va_idx in skf.split(np.zeros(len(y_enc)), y_enc):\n",
    "        Xtr, Xva = X_df.iloc[tr_idx], X_df.iloc[va_idx]\n",
    "        ytr, yva = y_enc[tr_idx], y_enc[va_idx]\n",
    "\n",
    "        fitted = model.fit(Xtr, ytr)\n",
    "\n",
    "        # probabilities\n",
    "        proba = fitted.predict_proba(Xva)\n",
    "        n_classes = proba.shape[1]\n",
    "\n",
    "        if multiclass or n_classes > 2:\n",
    "            roc = roc_auc_score(yva, proba, multi_class=\"ovr\")\n",
    "            pred = proba.argmax(axis=1)\n",
    "        else:\n",
    "            roc = roc_auc_score(yva, proba[:, 1])\n",
    "            pred = (proba[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "        aucs.append(float(roc))\n",
    "        accs.append(float(accuracy_score(yva, pred)))\n",
    "        f1s.append(float(f1_score(yva, pred, average=\"macro\")))\n",
    "\n",
    "    return {\n",
    "        \"roc_auc\": float(np.mean(aucs)),\n",
    "        \"acc\": float(np.mean(accs)),\n",
    "        \"macro_f1\": float(np.mean(f1s)),\n",
    "    }\n",
    "\n",
    "def cv_eval_reg_pipe(pipe_builder, X, y, folds: int = 5, random_state=None, **builder_kwargs):\n",
    "    \"\"\"\n",
    "    Cross-validated regression metrics for a pipeline BUILDER function.\n",
    "    Returns mean and std across folds: RMSE, RMSE_std, MAE, MAE_std.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "    if random_state is None:\n",
    "        random_state = globals().get(\"SEED\", 42)\n",
    "\n",
    "    if hasattr(X, \"iloc\"):\n",
    "        X_df = X.copy()\n",
    "    else:\n",
    "        X = np.asarray(X)\n",
    "        X_df = pd.DataFrame(X, columns=[f\"f{i}\" for i in range(X.shape[1])])\n",
    "\n",
    "    cat_cols = [c for c in X_df.columns if X_df[c].dtype.name in (\"object\", \"category\")]\n",
    "    num_cols = [c for c in X_df.columns if c not in cat_cols]\n",
    "\n",
    "    try:\n",
    "        pipe = pipe_builder(cat_cols=cat_cols, num_cols=num_cols, **builder_kwargs)\n",
    "    except TypeError:\n",
    "        pipe = pipe_builder()\n",
    "\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=random_state)\n",
    "    rmses, maes = [], []\n",
    "    y_arr = np.asarray(y)\n",
    "\n",
    "    for tr_idx, va_idx in kf.split(X_df):\n",
    "        Xtr, Xva = X_df.iloc[tr_idx], X_df.iloc[va_idx]\n",
    "        ytr, yva = y_arr[tr_idx], y_arr[va_idx]\n",
    "\n",
    "        fitted = pipe.fit(Xtr, ytr)\n",
    "        pred = fitted.predict(Xva)\n",
    "\n",
    "        rmses.append(float(np.sqrt(mean_squared_error(yva, pred))))\n",
    "        maes.append(float(mean_absolute_error(yva, pred)))\n",
    "\n",
    "    return {\n",
    "        \"RMSE\": float(np.mean(rmses)),\n",
    "        \"RMSE_std\": float(np.std(rmses, ddof=1)) if len(rmses) > 1 else 0.0,\n",
    "        \"MAE\": float(np.mean(maes)),\n",
    "        \"MAE_std\": float(np.std(maes, ddof=1)) if len(maes) > 1 else 0.0,\n",
    "    }\n",
    "\n",
    "if \"results\" not in globals() or not isinstance(globals().get(\"results\"), dict):\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ca3c8-3678-46f4-805b-3b468104fa49",
   "metadata": {},
   "source": [
    "## Results for mfeat-fourier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25cceae3",
   "metadata": {
    "id": "25cceae3",
    "outputId": "05051500-ba65-4a4f-a016-e5f2f4846dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mfeat] Running XGBoost...\n",
      "[mfeat] Running LightGBM...\n",
      "[mfeat] Running CatBoost...\n",
      "\n",
      "=== Classical models on mfeat-fourier ===\n",
      "      dataset    model    acc  macro_f1  roc_auc\n",
      "mfeat-fourier      xgb 0.8400    0.8396   0.9828\n",
      "mfeat-fourier     lgbm 0.8290    0.8278   0.9808\n",
      "mfeat-fourier catboost 0.8415    0.8406   0.9840\n",
      "Saved CSV -> /home/kutaytire/RL_training/results/classical_mfeat_fourier.csv\n",
      "[mfeat] Running TabPFN...\n",
      "[mfeat] Running TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 105 with best_epoch = 85 and best_val_accuracy = 0.72\n",
      "\n",
      "Early stopping occurred at epoch 91 with best_epoch = 71 and best_val_accuracy = 0.705\n",
      "\n",
      "Early stopping occurred at epoch 127 with best_epoch = 107 and best_val_accuracy = 0.71\n",
      "\n",
      "Early stopping occurred at epoch 115 with best_epoch = 95 and best_val_accuracy = 0.71\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 134 and best_val_accuracy = 0.77\n",
      "\n",
      "=== DL models on mfeat-fourier (TabPFN, TabNet) ===\n",
      "      dataset  model   acc  acc_std  macro_f1  macro_f1_std  roc_auc  roc_auc_std\n",
      "mfeat-fourier tabpfn 0.891   0.0087    0.8902        0.0088   0.9917       0.0014\n",
      "mfeat-fourier tabnet 0.723   0.0240    0.7187        0.0240   0.9510       0.0086\n",
      "Saved CSV -> /home/kutaytire/RL_training/results/dl_mfeat_fourier.csv\n",
      "[mfeat] Done.\n"
     ]
    }
   ],
   "source": [
    "# === RUN + PRINT + SAVE: mfeat-fourier (multi-class) ===\n",
    "import warnings, numpy as np, pandas as pd\n",
    "dset_name = \"mfeat-fourier\"\n",
    "\n",
    "# ---- run classical\n",
    "res_mf = {}\n",
    "print(\"[mfeat] Running XGBoost...\")\n",
    "res_mf['xgb'] = cv_eval_cls_df(build_cls_pipeline_xgb(cat_cols=[], num_cols=[], multiclass=True), mf_X, mf_y, multiclass=True)\n",
    "\n",
    "print(\"[mfeat] Running LightGBM...\")\n",
    "res_mf['lgbm'] = cv_eval_cls_df(build_cls_pipeline_lgbm(cat_cols=[], num_cols=[], multiclass=True), mf_X, mf_y, multiclass=True)\n",
    "\n",
    "print(\"[mfeat] Running CatBoost...\")\n",
    "res_mf['catboost'] = cv_eval_cls_df(build_cls_pipeline_catboost([], list(mf_X.columns), multiclass=True), mf_X, mf_y, multiclass=True)\n",
    "\n",
    "# ---- print + save classical\n",
    "def _print_save(df_rows, fname, title):\n",
    "    if not df_rows:\n",
    "        print(f\"No results for {title}.\"); return None\n",
    "    df = pd.DataFrame(df_rows)\n",
    "    metric_cols = [c for c in df.columns if c not in (\"dataset\", \"model\")]\n",
    "    df = df[[\"dataset\", \"model\"] + sorted(metric_cols)]\n",
    "    df_show = df.copy()\n",
    "    for c in metric_cols:\n",
    "        if pd.api.types.is_numeric_dtype(df_show[c]):\n",
    "            df_show[c] = df_show[c].astype(float).round(4)\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(df_show.to_string(index=False))\n",
    "\n",
    "    # save here\n",
    "    out = RESULTS_DIR / fname\n",
    "    df.to_csv(out, index=False)\n",
    "    print(f\"Saved CSV -> {out.resolve()}\")\n",
    "    return df\n",
    "\n",
    "rows_classical = []\n",
    "for k in [\"xgb\",\"lgbm\",\"catboost\"]:\n",
    "    if isinstance(res_mf.get(k), dict):\n",
    "        rows_classical.append({\"dataset\": dset_name, \"model\": k, **res_mf[k]})\n",
    "_ = _print_save(rows_classical, \"classical_mfeat_fourier.csv\", \"Classical models on mfeat-fourier\")\n",
    "\n",
    "# ---- run DL\n",
    "print(\"[mfeat] Running TabPFN...\")\n",
    "res_mf['tabpfn'] = run_tabpfn_cv(mf_X, mf_y, n_splits=5, is_multiclass=True)\n",
    "\n",
    "print(\"[mfeat] Running TabNet...\")\n",
    "res_mf['tabnet'] = _tabnet_cv_cls(mf_X_std.astype(np.float32), mf_y, n_splits=5, is_multiclass=True)\n",
    "\n",
    "# ---- print + save DL\n",
    "rows_dl = []\n",
    "for k in [\"tabpfn\",\"tabnet\"]:\n",
    "    if isinstance(res_mf.get(k), dict):\n",
    "        rows_dl.append({\"dataset\": dset_name, \"model\": k, **res_mf[k]})\n",
    "_ = _print_save(rows_dl, \"dl_mfeat_fourier.csv\", \"DL models on mfeat-fourier (TabPFN, TabNet)\")\n",
    "\n",
    "# stash to global results\n",
    "results['mfeat-fourier'] = res_mf\n",
    "print(\"[mfeat] Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32d47c-c27a-4210-82cd-f2c9b71665fb",
   "metadata": {},
   "source": [
    "## Results for credit-g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "061d341b-4874-498b-bd48-6ac16a8a76b5",
   "metadata": {
    "id": "061d341b-4874-498b-bd48-6ac16a8a76b5",
    "outputId": "c05687e0-9313-456b-86ea-97bca3b25ee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[credit-g] Running XGBoost...\n",
      "[credit-g] Running LightGBM...\n",
      "[credit-g] Running CatBoost...\n",
      "\n",
      "=== Classical models on credit-g ===\n",
      " dataset    model   acc  macro_f1  roc_auc\n",
      "credit-g      xgb 0.762    0.6973   0.7766\n",
      "credit-g     lgbm 0.718    0.6294   0.6974\n",
      "credit-g catboost 0.760    0.6860   0.7816\n",
      "Saved CSV -> /home/kutaytire/RL_training/results/classical_credit_g.csv\n",
      "[credit-g] Running TabPFN...\n",
      "[credit-g] Running TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_auc = 0.48982\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_auc = 0.46458\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_auc = 0.50054\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_auc = 0.40125\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_auc = 0.46387\n",
      "\n",
      "=== DL models on credit-g (TabPFN, TabNet) ===\n",
      " dataset  model   acc  acc_std  macro_f1  macro_f1_std  roc_auc  roc_auc_std\n",
      "credit-g tabpfn 0.753   0.0108    0.6716        0.0174   0.7888       0.0175\n",
      "credit-g tabnet 0.475   0.0365    0.4543        0.0345   0.4640       0.0345\n",
      "Saved CSV -> /home/kutaytire/RL_training/results/dl_credit_g.csv\n",
      "[credit-g] Done.\n"
     ]
    }
   ],
   "source": [
    "# === RUN + PRINT + SAVE: credit-g (binary) ===\n",
    "import warnings, numpy as np, pandas as pd\n",
    "warnings.filterwarnings(\"ignore\", message=\"Best weights from best epoch are automatically used!\", module=r\"pytorch_tabnet\\.callbacks\")\n",
    "\n",
    "dset_name = \"credit-g\"\n",
    "\n",
    "# ---- run classical\n",
    "res_cg = {}\n",
    "print(\"[credit-g] Running XGBoost...\")\n",
    "res_cg['xgb'] = cv_eval_cls_df(build_cls_pipeline_xgb(cat_cols=cg_cat_cols, num_cols=cg_num_cols, multiclass=False), cg_X, cg_y, multiclass=False)\n",
    "\n",
    "print(\"[credit-g] Running LightGBM...\")\n",
    "res_cg['lgbm'] = cv_eval_cls_df(build_cls_pipeline_lgbm(cat_cols=cg_cat_cols, num_cols=cg_num_cols, multiclass=False), cg_X, cg_y, multiclass=False)\n",
    "\n",
    "print(\"[credit-g] Running CatBoost...\")\n",
    "res_cg['catboost'] = cv_eval_cls_df(build_cls_pipeline_catboost(cat_cols=cg_cat_cols, num_cols=cg_num_cols, multiclass=False), cg_X, cg_y, multiclass=False)\n",
    "\n",
    "def _print_save(df_rows, fname, title):\n",
    "    if not df_rows:\n",
    "        print(f\"No results for {title}.\"); return None\n",
    "    df = pd.DataFrame(df_rows)\n",
    "    metric_cols = [c for c in df.columns if c not in (\"dataset\", \"model\")]\n",
    "    df = df[[\"dataset\", \"model\"] + sorted(metric_cols)]\n",
    "    df_show = df.copy()\n",
    "    for c in metric_cols:\n",
    "        if pd.api.types.is_numeric_dtype(df_show[c]):\n",
    "            df_show[c] = df_show[c].astype(float).round(4)\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(df_show.to_string(index=False))\n",
    "\n",
    "    # save here\n",
    "    out = RESULTS_DIR / fname\n",
    "    df.to_csv(out, index=False)\n",
    "    print(f\"Saved CSV -> {out.resolve()}\")\n",
    "    return df\n",
    "\n",
    "rows_classical = []\n",
    "for k in [\"xgb\",\"lgbm\",\"catboost\"]:\n",
    "    if isinstance(res_cg.get(k), dict):\n",
    "        rows_classical.append({\"dataset\": dset_name, \"model\": k, **res_cg[k]})\n",
    "_ = _print_save(rows_classical, \"classical_credit_g.csv\", \"Classical models on credit-g\")\n",
    "\n",
    "# ---- prep for DL (ordinal cats + scale nums)\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "cg_X_ord = cg_X.copy()\n",
    "if len(cg_cat_cols) > 0:\n",
    "    cg_X_ord[cg_cat_cols] = enc.fit_transform(cg_X_ord[cg_cat_cols])\n",
    "scaler_cg = StandardScaler()\n",
    "cg_X_scaled = cg_X_ord.copy()\n",
    "if len(cg_num_cols) > 0:\n",
    "    cg_X_scaled[cg_num_cols] = scaler_cg.fit_transform(cg_X_scaled[cg_num_cols])\n",
    "\n",
    "# ---- run DL\n",
    "print(\"[credit-g] Running TabPFN...\")\n",
    "res_cg['tabpfn'] = run_tabpfn_cv(cg_X_ord, cg_y, n_splits=5, is_multiclass=False)\n",
    "\n",
    "print(\"[credit-g] Running TabNet...\")\n",
    "res_cg['tabnet'] = _tabnet_cv_cls(cg_X_scaled.values.astype(np.float32), cg_y, n_splits=5, is_multiclass=False)\n",
    "\n",
    "rows_dl = []\n",
    "for k in [\"tabpfn\",\"tabnet\"]:\n",
    "    if isinstance(res_cg.get(k), dict):\n",
    "        rows_dl.append({\"dataset\": dset_name, \"model\": k, **res_cg[k]})\n",
    "_ = _print_save(rows_dl, \"dl_credit_g.csv\", \"DL models on credit-g (TabPFN, TabNet)\")\n",
    "\n",
    "# stash to global results\n",
    "results['credit-g'] = res_cg\n",
    "print(\"[credit-g] Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4634c6e9-bcdf-48c0-ad07-929ee2f2f8bc",
   "metadata": {},
   "source": [
    "## Results for SARCOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac78b0a4-b61c-4694-aa63-dc193a2fc6c4",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "7bc4e877176c4018a03d8c5244926ea5",
      "1500a942a5de4d4f8504ae76af0791dc"
     ]
    },
    "id": "ac78b0a4-b61c-4694-aa63-dc193a2fc6c4",
    "outputId": "ef39868e-2e86-4713-ec1d-4cf52b1dd800",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sarcos_y1] Running XGBoost...\n",
      "[sarcos_y1] Running LightGBM...\n",
      "[sarcos_y1] Running CatBoost...\n",
      "\n",
      "=== Classical models on sarcos_y1 (RMSE/MAE ± std) ===\n",
      "  dataset    model    MAE  MAE_std   RMSE  RMSE_std\n",
      "sarcos_y1      xgb 1.5322   0.0145 2.2181    0.0453\n",
      "sarcos_y1     lgbm 1.3980   0.0154 2.0665    0.0290\n",
      "sarcos_y1 catboost 1.3388   0.0079 1.9641    0.0279\n",
      "Saved CSV -> /home/kutaytire/RL_training/results/classical_sarcos_y1.csv\n",
      "[sarcos_y1] Running TabNet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97e6d08664a49c2b93378dbf172d093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TabNet CV (folds):   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === RUN + PRINT + SAVE: SARCOS (regression) ===\n",
    "import warnings, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\", message=\"Best weights from best epoch are automatically used!\", module=r\"pytorch_tabnet\\.callbacks\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
    "\n",
    "# dataset name uses whichever torque you picked (y1..y7)\n",
    "dset_name = f\"sarcos_{globals().get('sarcos_target','y1')}\"\n",
    "\n",
    "# results dir\n",
    "RESULTS_DIR = Path(\"./results\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Standardize features for TabNet (helps stability on SARCOS)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_sar = StandardScaler()\n",
    "bh_X_std = scaler_sar.fit_transform(bh_X.values)  # reuses bh_X from your SARCOS loader cell\n",
    "\n",
    "# ---- Run classical (builders + cv_eval_reg_pipe assumed defined with stds) ----\n",
    "res_bh = {}\n",
    "print(f\"[{dset_name}] Running XGBoost...\")\n",
    "res_bh['xgb'] = cv_eval_reg_pipe(build_reg_pipeline_xgb, bh_X.values, bh_y, folds=5)\n",
    "\n",
    "print(f\"[{dset_name}] Running LightGBM...\")\n",
    "res_bh['lgbm'] = cv_eval_reg_pipe(build_reg_pipeline_lgbm, bh_X.values, bh_y, folds=5)\n",
    "\n",
    "print(f\"[{dset_name}] Running CatBoost...\")\n",
    "res_bh['catboost'] = cv_eval_reg_pipe(build_reg_pipeline_catboost, bh_X.values, bh_y, folds=5)\n",
    "\n",
    "# unified printer\n",
    "def _print_save(df_rows, fname, title):\n",
    "    if not df_rows:\n",
    "        print(f\"No results for {title}.\"); return None\n",
    "    df = pd.DataFrame(df_rows)\n",
    "    metric_cols = [c for c in df.columns if c not in (\"dataset\", \"model\")]\n",
    "    df = df[[\"dataset\", \"model\"] + sorted(metric_cols)]\n",
    "    df_show = df.copy()\n",
    "    for c in metric_cols:\n",
    "        if pd.api.types.is_numeric_dtype(df_show[c]):\n",
    "            df_show[c] = df_show[c].astype(float).round(4)\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(df_show.to_string(index=False))\n",
    "    out = RESULTS_DIR / fname\n",
    "    df.to_csv(out, index=False)\n",
    "    print(f\"Saved CSV -> {out.resolve()}\")\n",
    "    return df\n",
    "\n",
    "# ---- print & save classical ----\n",
    "rows_classical = []\n",
    "for k in [\"xgb\",\"lgbm\",\"catboost\"]:\n",
    "    if isinstance(res_bh.get(k), dict):\n",
    "        rows_classical.append({\"dataset\": dset_name, \"model\": k, **res_bh[k]})\n",
    "_ = _print_save(rows_classical, f\"classical_{dset_name}.csv\", f\"Classical models on {dset_name} (RMSE/MAE ± std)\")\n",
    "\n",
    "# ---- DL: TabNet (uses your stabilized CV helper) ----\n",
    "print(f\"[{dset_name}] Running TabNet...\")\n",
    "res_bh['tabnet'] = _tabnet_cv_reg(bh_X_std, bh_y, n_splits=5)\n",
    "#print(f\"[{dset_name}] Best TabNet config: {res_bh['tabnet'].get('config')}\")\n",
    "\n",
    "# ---- DL: TabPFN Regressor (if available) ----\n",
    "print(f\"[{dset_name}] Running TabPFN-Regressor...\")\n",
    "try:\n",
    "    res_bh['tabpfn'] = run_tabpfn_reg_cv(bh_X.values, bh_y, n_splits=5)\n",
    "except NameError:\n",
    "    print(\"run_tabpfn_reg_cv not found. Make sure you ran the TabPFN CV cell.\")\n",
    "    res_bh['tabpfn'] = None\n",
    "\n",
    "# ---- print & save DL ----\n",
    "rows_dl = []\n",
    "for k in [\"tabnet\",\"tabpfn\"]:\n",
    "    if isinstance(res_bh.get(k), dict):\n",
    "        row = {\"dataset\": dset_name, \"model\": k, **{kk: vv for kk, vv in res_bh[k].items() if kk != \"config\"}}\n",
    "        rows_dl.append(row)\n",
    "_ = _print_save(rows_dl, f\"dl_{dset_name}.csv\", f\"DL models on {dset_name} (RMSE/MAE ± std)\")\n",
    "\n",
    "# stash to global results\n",
    "if \"results\" not in globals() or not isinstance(globals().get(\"results\"), dict):\n",
    "    results = {}\n",
    "results[dset_name] = res_bh\n",
    "print(f\"[{dset_name}] Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8cffea",
   "metadata": {},
   "source": [
    "## Ensembling (DL + Tree Baseline)\n",
    "\n",
    "**Task.** For **one dataset** (**credit-g**), combine a DL model (e.g., **TabPFN**) with a tree model (e.g., **XGBoost**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d37b91a7",
   "metadata": {
    "id": "d37b91a7",
    "outputId": "0a110557-698f-495d-9795-60d6225ec529"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost (OOF):    {'roc_auc': 0.7798, 'acc': 0.769, 'macro_f1': 0.6773}\n",
      "TabPFN (OOF):                 {'roc_auc': 0.7979, 'acc': 0.77, 'macro_f1': 0.7041}\n",
      "Stacking [TabPFN + XGB]:   {'roc_auc': 0.799, 'acc': 0.772, 'macro_f1': 0.6927}\n",
      "\n",
      "Δ vs XGB: {'roc_auc': 0.0192, 'acc': 0.003, 'macro_f1': 0.0154}\n",
      "Δ vs TabPFN:   {'roc_auc': 0.0012, 'acc': 0.002, 'macro_f1': -0.0114}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Expect these to exist: cg_X (DataFrame), cg_y (array-like 0/1),\n",
    "# cg_cat_cols, cg_num_cols, build_cls_pipeline_xgb\n",
    "\n",
    "# -- TabPFN availability\n",
    "try:\n",
    "    from tabpfn import TabPFNClassifier\n",
    "    _TABPFN_OK = True\n",
    "except Exception as e:\n",
    "    _TABPFN_OK = False\n",
    "    raise RuntimeError(f\"TabPFN not available: {e}\")\n",
    "\n",
    "SEED = globals().get(\"SEED\", 42)\n",
    "\n",
    "# -- helper: encode DataFrame for TabPFN\n",
    "def _encode_for_tabpfn(df: pd.DataFrame) -> np.ndarray:\n",
    "    enc = pd.DataFrame(index=df.index)\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype in (\"object\", \"category\"):\n",
    "            col = df[c].astype(\"category\")\n",
    "            enc[c] = (col.cat.codes + 1).astype(\"int32\")  # NaN -> -1 -> 0\n",
    "        else:\n",
    "            col = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float32\")\n",
    "            if col.isna().any():\n",
    "                col = col.fillna(col.median())\n",
    "            enc[c] = col\n",
    "    return np.ascontiguousarray(enc.values, dtype=np.float32)\n",
    "\n",
    "X_tab = _encode_for_tabpfn(cg_X)\n",
    "y_int  = pd.Series(cg_y).astype(\"category\").cat.codes.to_numpy()  # ensure 0/1 ints\n",
    "\n",
    "# -- robust TabPFN constructor\n",
    "def _make_tabpfn(n_ens=64):\n",
    "    for k in (\"N_ensemble_configurations\", \"N_ensembles\", \"n_ensembles\"):\n",
    "        try:\n",
    "            return TabPFNClassifier(**{k: int(n_ens)})\n",
    "        except TypeError:\n",
    "            continue\n",
    "    return TabPFNClassifier()\n",
    "\n",
    "# -- base XGB pipeline\n",
    "xgb_base = build_cls_pipeline_xgb(cat_cols=cg_cat_cols, num_cols=cg_num_cols, multiclass=False)\n",
    "\n",
    "# -- metrics helper\n",
    "def _metrics(y_true, proba):\n",
    "    pred = proba.argmax(1)\n",
    "    return dict(\n",
    "        roc_auc=float(roc_auc_score(y_true, proba[:,1])),\n",
    "        acc=float(accuracy_score(y_true, pred)),\n",
    "        macro_f1=float(f1_score(y_true, pred, average=\"macro\")),\n",
    "    )\n",
    "\n",
    "# Shared outer folds across all runs\n",
    "outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "# ===============================\n",
    "# 1) XGBoost (CALIBRATED) — OOF\n",
    "# ===============================\n",
    "proba_xgbcal_oof = np.zeros((len(y_int), 2), dtype=np.float32)\n",
    "for tr, te in outer.split(np.zeros(len(y_int)), y_int):\n",
    "    # Calibrate XGB on the outer-train split (3-fold isotonic)\n",
    "    cal_full = CalibratedClassifierCV(clone(xgb_base), method=\"isotonic\", cv=3)\n",
    "    cal_full.fit(cg_X.iloc[tr], y_int[tr])\n",
    "    proba_xgbcal_oof[te] = cal_full.predict_proba(cg_X.iloc[te])\n",
    "m_xgb_cal = _metrics(y_int, proba_xgbcal_oof)\n",
    "\n",
    "# =========================\n",
    "# 2) TabPFN only — OOF\n",
    "# =========================\n",
    "proba_tpfn_oof = np.zeros((len(y_int), 2), dtype=np.float32)\n",
    "for tr, te in outer.split(np.zeros(len(y_int)), y_int):\n",
    "    clf = _make_tabpfn(n_ens=64)\n",
    "    clf.fit(X_tab[tr], y_int[tr])\n",
    "    proba_tpfn_oof[te] = clf.predict_proba(X_tab[te])\n",
    "m_tpfn = _metrics(y_int, proba_tpfn_oof)\n",
    "\n",
    "# ===============================================\n",
    "# 3) Stacking: TabPFN + CALIBRATED XGB — nested\n",
    "# ===============================================\n",
    "inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED+1)\n",
    "proba_stack_oof = np.zeros((len(y_int), 2), dtype=np.float32)\n",
    "\n",
    "for tr_idx, te_idx in outer.split(np.zeros(len(y_int)), y_int):\n",
    "    X_tr_df, X_te_df = cg_X.iloc[tr_idx], cg_X.iloc[te_idx]\n",
    "    y_tr, y_te       = y_int[tr_idx], y_int[te_idx]\n",
    "    X_tr_tab, X_te_tab = X_tab[tr_idx], X_tab[te_idx]\n",
    "\n",
    "    # inner OOF for meta features (CALIBRATED XGB)\n",
    "    oof_tab = np.zeros((len(tr_idx), 2), dtype=np.float32)\n",
    "    oof_xgb = np.zeros((len(tr_idx), 2), dtype=np.float32)\n",
    "\n",
    "    for itr, iva in inner.split(np.zeros(len(tr_idx)), y_tr):\n",
    "        # ---- calibrated XGB on inner training split\n",
    "        cal = CalibratedClassifierCV(clone(xgb_base), method=\"isotonic\", cv=3)\n",
    "        cal.fit(X_tr_df.iloc[itr], y_tr[itr])\n",
    "        oof_xgb[iva] = cal.predict_proba(X_tr_df.iloc[iva])\n",
    "\n",
    "        # ---- TabPFN on encoded arrays\n",
    "        mt = _make_tabpfn(n_ens=64)\n",
    "        mt.fit(X_tr_tab[itr], y_tr[itr])\n",
    "        oof_tab[iva] = mt.predict_proba(X_tr_tab[iva])\n",
    "\n",
    "    # meta-learner on inner OOF probs (use raw probs; logits also work well)\n",
    "    X_meta = np.c_[oof_tab[:,1], oof_xgb[:,1]]\n",
    "    meta = LogisticRegression(max_iter=1000)\n",
    "    meta.fit(X_meta, y_tr)\n",
    "\n",
    "    # refit bases on full outer-train and predict outer-test\n",
    "    cal_full = CalibratedClassifierCV(clone(xgb_base), method=\"isotonic\", cv=3)\n",
    "    cal_full.fit(X_tr_df, y_tr)\n",
    "    p_xgb_te = cal_full.predict_proba(X_te_df)[:, 1]\n",
    "\n",
    "    mt_full = _make_tabpfn(n_ens=64).fit(X_tr_tab, y_tr)\n",
    "    p_tab_te = mt_full.predict_proba(X_te_tab)[:, 1]\n",
    "\n",
    "    p_meta_te = meta.predict_proba(np.c_[p_tab_te, p_xgb_te])[:, 1]\n",
    "    proba_stack_oof[te_idx] = np.c_[1 - p_meta_te, p_meta_te]\n",
    "\n",
    "m_stack = _metrics(y_int, proba_stack_oof)\n",
    "\n",
    "# =========================\n",
    "# Print comparison + deltas\n",
    "# =========================\n",
    "def _fmt(d): return {k: round(v, 4) for k, v in d.items()}\n",
    "print(\"XGBoost (OOF):   \", _fmt(m_xgb_cal))\n",
    "print(\"TabPFN (OOF):                \", _fmt(m_tpfn))\n",
    "print(\"Stacking [TabPFN + XGB]:  \", _fmt(m_stack))\n",
    "\n",
    "def _delta(base, stacked):\n",
    "    return {k: round(stacked[k] - base[k], 4) for k in base}\n",
    "\n",
    "print(\"\\nΔ vs XGB:\", _delta(m_xgb_cal, m_stack))\n",
    "print(\"Δ vs TabPFN:  \", _delta(m_tpfn, m_stack))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ofIGl0fQ4Nj",
   "metadata": {
    "id": "4ofIGl0fQ4Nj"
   },
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W8tRgG_x0RiK",
   "metadata": {
    "id": "W8tRgG_x0RiK"
   },
   "source": [
    "Fine-tune the same TABNet model on different datasets:\n",
    "  - Train on credit-g dataset.\n",
    "  - Fine-tune on mfeat-fourier dataset.\n",
    "\n",
    "To match the feature size, we use PCA to select the dominant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "QBQeAbamLjyc",
   "metadata": {
    "id": "QBQeAbamLjyc"
   },
   "outputs": [],
   "source": [
    "## -- Importing essential libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "HgmhrGW3DCnq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgmhrGW3DCnq",
    "outputId": "0e698c54-75cc-4238-abba-b0ccb72927cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimensions:\n",
      "  X1: (2000, 76) (76 features)\n",
      "  X2: (1000, 20) (20 features)\n",
      "\n",
      "X1 has MORE features (76 > 20)\n",
      "Applying PCA to X1...\n",
      "\n",
      "PCA complete!\n",
      "  X1: (2000, 76)->(2000, 20)\n",
      "  X2: (1000, 20)->(1000, 20)\n",
      "Total variance explained: 66.65%\n"
     ]
    }
   ],
   "source": [
    "## -- PCA function to match input dimensionality of both datasets\n",
    "# Expect these to exist: cg_X_scaled, mf_X_std\n",
    "def pca_function(x1, x2):\n",
    "  n_features_1 = x1.shape[1]\n",
    "  n_features_2 = x2.shape[1]\n",
    "\n",
    "  print(f\"Input dimensions:\")\n",
    "  print(f\"  X1: {x1.shape} ({n_features_1} features)\")\n",
    "  print(f\"  X2: {x2.shape} ({n_features_2} features)\")\n",
    "  if n_features_1 == n_features_2:\n",
    "      print(f\"\\nBoth datasets have same number of features ({n_features_1})\")\n",
    "      print(\"No PCA needed!\")\n",
    "      return x1, x2\n",
    "  elif n_features_1 > n_features_2:\n",
    "      X_high = x1\n",
    "      X_low = x2\n",
    "      n_high = n_features_1\n",
    "      n_low = n_features_2\n",
    "      print(f\"\\nX1 has MORE features ({n_features_1} > {n_features_2})\")\n",
    "      print(f\"Applying PCA to X1...\")\n",
    "  else:\n",
    "      X_high = x2\n",
    "      X_low = x1\n",
    "      n_high = n_features_2\n",
    "      n_low = n_features_1\n",
    "      print(f\"\\nX2 has MORE features ({n_features_2} > {n_features_1})\")\n",
    "      print(f\"Applying PCA to X2...\")\n",
    "\n",
    "  pca = PCA(n_components=n_low, random_state=42)\n",
    "  X_high_reduced = pca.fit_transform(X_high)\n",
    "\n",
    "  print(f\"\\nPCA complete!\")\n",
    "  if n_features_1 > n_features_2:\n",
    "    print(f\"  X1: {x1.shape}->{X_high_reduced.shape}\")\n",
    "    print(f\"  X2: {x2.shape}->{X_low.shape}\")\n",
    "  else:\n",
    "    print(f\"  X1: {x1.shape}->{X_low.shape}\")\n",
    "    print(f\"  X2: {x2.shape}->{X_high_reduced.shape}\")\n",
    "\n",
    "  variance_explained = pca.explained_variance_ratio_.sum()\n",
    "  print(f\"Total variance explained: {variance_explained*100:.2f}%\")\n",
    "  return X_high_reduced, X_low\n",
    "\n",
    "x1, x2 = pca_function(mf_X_std, cg_X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "Dv-odO4RQB8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dv-odO4RQB8e",
    "outputId": "c9685623-22a3-4ee6-c96a-33b541a5d712"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done split_data.\n"
     ]
    }
   ],
   "source": [
    "## -- Splitting data\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "\n",
    "  x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "\n",
    "  return x_train, x_test, y_train, y_test\n",
    "\n",
    "xtrain1, xtest1, ytrain1, ytest1 = split_data(x2.to_numpy(), cg_y, test_size=0.2, random_state=42)\n",
    "xtrain2, xtest2, ytrain2, ytest2 = split_data(x1, mf_y, test_size=0.2, random_state=42)\n",
    "print('Done split_data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "Kl0ztDfmesl4",
   "metadata": {
    "id": "Kl0ztDfmesl4"
   },
   "outputs": [],
   "source": [
    "## -- Pre-training TABNet function\n",
    "def _tabnet_singletrain_cls(X, y, xtest, ytest, val_size=0.125, ckpt_path=\"tabnet_A\", is_multiclass=False, seed=42, save_path=False):\n",
    "    if not TABNET_OK:\n",
    "        return None\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=seed)\n",
    "    tr_idx, va_idx = next(sss.split(X, y))\n",
    "    Xtr, Xva = X[tr_idx], X[va_idx]\n",
    "    ytr, yva = y[tr_idx], y[va_idx]\n",
    "\n",
    "    metric = ['auc'] if not is_multiclass else ['accuracy']\n",
    "\n",
    "    model = TabNetClassifier(\n",
    "        n_d=32, n_a=32, n_steps=5, gamma=1.3,\n",
    "        lambda_sparse=1e-4,\n",
    "        optimizer_params=dict(lr=2e-2),\n",
    "        seed=seed, verbose=0\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train=Xtr, y_train=ytr,\n",
    "        eval_set=[(Xva, yva)], eval_name=['val'],\n",
    "        eval_metric=metric,\n",
    "        max_epochs=250, patience=20,\n",
    "        batch_size=1024, virtual_batch_size=128\n",
    "    )\n",
    "\n",
    "    if save_path:\n",
    "      model.save_model(ckpt_path)\n",
    "\n",
    "    proba = model.predict_proba(xtest)\n",
    "    m = cls_metrics(ytest, proba, is_multiclass=is_multiclass)\n",
    "    return {k: float(m[k]) for k in m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5syajfGCq7YJ",
   "metadata": {
    "id": "5syajfGCq7YJ"
   },
   "outputs": [],
   "source": [
    "## -- Fine-tuning TABNet function\n",
    "def _tabnet_finetune_cls(X, y, xtest, ytest, val_size=0.125, ckpt_path=\"tabnet_A\", is_multiclass=False, seed=42):\n",
    "    if not TABNET_OK:\n",
    "        return None\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=seed)\n",
    "    tr_idx, va_idx = next(sss.split(X, y))\n",
    "    Xtr, Xva = X[tr_idx], X[va_idx]\n",
    "    ytr, yva = y[tr_idx], y[va_idx]\n",
    "\n",
    "    model = TabNetClassifier(\n",
    "        n_d=32, n_a=32, n_steps=5, gamma=1.3,\n",
    "        lambda_sparse=1e-4,\n",
    "        optimizer_params=dict(lr=2e-3),\n",
    "        seed=seed, verbose=0\n",
    "    )\n",
    "\n",
    "    load_path = ckpt_path if ckpt_path.endswith(\".zip\") else (ckpt_path + \".zip\")\n",
    "    print(f\"Loading pre-trained model from {load_path}...\")\n",
    "    model.load_model(load_path)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    weights_tr = compute_sample_weight(class_weight=\"balanced\", y=ytr)\n",
    "\n",
    "    metric = ['auc'] if not is_multiclass else ['accuracy']\n",
    "    model.fit(\n",
    "        X_train=Xtr, y_train=ytr,\n",
    "        eval_set=[(Xva, yva)], eval_name=['val'],\n",
    "        eval_metric=metric,\n",
    "        max_epochs=250, patience=20,\n",
    "        batch_size=1024, virtual_batch_size=128,\n",
    "        weights=weights_tr\n",
    "    )\n",
    "\n",
    "    proba = model.predict_proba(xtest)\n",
    "    m = cls_metrics(ytest, proba, is_multiclass=is_multiclass)\n",
    "    return {k: float(m[k]) for k in m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bEFHZ5m1AqvX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bEFHZ5m1AqvX",
    "outputId": "cabe1ef1-f940-4600-f9bb-2e39709cc332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training on credit-g dataset...\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_auc = 0.53795\n",
      "Successfully saved model at tabnet_A.zip\n",
      "\n",
      "Fine-tuning on mfeat-fourier dataset...\n",
      "Loading pre-trained model from tabnet_A.zip...\n",
      "Done.\n",
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 19 and best_val_accuracy = 0.56875\n",
      "\n",
      "Baseline on credit-g dataset...\n",
      "\n",
      "Early stopping occurred at epoch 112 with best_epoch = 92 and best_val_accuracy = 0.6875\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "## -- Running the pre-trained, fine-tuned, and baseline models\n",
    "print(f\"Pre-training on credit-g dataset...\")\n",
    "Pre_cg = _tabnet_singletrain_cls(xtrain1, ytrain1, xtest1, ytest1, val_size=0.2, is_multiclass=False, seed=42, save_path=True)\n",
    "print(f\"\\nFine-tuning on mfeat-fourier dataset...\")\n",
    "FT_mf = _tabnet_finetune_cls(xtrain2, ytrain2, xtest2, ytest2, val_size=0.2, is_multiclass=True, seed=42)\n",
    "print(f\"\\nBaseline on credit-g dataset...\")\n",
    "Baseline_mf = _tabnet_singletrain_cls(xtrain2, ytrain2, xtest2, ytest2, val_size=0.2, is_multiclass=True, seed=42)\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cclfOxV4BFPr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cclfOxV4BFPr",
    "outputId": "021cb575-c75d-4fd5-a043-44eee4a4adc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tResults of fine-tuned vs baseline:\n",
      "             roc_auc  macro_f1     acc\n",
      "Fine-tuned  0.855368  0.482453  0.5000\n",
      "Baseline    0.935868  0.669167  0.6725\n",
      "Difference -0.080500 -0.186714 -0.1725\n"
     ]
    }
   ],
   "source": [
    "## -- Results of fine-tuning\n",
    "results_df = pd.DataFrame({\n",
    "    'Fine-tuned': FT_mf,\n",
    "    'Baseline': Baseline_mf}).T\n",
    "results_df.loc['Difference'] = results_df.loc['Fine-tuned'] - results_df.loc['Baseline']\n",
    "print(\"\\tResults of fine-tuned vs baseline:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8H5iLqKA1wGC",
   "metadata": {
    "id": "8H5iLqKA1wGC"
   },
   "source": [
    "## Transfer Learning (TransTab)\n",
    "\n",
    "**Goal.** Train on dataset **A** and transfer to **B** for at least **two** pairs. TransTab can handle **unaligned or partially aligned schemas**.\n",
    "\n",
    "**Protocol**\n",
    "1. **Pretrain** on A with checkpoint saving.  \n",
    "2. **Build TransTab** model from checkpoint and update schema: `{'cat': ..., 'num': ..., 'bin': ...}`.  \n",
    "3. **Transfer-train** on B using GPU\n",
    "\n",
    "**Reporting**\n",
    "- Compare **transfer vs. training-from-scratch** on B.  \n",
    "- Include mean ± std over folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eLfmXd-2103E",
   "metadata": {
    "id": "eLfmXd-2103E"
   },
   "source": [
    "Transfer learning is performed using the following two pair of datasets:\n",
    "1. heart-statlog (ID: 49) and heart-c (ID: 53): datasets have partially aligned features.\n",
    "2. monks-problems-1 (ID: 333) and monks-problems-2 (ID: 335): datasets have fully aligned features.\n",
    "\n",
    "These datasets are selected from the TabPFN paper as they have matching features for transfer-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5amQDpOZVhV",
   "metadata": {
    "id": "b5amQDpOZVhV"
   },
   "outputs": [],
   "source": [
    "## -- Importing essential libraries\n",
    "!pip install -q transtab\n",
    "import transtab\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "if not hasattr(np, \"Inf\"):\n",
    "    np.Inf = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "MaDGuG9_xTMU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MaDGuG9_xTMU",
    "outputId": "f38a6565-2be1-4168-e93b-e8b348c72b0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded datasets:\n",
      "First pair of datasets:\n",
      "heart-statlog: (270, 13) (cats=0, nums=10, bin=3)\n",
      "heart-c    : (296, 13) (cats=2, nums=8, bin=3)\n",
      "common features  : ['age', 'sex', 'oldpeak', 'slope', 'thal']\n",
      "Features of first pair of dataset are partially aligned.\n",
      "\n",
      "Second pair of datasets:\n",
      "monks-problems-1: (556, 6) (cats=4, nums=0, bin=2)\n",
      "monks-problems-2: (554, 6) (cats=4, nums=0, bin=2)\n",
      "common features : ['attr1', 'attr2', 'attr3', 'attr4', 'attr5', 'attr6']\n",
      "Features of second pair of dataset are fully aligned.\n"
     ]
    }
   ],
   "source": [
    "## -- Divide columns into categorical, numerical, and binary\n",
    "def column_cat(X):\n",
    "  cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "  num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "  bin_cols = [c for c in X.columns if X[c].nunique(dropna=False) == 2]\n",
    "\n",
    "  cat_cols = [c for c in cat_cols if c not in bin_cols]\n",
    "  num_cols = [c for c in num_cols if c not in bin_cols]\n",
    "  return cat_cols, num_cols, bin_cols\n",
    "\n",
    "## -- Loading datasets from opem_ml\n",
    "def load_dataset(id):\n",
    "  d_df, d_df_y, _, _ = load_openml_dataset(id)\n",
    "  d_df = d_df.dropna()\n",
    "  d_X = d_df.drop(columns=[d_df_y])\n",
    "  d_y = d_df[d_df_y].astype('category').cat.codes\n",
    "  d_y = d_y.squeeze() if hasattr(d_y, 'squeeze') else np.asarray(d_y).ravel()\n",
    "  if id == 49:\n",
    "    d_X[\"slope\"] = (d_X[\"slope\"].astype(str).str.lower().map({\"up\":1,\"down\":3, \"flat\":2}).astype(\"int8\"))\n",
    "    d_X[\"thal\"] = (d_X[\"thal\"].astype(str).str.lower().map({\"normal\":3,\"fixed_defect\":6,\"reversable_defect\":7}).astype(\"int8\"))\n",
    "    d_X[\"sex\"] = (d_X[\"sex\"].astype(str).str.lower().map({\"male\":1,\"female\":0}).astype(\"int8\"))\n",
    "    d_X[\"fbs\"] = d_X['fbs'].astype(str).str.strip().str.lower().map({'t': 1, 'f': 0})\n",
    "    d_X[\"exang\"] = (d_X[\"exang\"].astype(str).str.lower().map({\"yes\":1,\"no\":0}).astype(\"int8\"))\n",
    "  cat_cols, num_cols, bin_cols = column_cat(d_X)\n",
    "  if id == 333 or id == 334 or id == 335:\n",
    "    for col in cat_cols:\n",
    "      d_X[col] = d_X[col].astype(str)\n",
    "    for col in bin_cols:\n",
    "      d_X[col] = d_X[col].astype(int)\n",
    "  return d_X, d_y, cat_cols, num_cols, bin_cols\n",
    "\n",
    "d_X1, d_y1, cat_cols1, num_cols1, bin_cols1 = load_dataset(53)\n",
    "d_X2, d_y2, cat_cols2, num_cols2, bin_cols2 = load_dataset(49)\n",
    "d_X3, d_y3, cat_cols3, num_cols3, bin_cols3 = load_dataset(333)\n",
    "d_X4, d_y4, cat_cols4, num_cols4, bin_cols4 = load_dataset(335)\n",
    "\n",
    "print('Loaded datasets:')\n",
    "print('First pair of datasets:')\n",
    "print('heart-statlog:', d_X1.shape, f'(cats={len(cat_cols1)}, nums={len(num_cols1)}, bin={len(bin_cols1)})')\n",
    "print('heart-c    :', d_X2.shape, f'(cats={len(cat_cols2)}, nums={len(num_cols2)}, bin={len(bin_cols2)})')\n",
    "print('common features  :', (d_X1.columns.intersection(d_X2.columns)).to_list())\n",
    "print('Features of first pair of dataset are partially aligned.')\n",
    "print('\\nSecond pair of datasets:')\n",
    "print('monks-problems-1:', d_X3.shape, f'(cats={len(cat_cols3)}, nums={len(num_cols3)}, bin={len(bin_cols3)})')\n",
    "print('monks-problems-2:', d_X4.shape, f'(cats={len(cat_cols4)}, nums={len(num_cols4)}, bin={len(bin_cols4)})')\n",
    "print('common features :', (d_X3.columns.intersection(d_X4.columns)).to_list())\n",
    "print('Features of second pair of dataset are fully aligned.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58o5omRB-hIq",
   "metadata": {
    "id": "58o5omRB-hIq"
   },
   "outputs": [],
   "source": [
    "## -- Transtab function\n",
    "def cv_transtab_5fold(X, y, cat_cols, num_cols, bin_cols, file_name='',\n",
    "                      num_epoch=50, batch_size=64, lr=5e-5,\n",
    "                      patience=15, transfer=False, save_ckpt=False, device=DEVICE, pin_memory=True):\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accs, f1s, aucs = [], [], []\n",
    "\n",
    "    for tr_idx, va_idx in skf.split(X, y):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "        if transfer == False and save_ckpt == True:\n",
    "          model = transtab.build_classifier(cat_cols, num_cols, bin_cols, pin_memory=pin_memory, device=DEVICE)\n",
    "          transtab.train(\n",
    "              model,\n",
    "              (X_tr, y_tr),\n",
    "              (X_va, y_va),\n",
    "              num_epoch=num_epoch,\n",
    "              batch_size=batch_size,\n",
    "              lr=lr,\n",
    "              patience=patience,\n",
    "              output_dir= \"./\" + file_name,\n",
    "              verbose=0\n",
    "          )\n",
    "        elif transfer == False and save_ckpt == False:\n",
    "          model = transtab.build_classifier(cat_cols, num_cols, bin_cols, pin_memory=pin_memory, device=DEVICE)\n",
    "          transtab.train(\n",
    "              model,\n",
    "              (X_tr, y_tr),\n",
    "              (X_va, y_va),\n",
    "              num_epoch=num_epoch,\n",
    "              batch_size=batch_size,\n",
    "              lr=lr,\n",
    "              patience=patience,\n",
    "              pin_memory=True,\n",
    "              verbose=0\n",
    "          )\n",
    "        else:\n",
    "          ckpt_path = os.path.join(os.getcwd(), file_name)  # getcwd() takes no args\n",
    "          model = transtab.build_classifier(\n",
    "                checkpoint=ckpt_path,\n",
    "                device=DEVICE,\n",
    "                pin_memory=True\n",
    "            )\n",
    "          model.update({'cat':cat_cols,'num':num_cols,'bin':bin_cols})\n",
    "          transtab.train(\n",
    "              model,\n",
    "              (X_tr, y_tr),\n",
    "              (X_va, y_va),\n",
    "              num_epoch=50,\n",
    "              batch_size=64,\n",
    "              lr=1e-4,\n",
    "              patience=10, pin_memory=False,\n",
    "              verbose=0\n",
    "              )\n",
    "\n",
    "        ypred = transtab.predict(model, X_va, y_va)\n",
    "        proba = np.asarray(ypred).ravel()\n",
    "        yhat = (proba.ravel() >= 0.5).astype(int)\n",
    "        auc  = roc_auc_score(y_va, proba.ravel())\n",
    "        f1   = f1_score(y_va, yhat)\n",
    "\n",
    "        acc = accuracy_score(y_va, yhat)\n",
    "        accs.append(acc); f1s.append(f1); aucs.append(auc)\n",
    "\n",
    "    return {\n",
    "        'acc_mean': float(np.mean(accs)), 'acc_std': float(np.std(accs, ddof=1)),\n",
    "        'f1_mean':  float(np.mean(f1s)),  'f1_std':  float(np.std(f1s,  ddof=1)),\n",
    "        'auc_mean': float(np.mean(aucs)), 'auc_std': float(np.std(aucs, ddof=1)),\n",
    "        'folds': 5\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "z8Hs5Y3H-_zI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "62363a90559a49f5badd6083989deab2",
      "7bdd646079904065b9a37ae489a8897d",
      "c0dc4415df244eecb855d16ab1d58fec",
      "f6a6ebe565914d19b0c63f5b378695db",
      "e52d92cfb17d4e51a2a7ee52f9ff4404",
      "30ffc944754a4f28a93c89d8a9f5a359",
      "991d29b36b6d482a9cf2db037adb2475",
      "53e18e9254034b4ba7471bed3568ec21",
      "8398ecdcc79f452083b9c53c716d94a0",
      "e4b058b7537746f0afe774438948731f",
      "c0aec25a6d5041119c3e056a6629c88a",
      "2ea25cf41bd244ddaa9e83ec9165ddc3",
      "360539495d18429cab4a8252b1424209",
      "4dff81868eba42ba9784b59d7f913015",
      "2bc0ade094f1408bb5dc583a8aafa6fe",
      "ba5a578954ca4ff09410c4210d1e6158",
      "b27b7b314163443aa092b3d4902e0309",
      "1fe86508201248d4865d0e8bc1cff32f",
      "8ac54d069bc142398ff2e087902caa17",
      "d78e96a066a745cab66d84df1ca15333",
      "243ceed4268842969df86d644a5a6733",
      "07b796002ecb4f8c97ccbf8ef5a127a5",
      "76626f177cb54b16b00bceac4cdb9517",
      "64c8dceed63049a99cca017f9ec3df81",
      "603963f595674c848478f2a0e1e28ddc",
      "a4aea045a07b49aa900c1c2870025c93",
      "08a2c4aa9a9a4daa8c68164228adc8cc",
      "030e8a456b3a460a8c32904131c6b28f",
      "50907e05ead14424bcc47f988e8d0d9f",
      "1e22210cbbff42aea0197afb5ff5e01c",
      "07807fff3f2442a6bc84b9986a1f2008",
      "ac055b8cd824410da6aa76896b77dc53",
      "018456243e66406aacba89f78e25d8e1",
      "8792a4878c0647b4b18de84aac3d0ae8",
      "2591d3f087354875878823b7b6bc8e94",
      "bb1c89f8ee3a44229fb908d59db43c12",
      "d718a09bf18f416d804f4fcd84d2c443",
      "3a3980e312ee4ced9f3378b9902fedb7",
      "392a019a36c44e3986a962431ddafbd8",
      "ce9980cc934b49b28fd40f0c7176a156",
      "51e59ad837ac44128d219f082dae8767",
      "ba23a3925c92407aae7cf39840e45fd3",
      "5f266878a4fc4b6fbaa3aeffb8b5cb0f",
      "7c7c6777c2b14d64aaeeae4cc9a7955a",
      "82ab9bae1a7d43eaa63c57a981a67368",
      "b94359e515034cf3911e106e5297d4e5",
      "0ecf09b325134a7bb5274ab4b5109ddc",
      "a77c1a2a06314ddfb604af18b1f9f0f6",
      "9fcb87be65cd4ffe8d2882fa99982b5b",
      "dd2ee9c2d107495fbd3bccb3affa3ea4",
      "637b6b83b2f1469e93769a28b78a2ef1",
      "5d3acf72a09c42c9a4ec57477bf1ce88",
      "188808d1e31446cc9eb7994bf9a669ea",
      "ee832e0b5f6c42c7ac912f5e491beec8",
      "f699806b35724678953c635b9e652630",
      "a16ce11d03fb4b458eeef621d79d42b7",
      "61d26db3022f4c03afbc0605e9bd89b3",
      "ff3b2d751aee4279998b140272b5433e",
      "30032aa0c02b4f03a8cdac397b4d4d11",
      "ccf3a81b78674fef8002a676ccb2c868",
      "b30ab0ba8ee846ef96d7b942ed8ad5a3",
      "cb63ac3209644cbbb54b3437711fe07e",
      "18c00fedd6bf4f18be604dcb4f4f38fa",
      "278fb0294ad34ee8b19b97d3e68a5440",
      "13209c9f4be14116b22680a45b8fe6b2",
      "ce6f75ea25b64b3fb83c38d94bcf3576",
      "be5e79cd23824334b6b3906a299c2657",
      "749bd4c1a8124988a0a347a2413ad775",
      "b6a82696ef004ae994ca94ca71a9ac72",
      "cdc2f3f102cf48fbafb90ad7b4286cd9",
      "18739b335fd84efab130e62c7b1addac",
      "9e10cbb8c0504bff9c7b3c62ed259a09",
      "c7e7c9f1482a4fbca2edf76dcf3e2e64",
      "1fd42e12a958418a869f3a1effe1f08e",
      "5df245fd46464a12ad07e576ab4e99af",
      "612c6c53bde54d62955fb61a8e09c57a",
      "e4cb11251be44bcd8583d2cdd63aac45",
      "28f7d0ba09964408bd477f809b0ea612",
      "a1fb2a5ecaa74c73a941a1ad4f5132fd",
      "303c7b3bea3d44f48915e0a72624929c",
      "40abeb2ce0e44ee3bb9dc86178d40ac8",
      "7454b044350b4bf8bfc3d6cc75a22e16",
      "4bf1cf00bc5f4ce6a4c95bce5d0cbd9c",
      "f14e3113b8c34776a85fee72fbd0da6a",
      "692b5e664442489bb1ffa629a97fc830",
      "5f3da58136c44ed7a16052babda648c7",
      "06a21a190cbf4feaaf95d50fe2e2fcaf",
      "6b72fdbb1bfd470db1274e46b80f0ae2",
      "05586f2059c7442e90e8c0fce9cfa851",
      "6e3c6165379048368bd5a1915eb5d25e",
      "a32e22e6c90245178858f54fa68e0df0",
      "9c648d6906064dc8999a7dc6e15931b3",
      "2fe8dc7316d541e1a547c843eb2adc4b",
      "e204161a089e4e9d945c8494fc91bbf7",
      "8be53eedf35e4f5d8b4ae45f21b4cef6",
      "304d8568abb84d208e431ab3e17b0456",
      "620db13cb51a4e9bbdedffed6973aab2",
      "2053838cd52e494ea63be7b8cc752ebc",
      "b145e65678fc4d9a8a4334572dbd1eaf",
      "2c7ddd82492d4510983d94ad2ee54c1b",
      "795624398eff49b88b2fd25cc134a94b",
      "10f4e687c702412b9328721f7d108775",
      "5bd891b216dc4c7490f60c359380d9ef",
      "0ef17278a6404fa8a62426018f4c55c5",
      "7f35e2f4d5c04b46ac8805ab68892e49",
      "69ca83feeae7407ca38946220c87776c",
      "4562472b55534a8388c795274d7ad3df",
      "ae7526dc91244faa9b8038099011a4a9",
      "d1b971fdbf984be6b8e754470d5f31ed",
      "aa5d508dd49c4c90a3f9c4eaeec42940",
      "0dcf92aa2f71410eaf376aef8398544b",
      "b7dc5ba16ffd42e183bc46652d83d3a8",
      "4028f4b3039041f09f045be7597cbf5e",
      "24f9d98b1144434f9728cb892bbe5a57",
      "c229769bef0448eaa1df231b5edd004e",
      "c4ca5bc8add440c589e31585f5a032eb",
      "d6a79c29c9f74f3ba68b1545a99316f0",
      "c059e2d6eb8249f8be8bb53ad28b4836",
      "cd92affdb92a4373b85c6090a1200738",
      "71cf530e8e4640e6a4ab7df00526ef8b",
      "4fac2d3787f14137831a1d2ae28d6a51",
      "1b667b075daf4192a25073eddd553eba",
      "359dafc92dc641189df3cbe8c9134e8d",
      "d6ff9697bd854695a969191c0d042c59",
      "eca7eb5e89a7497791430c8a0d1f2b6a",
      "dc9e15dde8fe4f2d92c9608ce469de8e",
      "1513c05d3fc444158f9846cf35a278bf",
      "50d26c2261a440df95755c7a78a652be",
      "1c2280c9d5d04fc189dad0bae5da410d",
      "5422cfafd6a84bef9b3274bfae46cef3",
      "9f120ed46e5a45c3ba63728aea3efddc",
      "79eeae5cf68b4e5396768ac1e7ad3a7f",
      "37c499faabdd4dd089422bdc5b29e222",
      "81dc769b83434651b35ce174779bf552",
      "b73e74f181964c52a8435d6e3e12f0ca",
      "c0f38d9f7e8d44e4b223cc1a02b7e511",
      "2cd4dd7b81e24a8da7e10bfd2e587015",
      "3e7bf6fe65b6477ba9209c098ecb6e4a",
      "e531e302fad84ed4ad07fd5bfa31291e",
      "ca8d606255574d7684e0ae9748790f20",
      "60d90a04e26949f899b826dfdf83911f",
      "a55d198112d6412fa25ee7f573bf8b3c",
      "4fe8d79cb34e4e4299b1b6f27fe9321e",
      "7a0f7654645f4e4e85648bf7cbb09cf3",
      "6fff2d899e20448aa24b52d1c2fb02f0",
      "4da36fb2d8ab479d8c50ba4901bb5dd8",
      "448c6dc1aa52414d97a8c0e496eb81bc",
      "a6e9a23ee04744c3b717a7505f6e7477",
      "510e589f867b4cc98940189b12aadc26",
      "37d432712afe4019953bbb6ff8364d93",
      "2534578ad5cb43b398c4ca6f454c97b4",
      "f2b59bb8097944b38d2c4458410fcd77",
      "a3077e3363114e139503e2649ac1e0b0",
      "69dbeecb9cd644658b1d6281ba555530",
      "2395d95124644939b178b5736102ceb0",
      "493eedfb5d3f45d4bb1302cdcc442abe",
      "df759809acbc49799803afb4b98acbf6",
      "03e46deb9d7946228d4297ed41721e6a",
      "368fe9934cf8421fac34f0715dfe24e8",
      "7ebc437481f24669b55ca9e2d4f59436",
      "7c9ec8d326104d7fbe6787b1947b71f5",
      "bbc241452200465492d5d141d5ca646d",
      "18deffe351864ce89c7e31109faa7f34",
      "c559c50da04b46d1bd98be776e5d5482",
      "25e62b12b6d14aa88bbd7dd82ab328f4"
     ]
    },
    "id": "z8Hs5Y3H-_zI",
    "outputId": "091254d7-d9f2-46cf-ed59-18fbb66173db"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1c9d2bdef44eb8bd5050c0619d9e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.817529\n",
      "epoch: 0, train loss: 2.7279, lr: 0.000050, spent: 0.3 secs\n",
      "epoch: 1, test auc: 0.811782\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 1, train loss: 2.6094, lr: 0.000050, spent: 0.4 secs\n",
      "epoch: 2, test auc: 0.807471\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 2, train loss: 2.5407, lr: 0.000050, spent: 0.5 secs\n",
      "epoch: 3, test auc: 0.803161\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 3, train loss: 2.5210, lr: 0.000050, spent: 0.6 secs\n",
      "epoch: 4, test auc: 0.804598\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 4, train loss: 2.3840, lr: 0.000050, spent: 0.7 secs\n",
      "epoch: 5, test auc: 0.810345\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 5, train loss: 2.3702, lr: 0.000050, spent: 0.8 secs\n",
      "epoch: 6, test auc: 0.811782\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 6, train loss: 2.2940, lr: 0.000050, spent: 0.9 secs\n",
      "epoch: 7, test auc: 0.811782\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 7, train loss: 2.2251, lr: 0.000050, spent: 1.0 secs\n",
      "epoch: 8, test auc: 0.811782\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 8, train loss: 2.2059, lr: 0.000050, spent: 1.1 secs\n",
      "epoch: 9, test auc: 0.817529\n",
      "epoch: 9, train loss: 2.1182, lr: 0.000050, spent: 1.2 secs\n",
      "epoch: 10, test auc: 0.820402\n",
      "epoch: 10, train loss: 2.1092, lr: 0.000050, spent: 1.4 secs\n",
      "epoch: 11, test auc: 0.824713\n",
      "epoch: 11, train loss: 2.1265, lr: 0.000050, spent: 1.5 secs\n",
      "epoch: 12, test auc: 0.833333\n",
      "epoch: 12, train loss: 2.0308, lr: 0.000050, spent: 1.6 secs\n",
      "epoch: 13, test auc: 0.836207\n",
      "epoch: 13, train loss: 2.0022, lr: 0.000050, spent: 1.7 secs\n",
      "epoch: 14, test auc: 0.846264\n",
      "epoch: 14, train loss: 1.8856, lr: 0.000050, spent: 1.9 secs\n",
      "epoch: 15, test auc: 0.853448\n",
      "epoch: 15, train loss: 2.0096, lr: 0.000050, spent: 2.0 secs\n",
      "epoch: 16, test auc: 0.853448\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 16, train loss: 1.8408, lr: 0.000050, spent: 2.1 secs\n",
      "epoch: 17, test auc: 0.852011\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 17, train loss: 1.8685, lr: 0.000050, spent: 2.2 secs\n",
      "epoch: 18, test auc: 0.850575\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 18, train loss: 1.7300, lr: 0.000050, spent: 2.3 secs\n",
      "epoch: 19, test auc: 0.866379\n",
      "epoch: 19, train loss: 1.7880, lr: 0.000050, spent: 2.4 secs\n",
      "epoch: 20, test auc: 0.876437\n",
      "epoch: 20, train loss: 1.7605, lr: 0.000050, spent: 2.6 secs\n",
      "epoch: 21, test auc: 0.885057\n",
      "epoch: 21, train loss: 1.7535, lr: 0.000050, spent: 2.7 secs\n",
      "epoch: 22, test auc: 0.880747\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 22, train loss: 1.6855, lr: 0.000050, spent: 3.0 secs\n",
      "epoch: 23, test auc: 0.885057\n",
      "epoch: 23, train loss: 1.6262, lr: 0.000050, spent: 3.1 secs\n",
      "epoch: 24, test auc: 0.892241\n",
      "epoch: 24, train loss: 1.5696, lr: 0.000050, spent: 3.2 secs\n",
      "epoch: 25, test auc: 0.895115\n",
      "epoch: 25, train loss: 1.5045, lr: 0.000050, spent: 3.4 secs\n",
      "epoch: 26, test auc: 0.892241\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 26, train loss: 1.5717, lr: 0.000050, spent: 3.5 secs\n",
      "epoch: 27, test auc: 0.892241\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 27, train loss: 1.4666, lr: 0.000050, spent: 3.6 secs\n",
      "epoch: 28, test auc: 0.896552\n",
      "epoch: 28, train loss: 1.4662, lr: 0.000050, spent: 3.7 secs\n",
      "epoch: 29, test auc: 0.895115\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 29, train loss: 1.4990, lr: 0.000050, spent: 3.8 secs\n",
      "epoch: 30, test auc: 0.895115\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 30, train loss: 1.4715, lr: 0.000050, spent: 3.9 secs\n",
      "epoch: 31, test auc: 0.897989\n",
      "epoch: 31, train loss: 1.5396, lr: 0.000050, spent: 4.0 secs\n",
      "epoch: 32, test auc: 0.895115\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 32, train loss: 1.5938, lr: 0.000050, spent: 4.1 secs\n",
      "epoch: 33, test auc: 0.900862\n",
      "epoch: 33, train loss: 1.4864, lr: 0.000050, spent: 4.2 secs\n",
      "epoch: 34, test auc: 0.897989\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 34, train loss: 1.6007, lr: 0.000050, spent: 4.3 secs\n",
      "epoch: 35, test auc: 0.899425\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 35, train loss: 1.4711, lr: 0.000050, spent: 4.4 secs\n",
      "epoch: 36, test auc: 0.903736\n",
      "epoch: 36, train loss: 1.5129, lr: 0.000050, spent: 4.5 secs\n",
      "epoch: 37, test auc: 0.896552\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 37, train loss: 1.3701, lr: 0.000050, spent: 4.6 secs\n",
      "epoch: 38, test auc: 0.892241\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 38, train loss: 1.4286, lr: 0.000050, spent: 4.7 secs\n",
      "epoch: 39, test auc: 0.893678\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 39, train loss: 1.4570, lr: 0.000050, spent: 5.0 secs\n",
      "epoch: 40, test auc: 0.890805\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 40, train loss: 1.3330, lr: 0.000050, spent: 5.1 secs\n",
      "epoch: 41, test auc: 0.899425\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 41, train loss: 1.4978, lr: 0.000050, spent: 5.2 secs\n",
      "epoch: 42, test auc: 0.897989\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 42, train loss: 1.3591, lr: 0.000050, spent: 5.3 secs\n",
      "epoch: 43, test auc: 0.897989\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 43, train loss: 1.3235, lr: 0.000050, spent: 5.4 secs\n",
      "epoch: 44, test auc: 0.896552\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 44, train loss: 1.3514, lr: 0.000050, spent: 5.5 secs\n",
      "epoch: 45, test auc: 0.897989\n",
      "EarlyStopping counter: 9 out of 15\n",
      "epoch: 45, train loss: 1.4257, lr: 0.000050, spent: 5.6 secs\n",
      "epoch: 46, test auc: 0.899425\n",
      "EarlyStopping counter: 10 out of 15\n",
      "epoch: 46, train loss: 1.3177, lr: 0.000050, spent: 5.7 secs\n",
      "epoch: 47, test auc: 0.893678\n",
      "EarlyStopping counter: 11 out of 15\n",
      "epoch: 47, train loss: 1.4361, lr: 0.000050, spent: 5.8 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:21:48.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./pretrain_checkpoint1\u001b[0m\n",
      "\u001b[32m2025-10-16 20:21:48.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./pretrain_checkpoint1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 48, test auc: 0.889368\n",
      "EarlyStopping counter: 12 out of 15\n",
      "epoch: 48, train loss: 1.2915, lr: 0.000050, spent: 5.9 secs\n",
      "epoch: 49, test auc: 0.885057\n",
      "EarlyStopping counter: 13 out of 15\n",
      "epoch: 49, train loss: 1.3999, lr: 0.000050, spent: 6.0 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:21:48.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 6.2 secs.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3062b303959a4e319c803194f7a4f4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.808908\n",
      "epoch: 0, train loss: 2.7217, lr: 0.000050, spent: 0.1 secs\n",
      "epoch: 1, test auc: 0.827586\n",
      "epoch: 1, train loss: 2.6936, lr: 0.000050, spent: 0.3 secs\n",
      "epoch: 2, test auc: 0.827586\n",
      "epoch: 2, train loss: 2.6019, lr: 0.000050, spent: 0.4 secs\n",
      "epoch: 3, test auc: 0.821839\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 3, train loss: 2.5609, lr: 0.000050, spent: 0.5 secs\n",
      "epoch: 4, test auc: 0.818966\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 4, train loss: 2.4978, lr: 0.000050, spent: 0.6 secs\n",
      "epoch: 5, test auc: 0.820402\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 5, train loss: 2.4455, lr: 0.000050, spent: 0.7 secs\n",
      "epoch: 6, test auc: 0.824713\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 6, train loss: 2.4415, lr: 0.000050, spent: 1.0 secs\n",
      "epoch: 7, test auc: 0.826149\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 7, train loss: 2.3717, lr: 0.000050, spent: 1.1 secs\n",
      "epoch: 8, test auc: 0.826149\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 8, train loss: 2.3049, lr: 0.000050, spent: 1.2 secs\n",
      "epoch: 9, test auc: 0.824713\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 9, train loss: 2.2097, lr: 0.000050, spent: 1.3 secs\n",
      "epoch: 10, test auc: 0.830460\n",
      "epoch: 10, train loss: 2.3725, lr: 0.000050, spent: 1.4 secs\n",
      "epoch: 11, test auc: 0.834770\n",
      "epoch: 11, train loss: 2.2925, lr: 0.000050, spent: 1.5 secs\n",
      "epoch: 12, test auc: 0.839080\n",
      "epoch: 12, train loss: 2.2230, lr: 0.000050, spent: 1.7 secs\n",
      "epoch: 13, test auc: 0.840517\n",
      "epoch: 13, train loss: 2.2092, lr: 0.000050, spent: 1.8 secs\n",
      "epoch: 14, test auc: 0.836207\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 14, train loss: 2.2065, lr: 0.000050, spent: 1.9 secs\n",
      "epoch: 15, test auc: 0.837644\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 15, train loss: 2.0410, lr: 0.000050, spent: 2.0 secs\n",
      "epoch: 16, test auc: 0.839080\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 16, train loss: 1.9989, lr: 0.000050, spent: 2.1 secs\n",
      "epoch: 17, test auc: 0.840517\n",
      "epoch: 17, train loss: 2.1040, lr: 0.000050, spent: 2.2 secs\n",
      "epoch: 18, test auc: 0.846264\n",
      "epoch: 18, train loss: 2.0231, lr: 0.000050, spent: 2.3 secs\n",
      "epoch: 19, test auc: 0.849138\n",
      "epoch: 19, train loss: 1.9767, lr: 0.000050, spent: 2.5 secs\n",
      "epoch: 20, test auc: 0.853448\n",
      "epoch: 20, train loss: 2.0034, lr: 0.000050, spent: 2.6 secs\n",
      "epoch: 21, test auc: 0.849138\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 21, train loss: 1.9933, lr: 0.000050, spent: 2.7 secs\n",
      "epoch: 22, test auc: 0.849138\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 22, train loss: 1.9439, lr: 0.000050, spent: 2.8 secs\n",
      "epoch: 23, test auc: 0.844828\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 23, train loss: 1.8684, lr: 0.000050, spent: 2.9 secs\n",
      "epoch: 24, test auc: 0.849138\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 24, train loss: 1.8844, lr: 0.000050, spent: 3.0 secs\n",
      "epoch: 25, test auc: 0.846264\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 25, train loss: 1.9131, lr: 0.000050, spent: 3.1 secs\n",
      "epoch: 26, test auc: 0.841954\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 26, train loss: 1.7811, lr: 0.000050, spent: 3.3 secs\n",
      "epoch: 27, test auc: 0.846264\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 27, train loss: 1.8439, lr: 0.000050, spent: 3.4 secs\n",
      "epoch: 28, test auc: 0.846264\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 28, train loss: 1.7198, lr: 0.000050, spent: 3.5 secs\n",
      "epoch: 29, test auc: 0.844828\n",
      "EarlyStopping counter: 9 out of 15\n",
      "epoch: 29, train loss: 1.6525, lr: 0.000050, spent: 3.6 secs\n",
      "epoch: 30, test auc: 0.847701\n",
      "EarlyStopping counter: 10 out of 15\n",
      "epoch: 30, train loss: 1.7850, lr: 0.000050, spent: 3.7 secs\n",
      "epoch: 31, test auc: 0.846264\n",
      "EarlyStopping counter: 11 out of 15\n",
      "epoch: 31, train loss: 1.7088, lr: 0.000050, spent: 3.8 secs\n",
      "epoch: 32, test auc: 0.849138\n",
      "EarlyStopping counter: 12 out of 15\n",
      "epoch: 32, train loss: 1.9048, lr: 0.000050, spent: 3.9 secs\n",
      "epoch: 33, test auc: 0.850575\n",
      "EarlyStopping counter: 13 out of 15\n",
      "epoch: 33, train loss: 1.6568, lr: 0.000050, spent: 4.0 secs\n",
      "epoch: 34, test auc: 0.856322\n",
      "epoch: 34, train loss: 1.6850, lr: 0.000050, spent: 4.1 secs\n",
      "epoch: 35, test auc: 0.860632\n",
      "epoch: 35, train loss: 1.7275, lr: 0.000050, spent: 4.3 secs\n",
      "epoch: 36, test auc: 0.875000\n",
      "epoch: 36, train loss: 1.5790, lr: 0.000050, spent: 4.4 secs\n",
      "epoch: 37, test auc: 0.873563\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 37, train loss: 1.5973, lr: 0.000050, spent: 4.5 secs\n",
      "epoch: 38, test auc: 0.873563\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 38, train loss: 1.7110, lr: 0.000050, spent: 4.6 secs\n",
      "epoch: 39, test auc: 0.875000\n",
      "epoch: 39, train loss: 1.6399, lr: 0.000050, spent: 4.7 secs\n",
      "epoch: 40, test auc: 0.876437\n",
      "epoch: 40, train loss: 1.6855, lr: 0.000050, spent: 4.9 secs\n",
      "epoch: 41, test auc: 0.870690\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 41, train loss: 1.5738, lr: 0.000050, spent: 5.2 secs\n",
      "epoch: 42, test auc: 0.870690\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 42, train loss: 1.6384, lr: 0.000050, spent: 5.3 secs\n",
      "epoch: 43, test auc: 0.869253\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 43, train loss: 1.6465, lr: 0.000050, spent: 5.4 secs\n",
      "epoch: 44, test auc: 0.867816\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 44, train loss: 1.5406, lr: 0.000050, spent: 5.5 secs\n",
      "epoch: 45, test auc: 0.873563\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 45, train loss: 1.7328, lr: 0.000050, spent: 5.5 secs\n",
      "epoch: 46, test auc: 0.883621\n",
      "epoch: 46, train loss: 1.4179, lr: 0.000050, spent: 5.7 secs\n",
      "epoch: 47, test auc: 0.880747\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 47, train loss: 1.4564, lr: 0.000050, spent: 5.8 secs\n",
      "epoch: 48, test auc: 0.880747\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 48, train loss: 1.4737, lr: 0.000050, spent: 5.9 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:21:54.719\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./pretrain_checkpoint1\u001b[0m\n",
      "\u001b[32m2025-10-16 20:21:54.737\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./pretrain_checkpoint1\u001b[0m\n",
      "\u001b[32m2025-10-16 20:21:54.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 6.1 secs.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49, test auc: 0.872126\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 49, train loss: 1.4519, lr: 0.000050, spent: 6.0 secs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84cd794fe16c4e178c698f9277014f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.650862\n",
      "epoch: 0, train loss: 2.8417, lr: 0.000050, spent: 0.1 secs\n",
      "epoch: 1, test auc: 0.659483\n",
      "epoch: 1, train loss: 2.7570, lr: 0.000050, spent: 0.3 secs\n",
      "epoch: 2, test auc: 0.639368\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 2, train loss: 2.6564, lr: 0.000050, spent: 0.4 secs\n",
      "epoch: 3, test auc: 0.630747\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 3, train loss: 2.6082, lr: 0.000050, spent: 0.5 secs\n",
      "epoch: 4, test auc: 0.623563\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 4, train loss: 2.5585, lr: 0.000050, spent: 0.6 secs\n",
      "epoch: 5, test auc: 0.616379\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 5, train loss: 2.4568, lr: 0.000050, spent: 0.6 secs\n",
      "epoch: 6, test auc: 0.617816\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 6, train loss: 2.4423, lr: 0.000050, spent: 0.7 secs\n",
      "epoch: 7, test auc: 0.614943\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 7, train loss: 2.3292, lr: 0.000050, spent: 0.8 secs\n",
      "epoch: 8, test auc: 0.616379\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 8, train loss: 2.2415, lr: 0.000050, spent: 0.9 secs\n",
      "epoch: 9, test auc: 0.619253\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 9, train loss: 2.2160, lr: 0.000050, spent: 1.0 secs\n",
      "epoch: 10, test auc: 0.617816\n",
      "EarlyStopping counter: 9 out of 15\n",
      "epoch: 10, train loss: 2.0992, lr: 0.000050, spent: 1.1 secs\n",
      "epoch: 11, test auc: 0.626437\n",
      "EarlyStopping counter: 10 out of 15\n",
      "epoch: 11, train loss: 2.0607, lr: 0.000050, spent: 1.4 secs\n",
      "epoch: 12, test auc: 0.620690\n",
      "EarlyStopping counter: 11 out of 15\n",
      "epoch: 12, train loss: 1.9189, lr: 0.000050, spent: 1.5 secs\n",
      "epoch: 13, test auc: 0.617816\n",
      "EarlyStopping counter: 12 out of 15\n",
      "epoch: 13, train loss: 1.8391, lr: 0.000050, spent: 1.6 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:21:56.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./pretrain_checkpoint1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, test auc: 0.627874\n",
      "EarlyStopping counter: 13 out of 15\n",
      "epoch: 14, train loss: 1.9015, lr: 0.000050, spent: 1.7 secs\n",
      "epoch: 15, test auc: 0.636494\n",
      "EarlyStopping counter: 14 out of 15\n",
      "epoch: 15, train loss: 1.7887, lr: 0.000050, spent: 1.8 secs\n",
      "epoch: 16, test auc: 0.633621\n",
      "EarlyStopping counter: 15 out of 15\n",
      "early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:21:56.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./pretrain_checkpoint1\u001b[0m\n",
      "\u001b[32m2025-10-16 20:21:57.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 2.1 secs.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f0db2fb93e42a0b0b158da41f777fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.862319\n",
      "epoch: 0, train loss: 2.8986, lr: 0.000050, spent: 0.1 secs\n",
      "epoch: 1, test auc: 0.860870\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 1, train loss: 2.6041, lr: 0.000050, spent: 0.2 secs\n",
      "epoch: 2, test auc: 0.866667\n",
      "epoch: 2, train loss: 2.5971, lr: 0.000050, spent: 0.4 secs\n",
      "epoch: 3, test auc: 0.868116\n",
      "epoch: 3, train loss: 2.5381, lr: 0.000050, spent: 0.5 secs\n",
      "epoch: 4, test auc: 0.868116\n",
      "epoch: 4, train loss: 2.4853, lr: 0.000050, spent: 0.6 secs\n",
      "epoch: 5, test auc: 0.868116\n",
      "epoch: 5, train loss: 2.4853, lr: 0.000050, spent: 0.7 secs\n",
      "epoch: 6, test auc: 0.872464\n",
      "epoch: 6, train loss: 2.4222, lr: 0.000050, spent: 0.9 secs\n",
      "epoch: 7, test auc: 0.872464\n",
      "epoch: 7, train loss: 2.3709, lr: 0.000050, spent: 1.0 secs\n",
      "epoch: 8, test auc: 0.875362\n",
      "epoch: 8, train loss: 2.3206, lr: 0.000050, spent: 1.1 secs\n",
      "epoch: 9, test auc: 0.878261\n",
      "epoch: 9, train loss: 2.3255, lr: 0.000050, spent: 1.2 secs\n",
      "epoch: 10, test auc: 0.876812\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 10, train loss: 2.2810, lr: 0.000050, spent: 1.3 secs\n",
      "epoch: 11, test auc: 0.875362\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 11, train loss: 2.2322, lr: 0.000050, spent: 1.4 secs\n",
      "epoch: 12, test auc: 0.873913\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 12, train loss: 2.1710, lr: 0.000050, spent: 1.7 secs\n",
      "epoch: 13, test auc: 0.875362\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 13, train loss: 2.0837, lr: 0.000050, spent: 1.8 secs\n",
      "epoch: 14, test auc: 0.876812\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 14, train loss: 2.1033, lr: 0.000050, spent: 1.9 secs\n",
      "epoch: 15, test auc: 0.882609\n",
      "epoch: 15, train loss: 2.0475, lr: 0.000050, spent: 2.0 secs\n",
      "epoch: 16, test auc: 0.886957\n",
      "epoch: 16, train loss: 2.0757, lr: 0.000050, spent: 2.2 secs\n",
      "epoch: 17, test auc: 0.889855\n",
      "epoch: 17, train loss: 1.9672, lr: 0.000050, spent: 2.3 secs\n",
      "epoch: 18, test auc: 0.897101\n",
      "epoch: 18, train loss: 1.9805, lr: 0.000050, spent: 2.4 secs\n",
      "epoch: 19, test auc: 0.895652\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 19, train loss: 1.8847, lr: 0.000050, spent: 2.5 secs\n",
      "epoch: 20, test auc: 0.888406\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 20, train loss: 1.7740, lr: 0.000050, spent: 2.6 secs\n",
      "epoch: 21, test auc: 0.891304\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 21, train loss: 1.7999, lr: 0.000050, spent: 2.7 secs\n",
      "epoch: 22, test auc: 0.898551\n",
      "epoch: 22, train loss: 1.8291, lr: 0.000050, spent: 2.8 secs\n",
      "epoch: 23, test auc: 0.904348\n",
      "epoch: 23, train loss: 1.7945, lr: 0.000050, spent: 2.9 secs\n",
      "epoch: 24, test auc: 0.905797\n",
      "epoch: 24, train loss: 1.7157, lr: 0.000050, spent: 3.1 secs\n",
      "epoch: 25, test auc: 0.904348\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 25, train loss: 1.7142, lr: 0.000050, spent: 3.2 secs\n",
      "epoch: 26, test auc: 0.904348\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 26, train loss: 1.6802, lr: 0.000050, spent: 3.3 secs\n",
      "epoch: 27, test auc: 0.901449\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 27, train loss: 1.7537, lr: 0.000050, spent: 3.4 secs\n",
      "epoch: 28, test auc: 0.904348\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 28, train loss: 1.6736, lr: 0.000050, spent: 3.5 secs\n",
      "epoch: 29, test auc: 0.901449\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 29, train loss: 1.6467, lr: 0.000050, spent: 3.7 secs\n",
      "epoch: 30, test auc: 0.905797\n",
      "epoch: 30, train loss: 1.5574, lr: 0.000050, spent: 3.9 secs\n",
      "epoch: 31, test auc: 0.905797\n",
      "epoch: 31, train loss: 1.6080, lr: 0.000050, spent: 4.0 secs\n",
      "epoch: 32, test auc: 0.911594\n",
      "epoch: 32, train loss: 1.5282, lr: 0.000050, spent: 4.1 secs\n",
      "epoch: 33, test auc: 0.913043\n",
      "epoch: 33, train loss: 1.5406, lr: 0.000050, spent: 4.3 secs\n",
      "epoch: 34, test auc: 0.913043\n",
      "epoch: 34, train loss: 1.6191, lr: 0.000050, spent: 4.4 secs\n",
      "epoch: 35, test auc: 0.911594\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 35, train loss: 1.5710, lr: 0.000050, spent: 4.5 secs\n",
      "epoch: 36, test auc: 0.911594\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 36, train loss: 1.4606, lr: 0.000050, spent: 4.6 secs\n",
      "epoch: 37, test auc: 0.910145\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 37, train loss: 1.4415, lr: 0.000050, spent: 4.7 secs\n",
      "epoch: 38, test auc: 0.908696\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 38, train loss: 1.5261, lr: 0.000050, spent: 4.8 secs\n",
      "epoch: 39, test auc: 0.910145\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 39, train loss: 1.6163, lr: 0.000050, spent: 4.9 secs\n",
      "epoch: 40, test auc: 0.914493\n",
      "epoch: 40, train loss: 1.4941, lr: 0.000050, spent: 5.0 secs\n",
      "epoch: 41, test auc: 0.907246\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 41, train loss: 1.5055, lr: 0.000050, spent: 5.1 secs\n",
      "epoch: 42, test auc: 0.898551\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 42, train loss: 1.5549, lr: 0.000050, spent: 5.2 secs\n",
      "epoch: 43, test auc: 0.905797\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 43, train loss: 1.4144, lr: 0.000050, spent: 5.3 secs\n",
      "epoch: 44, test auc: 0.907246\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 44, train loss: 1.4448, lr: 0.000050, spent: 5.4 secs\n",
      "epoch: 45, test auc: 0.892754\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 45, train loss: 1.4863, lr: 0.000050, spent: 5.5 secs\n",
      "epoch: 46, test auc: 0.895652\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 46, train loss: 1.4310, lr: 0.000050, spent: 5.7 secs\n",
      "epoch: 47, test auc: 0.895652\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 47, train loss: 1.5449, lr: 0.000050, spent: 5.8 secs\n",
      "epoch: 48, test auc: 0.892754\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 48, train loss: 1.3685, lr: 0.000050, spent: 5.9 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:22:03.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./pretrain_checkpoint1\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:03.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./pretrain_checkpoint1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49, test auc: 0.881159\n",
      "EarlyStopping counter: 9 out of 15\n",
      "epoch: 49, train loss: 1.4785, lr: 0.000050, spent: 6.0 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:22:03.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 6.3 secs.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d4573850314b46b082a16bc9b29067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.376812\n",
      "epoch: 0, train loss: 2.8697, lr: 0.000050, spent: 0.1 secs\n",
      "epoch: 1, test auc: 0.568116\n",
      "epoch: 1, train loss: 2.7563, lr: 0.000050, spent: 0.3 secs\n",
      "epoch: 2, test auc: 0.681159\n",
      "epoch: 2, train loss: 2.7447, lr: 0.000050, spent: 0.4 secs\n",
      "epoch: 3, test auc: 0.753623\n",
      "epoch: 3, train loss: 2.6745, lr: 0.000050, spent: 0.5 secs\n",
      "epoch: 4, test auc: 0.808696\n",
      "epoch: 4, train loss: 2.6183, lr: 0.000050, spent: 0.6 secs\n",
      "epoch: 5, test auc: 0.821739\n",
      "epoch: 5, train loss: 2.6628, lr: 0.000050, spent: 0.8 secs\n",
      "epoch: 6, test auc: 0.830435\n",
      "epoch: 6, train loss: 2.6130, lr: 0.000050, spent: 0.9 secs\n",
      "epoch: 7, test auc: 0.850725\n",
      "epoch: 7, train loss: 2.5493, lr: 0.000050, spent: 1.0 secs\n",
      "epoch: 8, test auc: 0.863768\n",
      "epoch: 8, train loss: 2.4944, lr: 0.000050, spent: 1.2 secs\n",
      "epoch: 9, test auc: 0.876812\n",
      "epoch: 9, train loss: 2.4804, lr: 0.000050, spent: 1.3 secs\n",
      "epoch: 10, test auc: 0.886957\n",
      "epoch: 10, train loss: 2.4805, lr: 0.000050, spent: 1.4 secs\n",
      "epoch: 11, test auc: 0.891304\n",
      "epoch: 11, train loss: 2.3561, lr: 0.000050, spent: 1.5 secs\n",
      "epoch: 12, test auc: 0.900000\n",
      "epoch: 12, train loss: 2.3790, lr: 0.000050, spent: 1.7 secs\n",
      "epoch: 13, test auc: 0.913043\n",
      "epoch: 13, train loss: 2.2703, lr: 0.000050, spent: 1.8 secs\n",
      "epoch: 14, test auc: 0.918841\n",
      "epoch: 14, train loss: 2.2649, lr: 0.000050, spent: 1.9 secs\n",
      "epoch: 15, test auc: 0.918841\n",
      "epoch: 15, train loss: 2.2391, lr: 0.000050, spent: 2.3 secs\n",
      "epoch: 16, test auc: 0.920290\n",
      "epoch: 16, train loss: 2.1925, lr: 0.000050, spent: 2.4 secs\n",
      "epoch: 17, test auc: 0.923188\n",
      "epoch: 17, train loss: 2.0987, lr: 0.000050, spent: 2.5 secs\n",
      "epoch: 18, test auc: 0.923188\n",
      "epoch: 18, train loss: 2.1872, lr: 0.000050, spent: 2.6 secs\n",
      "epoch: 19, test auc: 0.928986\n",
      "epoch: 19, train loss: 2.0744, lr: 0.000050, spent: 2.8 secs\n",
      "epoch: 20, test auc: 0.936232\n",
      "epoch: 20, train loss: 2.0860, lr: 0.000050, spent: 2.9 secs\n",
      "epoch: 21, test auc: 0.923188\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 21, train loss: 2.0227, lr: 0.000050, spent: 3.0 secs\n",
      "epoch: 22, test auc: 0.917391\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 22, train loss: 2.0058, lr: 0.000050, spent: 3.1 secs\n",
      "epoch: 23, test auc: 0.921739\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 23, train loss: 1.9184, lr: 0.000050, spent: 3.2 secs\n",
      "epoch: 24, test auc: 0.915942\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 24, train loss: 1.8972, lr: 0.000050, spent: 3.3 secs\n",
      "epoch: 25, test auc: 0.907246\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 25, train loss: 1.9198, lr: 0.000050, spent: 3.4 secs\n",
      "epoch: 26, test auc: 0.915942\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 26, train loss: 1.8429, lr: 0.000050, spent: 3.5 secs\n",
      "epoch: 27, test auc: 0.923188\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 27, train loss: 1.9216, lr: 0.000050, spent: 3.5 secs\n",
      "epoch: 28, test auc: 0.921739\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 28, train loss: 1.8308, lr: 0.000050, spent: 3.6 secs\n",
      "epoch: 29, test auc: 0.921739\n",
      "EarlyStopping counter: 9 out of 15\n",
      "epoch: 29, train loss: 1.7661, lr: 0.000050, spent: 3.7 secs\n",
      "epoch: 30, test auc: 0.923188\n",
      "EarlyStopping counter: 10 out of 15\n",
      "epoch: 30, train loss: 1.7904, lr: 0.000050, spent: 3.8 secs\n",
      "epoch: 31, test auc: 0.907246\n",
      "EarlyStopping counter: 11 out of 15\n",
      "epoch: 31, train loss: 1.8833, lr: 0.000050, spent: 4.1 secs\n",
      "epoch: 32, test auc: 0.915942\n",
      "EarlyStopping counter: 12 out of 15\n",
      "epoch: 32, train loss: 1.6839, lr: 0.000050, spent: 4.2 secs\n",
      "epoch: 33, test auc: 0.917391\n",
      "EarlyStopping counter: 13 out of 15\n",
      "epoch: 33, train loss: 1.6677, lr: 0.000050, spent: 4.3 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:22:08.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./pretrain_checkpoint1\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:08.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./pretrain_checkpoint1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34, test auc: 0.913043\n",
      "EarlyStopping counter: 14 out of 15\n",
      "epoch: 34, train loss: 1.7057, lr: 0.000050, spent: 4.4 secs\n",
      "epoch: 35, test auc: 0.917391\n",
      "EarlyStopping counter: 15 out of 15\n",
      "early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:22:08.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 4.7 secs.\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:08.352\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36m_check_column_overlap\u001b[0m:\u001b[36m254\u001b[0m - \u001b[33m\u001b[1mNo cat/num/bin cols specified, will take ALL columns as categorical! Ignore this warning if you specify the `checkpoint` to load the model.\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:08.424\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m774\u001b[0m - \u001b[1mmissing keys: []\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:08.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m775\u001b[0m - \u001b[1munexpected keys: []\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:08.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m776\u001b[0m - \u001b[1mload model from /home/kutaytire/RL_training/pretrain_checkpoint1\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:08.456\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mload feature extractor from /home/kutaytire/RL_training/pretrain_checkpoint1/extractor/extractor.json\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ca70bfd6d04b45a9d671283a079cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.936343\n",
      "epoch: 0, train loss: 2.7731, lr: 0.000100, spent: 0.2 secs\n",
      "epoch: 1, test auc: 0.944444\n",
      "epoch: 1, train loss: 2.6060, lr: 0.000100, spent: 0.4 secs\n",
      "epoch: 2, test auc: 0.939815\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 2, train loss: 2.4853, lr: 0.000100, spent: 0.5 secs\n",
      "epoch: 3, test auc: 0.940972\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 3, train loss: 2.4416, lr: 0.000100, spent: 0.7 secs\n",
      "epoch: 4, test auc: 0.943287\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 4, train loss: 2.3743, lr: 0.000100, spent: 0.8 secs\n",
      "epoch: 5, test auc: 0.944444\n",
      "epoch: 5, train loss: 2.3037, lr: 0.000100, spent: 1.2 secs\n",
      "epoch: 6, test auc: 0.945602\n",
      "epoch: 6, train loss: 2.2453, lr: 0.000100, spent: 1.3 secs\n",
      "epoch: 7, test auc: 0.949074\n",
      "epoch: 7, train loss: 2.1772, lr: 0.000100, spent: 1.5 secs\n",
      "epoch: 8, test auc: 0.950231\n",
      "epoch: 8, train loss: 2.0909, lr: 0.000100, spent: 1.7 secs\n",
      "epoch: 9, test auc: 0.952546\n",
      "epoch: 9, train loss: 2.0464, lr: 0.000100, spent: 1.9 secs\n",
      "epoch: 10, test auc: 0.952546\n",
      "epoch: 10, train loss: 1.9987, lr: 0.000100, spent: 2.1 secs\n",
      "epoch: 11, test auc: 0.950231\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 11, train loss: 2.0350, lr: 0.000100, spent: 2.2 secs\n",
      "epoch: 12, test auc: 0.952546\n",
      "epoch: 12, train loss: 1.9157, lr: 0.000100, spent: 2.6 secs\n",
      "epoch: 13, test auc: 0.951389\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 13, train loss: 1.8491, lr: 0.000100, spent: 2.7 secs\n",
      "epoch: 14, test auc: 0.951389\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 14, train loss: 1.8395, lr: 0.000100, spent: 2.9 secs\n",
      "epoch: 15, test auc: 0.950231\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 15, train loss: 1.7861, lr: 0.000100, spent: 3.0 secs\n",
      "epoch: 16, test auc: 0.952546\n",
      "epoch: 16, train loss: 1.6877, lr: 0.000100, spent: 3.2 secs\n",
      "epoch: 17, test auc: 0.954861\n",
      "epoch: 17, train loss: 1.6796, lr: 0.000100, spent: 3.4 secs\n",
      "epoch: 18, test auc: 0.954861\n",
      "epoch: 18, train loss: 1.7046, lr: 0.000100, spent: 3.6 secs\n",
      "epoch: 19, test auc: 0.954861\n",
      "epoch: 19, train loss: 1.5878, lr: 0.000100, spent: 3.9 secs\n",
      "epoch: 20, test auc: 0.957176\n",
      "epoch: 20, train loss: 1.5208, lr: 0.000100, spent: 4.1 secs\n",
      "epoch: 21, test auc: 0.956019\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 21, train loss: 1.5510, lr: 0.000100, spent: 4.3 secs\n",
      "epoch: 22, test auc: 0.957176\n",
      "epoch: 22, train loss: 1.5272, lr: 0.000100, spent: 4.5 secs\n",
      "epoch: 23, test auc: 0.952546\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 23, train loss: 1.4869, lr: 0.000100, spent: 4.6 secs\n",
      "epoch: 24, test auc: 0.957176\n",
      "epoch: 24, train loss: 1.4341, lr: 0.000100, spent: 4.9 secs\n",
      "epoch: 25, test auc: 0.956019\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 25, train loss: 1.4807, lr: 0.000100, spent: 5.1 secs\n",
      "epoch: 26, test auc: 0.956019\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 26, train loss: 1.4520, lr: 0.000100, spent: 5.5 secs\n",
      "epoch: 27, test auc: 0.950231\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 27, train loss: 1.4665, lr: 0.000100, spent: 5.6 secs\n",
      "epoch: 28, test auc: 0.944444\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch: 28, train loss: 1.3558, lr: 0.000100, spent: 5.7 secs\n",
      "epoch: 29, test auc: 0.949074\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch: 29, train loss: 1.4175, lr: 0.000100, spent: 5.9 secs\n",
      "epoch: 30, test auc: 0.945602\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch: 30, train loss: 1.3909, lr: 0.000100, spent: 6.0 secs\n",
      "epoch: 31, test auc: 0.942130\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch: 31, train loss: 1.2572, lr: 0.000100, spent: 6.2 secs\n",
      "epoch: 32, test auc: 0.943287\n",
      "EarlyStopping counter: 8 out of 10\n",
      "epoch: 32, train loss: 1.3412, lr: 0.000100, spent: 6.3 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:22:15.299\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:15.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33, test auc: 0.937500\n",
      "EarlyStopping counter: 9 out of 10\n",
      "epoch: 33, train loss: 1.3188, lr: 0.000100, spent: 6.7 secs\n",
      "epoch: 34, test auc: 0.956019\n",
      "EarlyStopping counter: 10 out of 10\n",
      "early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:22:15.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 7.1 secs.\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:15.612\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36m_check_column_overlap\u001b[0m:\u001b[36m254\u001b[0m - \u001b[33m\u001b[1mNo cat/num/bin cols specified, will take ALL columns as categorical! Ignore this warning if you specify the `checkpoint` to load the model.\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:15.686\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m774\u001b[0m - \u001b[1mmissing keys: []\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:15.687\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m775\u001b[0m - \u001b[1munexpected keys: []\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:15.688\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m776\u001b[0m - \u001b[1mload model from /home/kutaytire/RL_training/pretrain_checkpoint1\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:15.717\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mload feature extractor from /home/kutaytire/RL_training/pretrain_checkpoint1/extractor/extractor.json\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9805ebf7832433d952d76832dacee26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.811298\n",
      "epoch: 0, train loss: 2.6448, lr: 0.000100, spent: 0.2 secs\n",
      "epoch: 1, test auc: 0.777644\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 1, train loss: 2.4074, lr: 0.000100, spent: 0.3 secs\n",
      "epoch: 2, test auc: 0.770433\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 2, train loss: 2.3296, lr: 0.000100, spent: 0.5 secs\n",
      "epoch: 3, test auc: 0.795673\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 3, train loss: 2.2327, lr: 0.000100, spent: 0.6 secs\n",
      "epoch: 4, test auc: 0.811298\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch: 4, train loss: 2.1173, lr: 0.000100, spent: 0.8 secs\n",
      "epoch: 5, test auc: 0.817308\n",
      "epoch: 5, train loss: 2.0610, lr: 0.000100, spent: 1.2 secs\n",
      "epoch: 6, test auc: 0.823317\n",
      "epoch: 6, train loss: 1.9479, lr: 0.000100, spent: 1.3 secs\n",
      "epoch: 7, test auc: 0.844952\n",
      "epoch: 7, train loss: 1.9189, lr: 0.000100, spent: 1.5 secs\n",
      "epoch: 8, test auc: 0.853365\n",
      "epoch: 8, train loss: 1.8338, lr: 0.000100, spent: 1.7 secs\n",
      "epoch: 9, test auc: 0.862981\n",
      "epoch: 9, train loss: 1.7884, lr: 0.000100, spent: 1.9 secs\n",
      "epoch: 10, test auc: 0.875000\n",
      "epoch: 10, train loss: 1.8947, lr: 0.000100, spent: 2.1 secs\n",
      "epoch: 11, test auc: 0.888221\n",
      "epoch: 11, train loss: 1.7008, lr: 0.000100, spent: 2.2 secs\n",
      "epoch: 12, test auc: 0.891827\n",
      "epoch: 12, train loss: 1.6824, lr: 0.000100, spent: 2.6 secs\n",
      "epoch: 13, test auc: 0.891827\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 13, train loss: 1.6037, lr: 0.000100, spent: 2.8 secs\n",
      "epoch: 14, test auc: 0.899038\n",
      "epoch: 14, train loss: 1.5514, lr: 0.000100, spent: 3.0 secs\n",
      "epoch: 15, test auc: 0.897837\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 15, train loss: 1.5788, lr: 0.000100, spent: 3.1 secs\n",
      "epoch: 16, test auc: 0.899038\n",
      "epoch: 16, train loss: 1.5666, lr: 0.000100, spent: 3.3 secs\n",
      "epoch: 17, test auc: 0.899038\n",
      "epoch: 17, train loss: 1.4840, lr: 0.000100, spent: 3.5 secs\n",
      "epoch: 18, test auc: 0.896635\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 18, train loss: 1.4762, lr: 0.000100, spent: 3.6 secs\n",
      "epoch: 19, test auc: 0.903846\n",
      "epoch: 19, train loss: 1.4820, lr: 0.000100, spent: 4.0 secs\n",
      "epoch: 20, test auc: 0.905048\n",
      "epoch: 20, train loss: 1.4432, lr: 0.000100, spent: 4.2 secs\n",
      "epoch: 21, test auc: 0.911058\n",
      "epoch: 21, train loss: 1.3722, lr: 0.000100, spent: 4.3 secs\n",
      "epoch: 22, test auc: 0.913462\n",
      "epoch: 22, train loss: 1.3823, lr: 0.000100, spent: 4.5 secs\n",
      "epoch: 23, test auc: 0.907452\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 23, train loss: 1.3388, lr: 0.000100, spent: 4.7 secs\n",
      "epoch: 24, test auc: 0.899038\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 24, train loss: 1.3229, lr: 0.000100, spent: 4.8 secs\n",
      "epoch: 25, test auc: 0.906250\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 25, train loss: 1.3414, lr: 0.000100, spent: 5.0 secs\n",
      "epoch: 26, test auc: 0.899038\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch: 26, train loss: 1.3725, lr: 0.000100, spent: 5.3 secs\n",
      "epoch: 27, test auc: 0.900240\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch: 27, train loss: 1.3916, lr: 0.000100, spent: 5.5 secs\n",
      "epoch: 28, test auc: 0.897837\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch: 28, train loss: 1.2705, lr: 0.000100, spent: 5.6 secs\n",
      "epoch: 29, test auc: 0.890625\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch: 29, train loss: 1.2740, lr: 0.000100, spent: 5.8 secs\n",
      "epoch: 30, test auc: 0.901442\n",
      "EarlyStopping counter: 8 out of 10\n",
      "epoch: 30, train loss: 1.2825, lr: 0.000100, spent: 5.9 secs\n",
      "epoch: 31, test auc: 0.897837\n",
      "EarlyStopping counter: 9 out of 10\n",
      "epoch: 31, train loss: 1.2254, lr: 0.000100, spent: 6.0 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:22:21.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:21.961\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32, test auc: 0.896635\n",
      "EarlyStopping counter: 10 out of 10\n",
      "early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:22:22.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 6.5 secs.\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:22.262\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36m_check_column_overlap\u001b[0m:\u001b[36m254\u001b[0m - \u001b[33m\u001b[1mNo cat/num/bin cols specified, will take ALL columns as categorical! Ignore this warning if you specify the `checkpoint` to load the model.\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:22.343\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m774\u001b[0m - \u001b[1mmissing keys: []\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:22.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m775\u001b[0m - \u001b[1munexpected keys: []\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:22.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m776\u001b[0m - \u001b[1mload model from /home/kutaytire/RL_training/pretrain_checkpoint1\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:22.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mload feature extractor from /home/kutaytire/RL_training/pretrain_checkpoint1/extractor/extractor.json\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e98c8f13dc4427aac408111021ce34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.789663\n",
      "epoch: 0, train loss: 2.7441, lr: 0.000100, spent: 0.4 secs\n",
      "epoch: 1, test auc: 0.772837\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 1, train loss: 2.5255, lr: 0.000100, spent: 0.5 secs\n",
      "epoch: 2, test auc: 0.750000\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 2, train loss: 2.3610, lr: 0.000100, spent: 0.7 secs\n",
      "epoch: 3, test auc: 0.754808\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 3, train loss: 2.3534, lr: 0.000100, spent: 0.8 secs\n",
      "epoch: 4, test auc: 0.771635\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch: 4, train loss: 2.2446, lr: 0.000100, spent: 1.0 secs\n",
      "epoch: 5, test auc: 0.765625\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch: 5, train loss: 2.1252, lr: 0.000100, spent: 1.1 secs\n",
      "epoch: 6, test auc: 0.774038\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch: 6, train loss: 2.1316, lr: 0.000100, spent: 1.3 secs\n",
      "epoch: 7, test auc: 0.789663\n",
      "epoch: 7, train loss: 2.0185, lr: 0.000100, spent: 1.7 secs\n",
      "epoch: 8, test auc: 0.788462\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 8, train loss: 1.9515, lr: 0.000100, spent: 1.8 secs\n",
      "epoch: 9, test auc: 0.789663\n",
      "epoch: 9, train loss: 1.8448, lr: 0.000100, spent: 2.0 secs\n",
      "epoch: 10, test auc: 0.805288\n",
      "epoch: 10, train loss: 1.7686, lr: 0.000100, spent: 2.2 secs\n",
      "epoch: 11, test auc: 0.802885\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 11, train loss: 1.7109, lr: 0.000100, spent: 2.3 secs\n",
      "epoch: 12, test auc: 0.802885\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 12, train loss: 1.6859, lr: 0.000100, spent: 2.5 secs\n",
      "epoch: 13, test auc: 0.806490\n",
      "epoch: 13, train loss: 1.6397, lr: 0.000100, spent: 2.6 secs\n",
      "epoch: 14, test auc: 0.805288\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 14, train loss: 1.5314, lr: 0.000100, spent: 3.0 secs\n",
      "epoch: 15, test auc: 0.810096\n",
      "epoch: 15, train loss: 1.5407, lr: 0.000100, spent: 3.2 secs\n",
      "epoch: 16, test auc: 0.817308\n",
      "epoch: 16, train loss: 1.4777, lr: 0.000100, spent: 3.4 secs\n",
      "epoch: 17, test auc: 0.817308\n",
      "epoch: 17, train loss: 1.4416, lr: 0.000100, spent: 3.5 secs\n",
      "epoch: 18, test auc: 0.825721\n",
      "epoch: 18, train loss: 1.4060, lr: 0.000100, spent: 3.7 secs\n",
      "epoch: 19, test auc: 0.825721\n",
      "epoch: 19, train loss: 1.3887, lr: 0.000100, spent: 3.9 secs\n",
      "epoch: 20, test auc: 0.842548\n",
      "epoch: 20, train loss: 1.4211, lr: 0.000100, spent: 4.1 secs\n",
      "epoch: 21, test auc: 0.837740\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 21, train loss: 1.4181, lr: 0.000100, spent: 4.4 secs\n",
      "epoch: 22, test auc: 0.838942\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 22, train loss: 1.3794, lr: 0.000100, spent: 4.6 secs\n",
      "epoch: 23, test auc: 0.842548\n",
      "epoch: 23, train loss: 1.3652, lr: 0.000100, spent: 4.8 secs\n",
      "epoch: 24, test auc: 0.844952\n",
      "epoch: 24, train loss: 1.2999, lr: 0.000100, spent: 4.9 secs\n",
      "epoch: 25, test auc: 0.843750\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 25, train loss: 1.3284, lr: 0.000100, spent: 5.1 secs\n",
      "epoch: 26, test auc: 0.846154\n",
      "epoch: 26, train loss: 1.3503, lr: 0.000100, spent: 5.3 secs\n",
      "epoch: 27, test auc: 0.835337\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 27, train loss: 1.2510, lr: 0.000100, spent: 5.4 secs\n",
      "epoch: 28, test auc: 0.856971\n",
      "epoch: 28, train loss: 1.2683, lr: 0.000100, spent: 5.6 secs\n",
      "epoch: 29, test auc: 0.856971\n",
      "epoch: 29, train loss: 1.2900, lr: 0.000100, spent: 6.0 secs\n",
      "epoch: 30, test auc: 0.849760\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 30, train loss: 1.2456, lr: 0.000100, spent: 6.1 secs\n",
      "epoch: 31, test auc: 0.840144\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 31, train loss: 1.2645, lr: 0.000100, spent: 6.3 secs\n",
      "epoch: 32, test auc: 0.847356\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 32, train loss: 1.2335, lr: 0.000100, spent: 6.4 secs\n",
      "epoch: 33, test auc: 0.849760\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch: 33, train loss: 1.2405, lr: 0.000100, spent: 6.6 secs\n",
      "epoch: 34, test auc: 0.847356\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch: 34, train loss: 1.2977, lr: 0.000100, spent: 6.7 secs\n",
      "epoch: 35, test auc: 0.852163\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch: 35, train loss: 1.2350, lr: 0.000100, spent: 6.9 secs\n",
      "epoch: 36, test auc: 0.841346\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch: 36, train loss: 1.2024, lr: 0.000100, spent: 7.2 secs\n",
      "epoch: 37, test auc: 0.844952\n",
      "EarlyStopping counter: 8 out of 10\n",
      "epoch: 37, train loss: 1.2386, lr: 0.000100, spent: 7.4 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:22:30.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:30.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 38, test auc: 0.852163\n",
      "EarlyStopping counter: 9 out of 10\n",
      "epoch: 38, train loss: 1.1947, lr: 0.000100, spent: 7.5 secs\n",
      "epoch: 39, test auc: 0.849760\n",
      "EarlyStopping counter: 10 out of 10\n",
      "early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:22:30.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 7.8 secs.\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:30.308\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36m_check_column_overlap\u001b[0m:\u001b[36m254\u001b[0m - \u001b[33m\u001b[1mNo cat/num/bin cols specified, will take ALL columns as categorical! Ignore this warning if you specify the `checkpoint` to load the model.\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:30.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m774\u001b[0m - \u001b[1mmissing keys: []\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:30.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m775\u001b[0m - \u001b[1munexpected keys: []\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:30.383\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m776\u001b[0m - \u001b[1mload model from /home/kutaytire/RL_training/pretrain_checkpoint1\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:30.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mload feature extractor from /home/kutaytire/RL_training/pretrain_checkpoint1/extractor/extractor.json\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd82601d5a1241a99c9fe6be3761f975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.784856\n",
      "epoch: 0, train loss: 2.7788, lr: 0.000100, spent: 0.2 secs\n",
      "epoch: 1, test auc: 0.796875\n",
      "epoch: 1, train loss: 2.6024, lr: 0.000100, spent: 0.4 secs\n",
      "epoch: 2, test auc: 0.801683\n",
      "epoch: 2, train loss: 2.5204, lr: 0.000100, spent: 0.6 secs\n",
      "epoch: 3, test auc: 0.800481\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 3, train loss: 2.3583, lr: 0.000100, spent: 0.9 secs\n",
      "epoch: 4, test auc: 0.788462\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 4, train loss: 2.3013, lr: 0.000100, spent: 1.0 secs\n",
      "epoch: 5, test auc: 0.799279\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 5, train loss: 2.2120, lr: 0.000100, spent: 1.2 secs\n",
      "epoch: 6, test auc: 0.800481\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch: 6, train loss: 2.1406, lr: 0.000100, spent: 1.3 secs\n",
      "epoch: 7, test auc: 0.800481\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch: 7, train loss: 2.0683, lr: 0.000100, spent: 1.5 secs\n",
      "epoch: 8, test auc: 0.800481\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch: 8, train loss: 1.9517, lr: 0.000100, spent: 1.6 secs\n",
      "epoch: 9, test auc: 0.804087\n",
      "epoch: 9, train loss: 1.8747, lr: 0.000100, spent: 1.8 secs\n",
      "epoch: 10, test auc: 0.800481\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 10, train loss: 1.7940, lr: 0.000100, spent: 2.0 secs\n",
      "epoch: 11, test auc: 0.806490\n",
      "epoch: 11, train loss: 1.8170, lr: 0.000100, spent: 2.4 secs\n",
      "epoch: 12, test auc: 0.808894\n",
      "epoch: 12, train loss: 1.8002, lr: 0.000100, spent: 2.5 secs\n",
      "epoch: 13, test auc: 0.818510\n",
      "epoch: 13, train loss: 1.7207, lr: 0.000100, spent: 2.7 secs\n",
      "epoch: 14, test auc: 0.822115\n",
      "epoch: 14, train loss: 1.5982, lr: 0.000100, spent: 2.9 secs\n",
      "epoch: 15, test auc: 0.828125\n",
      "epoch: 15, train loss: 1.6393, lr: 0.000100, spent: 3.1 secs\n",
      "epoch: 16, test auc: 0.837740\n",
      "epoch: 16, train loss: 1.6074, lr: 0.000100, spent: 3.3 secs\n",
      "epoch: 17, test auc: 0.842548\n",
      "epoch: 17, train loss: 1.5440, lr: 0.000100, spent: 3.5 secs\n",
      "epoch: 18, test auc: 0.852163\n",
      "epoch: 18, train loss: 1.4815, lr: 0.000100, spent: 3.8 secs\n",
      "epoch: 19, test auc: 0.852163\n",
      "epoch: 19, train loss: 1.4597, lr: 0.000100, spent: 4.0 secs\n",
      "epoch: 20, test auc: 0.859375\n",
      "epoch: 20, train loss: 1.4140, lr: 0.000100, spent: 4.2 secs\n",
      "epoch: 21, test auc: 0.866587\n",
      "epoch: 21, train loss: 1.4331, lr: 0.000100, spent: 4.4 secs\n",
      "epoch: 22, test auc: 0.872596\n",
      "epoch: 22, train loss: 1.3798, lr: 0.000100, spent: 4.6 secs\n",
      "epoch: 23, test auc: 0.870192\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 23, train loss: 1.3551, lr: 0.000100, spent: 4.7 secs\n",
      "epoch: 24, test auc: 0.870192\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 24, train loss: 1.3689, lr: 0.000100, spent: 4.9 secs\n",
      "epoch: 25, test auc: 0.876202\n",
      "epoch: 25, train loss: 1.5117, lr: 0.000100, spent: 5.3 secs\n",
      "epoch: 26, test auc: 0.876202\n",
      "epoch: 26, train loss: 1.3075, lr: 0.000100, spent: 5.4 secs\n",
      "epoch: 27, test auc: 0.875000\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 27, train loss: 1.2918, lr: 0.000100, spent: 5.6 secs\n",
      "epoch: 28, test auc: 0.878606\n",
      "epoch: 28, train loss: 1.2793, lr: 0.000100, spent: 5.8 secs\n",
      "epoch: 29, test auc: 0.873798\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 29, train loss: 1.2732, lr: 0.000100, spent: 5.9 secs\n",
      "epoch: 30, test auc: 0.867788\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 30, train loss: 1.2729, lr: 0.000100, spent: 6.1 secs\n",
      "epoch: 31, test auc: 0.867788\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 31, train loss: 1.2785, lr: 0.000100, spent: 6.2 secs\n",
      "epoch: 32, test auc: 0.872596\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch: 32, train loss: 1.2160, lr: 0.000100, spent: 6.4 secs\n",
      "epoch: 33, test auc: 0.876202\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch: 33, train loss: 1.2857, lr: 0.000100, spent: 6.7 secs\n",
      "epoch: 34, test auc: 0.882212\n",
      "epoch: 34, train loss: 1.2147, lr: 0.000100, spent: 6.9 secs\n",
      "epoch: 35, test auc: 0.872596\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 35, train loss: 1.2115, lr: 0.000100, spent: 7.0 secs\n",
      "epoch: 36, test auc: 0.877404\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 36, train loss: 1.1689, lr: 0.000100, spent: 7.2 secs\n",
      "epoch: 37, test auc: 0.870192\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 37, train loss: 1.2151, lr: 0.000100, spent: 7.3 secs\n",
      "epoch: 38, test auc: 0.888221\n",
      "epoch: 38, train loss: 1.3279, lr: 0.000100, spent: 7.5 secs\n",
      "epoch: 39, test auc: 0.878606\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 39, train loss: 1.2631, lr: 0.000100, spent: 7.7 secs\n",
      "epoch: 40, test auc: 0.867788\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 40, train loss: 1.1949, lr: 0.000100, spent: 8.0 secs\n",
      "epoch: 41, test auc: 0.872596\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 41, train loss: 1.1542, lr: 0.000100, spent: 8.2 secs\n",
      "epoch: 42, test auc: 0.867788\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch: 42, train loss: 1.1480, lr: 0.000100, spent: 8.3 secs\n",
      "epoch: 43, test auc: 0.875000\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch: 43, train loss: 1.0888, lr: 0.000100, spent: 8.5 secs\n",
      "epoch: 44, test auc: 0.865385\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch: 44, train loss: 1.1073, lr: 0.000100, spent: 8.6 secs\n",
      "epoch: 45, test auc: 0.868990\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch: 45, train loss: 1.1031, lr: 0.000100, spent: 8.8 secs\n",
      "epoch: 46, test auc: 0.870192\n",
      "EarlyStopping counter: 8 out of 10\n",
      "epoch: 46, train loss: 1.0764, lr: 0.000100, spent: 8.9 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:22:39.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:39.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 47, test auc: 0.871394\n",
      "EarlyStopping counter: 9 out of 10\n",
      "epoch: 47, train loss: 1.0879, lr: 0.000100, spent: 9.3 secs\n",
      "epoch: 48, test auc: 0.868990\n",
      "EarlyStopping counter: 10 out of 10\n",
      "early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:22:40.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 9.7 secs.\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:40.157\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36m_check_column_overlap\u001b[0m:\u001b[36m254\u001b[0m - \u001b[33m\u001b[1mNo cat/num/bin cols specified, will take ALL columns as categorical! Ignore this warning if you specify the `checkpoint` to load the model.\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:40.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m774\u001b[0m - \u001b[1mmissing keys: []\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:40.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m775\u001b[0m - \u001b[1munexpected keys: []\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:40.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m776\u001b[0m - \u001b[1mload model from /home/kutaytire/RL_training/pretrain_checkpoint1\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:40.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mload feature extractor from /home/kutaytire/RL_training/pretrain_checkpoint1/extractor/extractor.json\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970d007b80e744438db100b9fd850a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.818510\n",
      "epoch: 0, train loss: 2.7177, lr: 0.000100, spent: 0.2 secs\n",
      "epoch: 1, test auc: 0.736779\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 1, train loss: 2.5148, lr: 0.000100, spent: 0.3 secs\n",
      "epoch: 2, test auc: 0.728365\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 2, train loss: 2.3950, lr: 0.000100, spent: 0.5 secs\n",
      "epoch: 3, test auc: 0.731971\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 3, train loss: 2.2716, lr: 0.000100, spent: 0.6 secs\n",
      "epoch: 4, test auc: 0.746394\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch: 4, train loss: 2.1993, lr: 0.000100, spent: 0.8 secs\n",
      "epoch: 5, test auc: 0.766827\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch: 5, train loss: 2.0917, lr: 0.000100, spent: 1.1 secs\n",
      "epoch: 6, test auc: 0.783654\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch: 6, train loss: 2.0613, lr: 0.000100, spent: 1.3 secs\n",
      "epoch: 7, test auc: 0.800481\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch: 7, train loss: 2.0112, lr: 0.000100, spent: 1.4 secs\n",
      "epoch: 8, test auc: 0.824519\n",
      "epoch: 8, train loss: 1.9491, lr: 0.000100, spent: 1.6 secs\n",
      "epoch: 9, test auc: 0.834135\n",
      "epoch: 9, train loss: 1.8929, lr: 0.000100, spent: 1.8 secs\n",
      "epoch: 10, test auc: 0.844952\n",
      "epoch: 10, train loss: 1.8225, lr: 0.000100, spent: 2.0 secs\n",
      "epoch: 11, test auc: 0.868990\n",
      "epoch: 11, train loss: 1.8469, lr: 0.000100, spent: 2.2 secs\n",
      "epoch: 12, test auc: 0.883413\n",
      "epoch: 12, train loss: 1.7608, lr: 0.000100, spent: 2.5 secs\n",
      "epoch: 13, test auc: 0.888221\n",
      "epoch: 13, train loss: 1.7264, lr: 0.000100, spent: 2.7 secs\n",
      "epoch: 14, test auc: 0.888221\n",
      "epoch: 14, train loss: 1.6958, lr: 0.000100, spent: 2.9 secs\n",
      "epoch: 15, test auc: 0.897837\n",
      "epoch: 15, train loss: 1.6621, lr: 0.000100, spent: 3.1 secs\n",
      "epoch: 16, test auc: 0.900240\n",
      "epoch: 16, train loss: 1.5969, lr: 0.000100, spent: 3.3 secs\n",
      "epoch: 17, test auc: 0.899038\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 17, train loss: 1.6036, lr: 0.000100, spent: 3.4 secs\n",
      "epoch: 18, test auc: 0.906250\n",
      "epoch: 18, train loss: 1.5785, lr: 0.000100, spent: 3.6 secs\n",
      "epoch: 19, test auc: 0.906250\n",
      "epoch: 19, train loss: 1.5285, lr: 0.000100, spent: 4.0 secs\n",
      "epoch: 20, test auc: 0.912260\n",
      "epoch: 20, train loss: 1.4827, lr: 0.000100, spent: 4.2 secs\n",
      "epoch: 21, test auc: 0.906250\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 21, train loss: 1.4533, lr: 0.000100, spent: 4.3 secs\n",
      "epoch: 22, test auc: 0.899038\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 22, train loss: 1.4461, lr: 0.000100, spent: 4.5 secs\n",
      "epoch: 23, test auc: 0.918269\n",
      "epoch: 23, train loss: 1.3933, lr: 0.000100, spent: 4.7 secs\n",
      "epoch: 24, test auc: 0.913462\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 24, train loss: 1.3837, lr: 0.000100, spent: 4.8 secs\n",
      "epoch: 25, test auc: 0.896635\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 25, train loss: 1.3849, lr: 0.000100, spent: 5.0 secs\n",
      "epoch: 26, test auc: 0.891827\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 26, train loss: 1.3350, lr: 0.000100, spent: 5.1 secs\n",
      "epoch: 27, test auc: 0.894231\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch: 27, train loss: 1.3271, lr: 0.000100, spent: 5.5 secs\n",
      "epoch: 28, test auc: 0.895433\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch: 28, train loss: 1.3181, lr: 0.000100, spent: 5.6 secs\n",
      "epoch: 29, test auc: 0.899038\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch: 29, train loss: 1.3251, lr: 0.000100, spent: 5.8 secs\n",
      "epoch: 30, test auc: 0.912260\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch: 30, train loss: 1.3026, lr: 0.000100, spent: 5.9 secs\n",
      "epoch: 31, test auc: 0.895433\n",
      "EarlyStopping counter: 8 out of 10\n",
      "epoch: 31, train loss: 1.3475, lr: 0.000100, spent: 6.1 secs\n",
      "epoch: 32, test auc: 0.894231\n",
      "EarlyStopping counter: 9 out of 10\n",
      "epoch: 32, train loss: 1.3164, lr: 0.000100, spent: 6.2 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:22:46.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:46.677\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:46.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 6.6 secs.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33, test auc: 0.890625\n",
      "EarlyStopping counter: 10 out of 10\n",
      "early stopped\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17aee110ea1a4ffd960bc90c4980cdc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.842593\n",
      "epoch: 0, train loss: 2.7671, lr: 0.000050, spent: 0.2 secs\n",
      "epoch: 1, test auc: 0.901620\n",
      "epoch: 1, train loss: 2.7315, lr: 0.000050, spent: 0.4 secs\n",
      "epoch: 2, test auc: 0.912037\n",
      "epoch: 2, train loss: 2.6961, lr: 0.000050, spent: 0.6 secs\n",
      "epoch: 3, test auc: 0.916667\n",
      "epoch: 3, train loss: 2.6444, lr: 0.000050, spent: 0.7 secs\n",
      "epoch: 4, test auc: 0.924769\n",
      "epoch: 4, train loss: 2.6178, lr: 0.000050, spent: 0.9 secs\n",
      "epoch: 5, test auc: 0.929398\n",
      "epoch: 5, train loss: 2.5650, lr: 0.000050, spent: 1.1 secs\n",
      "epoch: 6, test auc: 0.913194\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 6, train loss: 2.5272, lr: 0.000050, spent: 1.5 secs\n",
      "epoch: 7, test auc: 0.910880\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 7, train loss: 2.4713, lr: 0.000050, spent: 1.6 secs\n",
      "epoch: 8, test auc: 0.917824\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 8, train loss: 2.4676, lr: 0.000050, spent: 1.7 secs\n",
      "epoch: 9, test auc: 0.916667\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 9, train loss: 2.4277, lr: 0.000050, spent: 1.9 secs\n",
      "epoch: 10, test auc: 0.928241\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 10, train loss: 2.3690, lr: 0.000050, spent: 2.0 secs\n",
      "epoch: 11, test auc: 0.932870\n",
      "epoch: 11, train loss: 2.3850, lr: 0.000050, spent: 2.2 secs\n",
      "epoch: 12, test auc: 0.934028\n",
      "epoch: 12, train loss: 2.3307, lr: 0.000050, spent: 2.4 secs\n",
      "epoch: 13, test auc: 0.943287\n",
      "epoch: 13, train loss: 2.2562, lr: 0.000050, spent: 2.8 secs\n",
      "epoch: 14, test auc: 0.942130\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 14, train loss: 2.2361, lr: 0.000050, spent: 3.0 secs\n",
      "epoch: 15, test auc: 0.944444\n",
      "epoch: 15, train loss: 2.1876, lr: 0.000050, spent: 3.1 secs\n",
      "epoch: 16, test auc: 0.947917\n",
      "epoch: 16, train loss: 2.1285, lr: 0.000050, spent: 3.3 secs\n",
      "epoch: 17, test auc: 0.950231\n",
      "epoch: 17, train loss: 2.0771, lr: 0.000050, spent: 3.5 secs\n",
      "epoch: 18, test auc: 0.953704\n",
      "epoch: 18, train loss: 2.0407, lr: 0.000050, spent: 3.7 secs\n",
      "epoch: 19, test auc: 0.951389\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 19, train loss: 2.0694, lr: 0.000050, spent: 3.8 secs\n",
      "epoch: 20, test auc: 0.947917\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 20, train loss: 1.9996, lr: 0.000050, spent: 4.0 secs\n",
      "epoch: 21, test auc: 0.951389\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 21, train loss: 2.0624, lr: 0.000050, spent: 4.3 secs\n",
      "epoch: 22, test auc: 0.956019\n",
      "epoch: 22, train loss: 1.9583, lr: 0.000050, spent: 4.5 secs\n",
      "epoch: 23, test auc: 0.954861\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 23, train loss: 1.9529, lr: 0.000050, spent: 4.7 secs\n",
      "epoch: 24, test auc: 0.958333\n",
      "epoch: 24, train loss: 1.9199, lr: 0.000050, spent: 4.8 secs\n",
      "epoch: 25, test auc: 0.956019\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 25, train loss: 1.9043, lr: 0.000050, spent: 5.0 secs\n",
      "epoch: 26, test auc: 0.956019\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 26, train loss: 1.8465, lr: 0.000050, spent: 5.1 secs\n",
      "epoch: 27, test auc: 0.957176\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 27, train loss: 1.8238, lr: 0.000050, spent: 5.3 secs\n",
      "epoch: 28, test auc: 0.957176\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 28, train loss: 1.8177, lr: 0.000050, spent: 5.6 secs\n",
      "epoch: 29, test auc: 0.959491\n",
      "epoch: 29, train loss: 1.7744, lr: 0.000050, spent: 5.8 secs\n",
      "epoch: 30, test auc: 0.956019\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 30, train loss: 1.7625, lr: 0.000050, spent: 6.0 secs\n",
      "epoch: 31, test auc: 0.954861\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 31, train loss: 1.7414, lr: 0.000050, spent: 6.1 secs\n",
      "epoch: 32, test auc: 0.952546\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 32, train loss: 1.6940, lr: 0.000050, spent: 6.2 secs\n",
      "epoch: 33, test auc: 0.956019\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 33, train loss: 1.7062, lr: 0.000050, spent: 6.4 secs\n",
      "epoch: 34, test auc: 0.951389\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 34, train loss: 1.6169, lr: 0.000050, spent: 6.5 secs\n",
      "epoch: 35, test auc: 0.951389\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 35, train loss: 1.6368, lr: 0.000050, spent: 6.9 secs\n",
      "epoch: 36, test auc: 0.951389\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 36, train loss: 1.6510, lr: 0.000050, spent: 7.0 secs\n",
      "epoch: 37, test auc: 0.952546\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 37, train loss: 1.6355, lr: 0.000050, spent: 7.2 secs\n",
      "epoch: 38, test auc: 0.954861\n",
      "EarlyStopping counter: 9 out of 15\n",
      "epoch: 38, train loss: 1.6246, lr: 0.000050, spent: 7.3 secs\n",
      "epoch: 39, test auc: 0.954861\n",
      "EarlyStopping counter: 10 out of 15\n",
      "epoch: 39, train loss: 1.6513, lr: 0.000050, spent: 7.5 secs\n",
      "epoch: 40, test auc: 0.956019\n",
      "EarlyStopping counter: 11 out of 15\n",
      "epoch: 40, train loss: 1.5460, lr: 0.000050, spent: 7.6 secs\n",
      "epoch: 41, test auc: 0.958333\n",
      "EarlyStopping counter: 12 out of 15\n",
      "epoch: 41, train loss: 1.5673, lr: 0.000050, spent: 7.8 secs\n",
      "epoch: 42, test auc: 0.959491\n",
      "epoch: 42, train loss: 1.4886, lr: 0.000050, spent: 7.9 secs\n",
      "epoch: 43, test auc: 0.954861\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 43, train loss: 1.4986, lr: 0.000050, spent: 8.3 secs\n",
      "epoch: 44, test auc: 0.953704\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 44, train loss: 1.4830, lr: 0.000050, spent: 8.4 secs\n",
      "epoch: 45, test auc: 0.952546\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 45, train loss: 1.4523, lr: 0.000050, spent: 8.6 secs\n",
      "epoch: 46, test auc: 0.953704\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 46, train loss: 1.4228, lr: 0.000050, spent: 8.7 secs\n",
      "epoch: 47, test auc: 0.952546\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 47, train loss: 1.4394, lr: 0.000050, spent: 8.9 secs\n",
      "epoch: 48, test auc: 0.954861\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 48, train loss: 1.4074, lr: 0.000050, spent: 9.0 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:22:56.360\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:22:56.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49, test auc: 0.956019\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 49, train loss: 1.4239, lr: 0.000050, spent: 9.2 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:22:56.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 9.4 secs.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057a70543db34f3c947978c61aa6e40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.633413\n",
      "epoch: 0, train loss: 3.0018, lr: 0.000050, spent: 0.2 secs\n",
      "epoch: 1, test auc: 0.695913\n",
      "epoch: 1, train loss: 2.7077, lr: 0.000050, spent: 0.4 secs\n",
      "epoch: 2, test auc: 0.745192\n",
      "epoch: 2, train loss: 2.6937, lr: 0.000050, spent: 0.5 secs\n",
      "epoch: 3, test auc: 0.769231\n",
      "epoch: 3, train loss: 2.6633, lr: 0.000050, spent: 0.7 secs\n",
      "epoch: 4, test auc: 0.776442\n",
      "epoch: 4, train loss: 2.5935, lr: 0.000050, spent: 0.9 secs\n",
      "epoch: 5, test auc: 0.775240\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 5, train loss: 2.5395, lr: 0.000050, spent: 1.1 secs\n",
      "epoch: 6, test auc: 0.777644\n",
      "epoch: 6, train loss: 2.5150, lr: 0.000050, spent: 1.4 secs\n",
      "epoch: 7, test auc: 0.775240\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 7, train loss: 2.4342, lr: 0.000050, spent: 1.6 secs\n",
      "epoch: 8, test auc: 0.778846\n",
      "epoch: 8, train loss: 2.3733, lr: 0.000050, spent: 1.8 secs\n",
      "epoch: 9, test auc: 0.776442\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 9, train loss: 2.3219, lr: 0.000050, spent: 1.9 secs\n",
      "epoch: 10, test auc: 0.783654\n",
      "epoch: 10, train loss: 2.2578, lr: 0.000050, spent: 2.1 secs\n",
      "epoch: 11, test auc: 0.782452\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 11, train loss: 2.2001, lr: 0.000050, spent: 2.2 secs\n",
      "epoch: 12, test auc: 0.786058\n",
      "epoch: 12, train loss: 2.1317, lr: 0.000050, spent: 2.4 secs\n",
      "epoch: 13, test auc: 0.790865\n",
      "epoch: 13, train loss: 2.1177, lr: 0.000050, spent: 2.8 secs\n",
      "epoch: 14, test auc: 0.799279\n",
      "epoch: 14, train loss: 2.0488, lr: 0.000050, spent: 3.0 secs\n",
      "epoch: 15, test auc: 0.804087\n",
      "epoch: 15, train loss: 2.0004, lr: 0.000050, spent: 3.1 secs\n",
      "epoch: 16, test auc: 0.807692\n",
      "epoch: 16, train loss: 1.9618, lr: 0.000050, spent: 3.3 secs\n",
      "epoch: 17, test auc: 0.818510\n",
      "epoch: 17, train loss: 1.9101, lr: 0.000050, spent: 3.5 secs\n",
      "epoch: 18, test auc: 0.831731\n",
      "epoch: 18, train loss: 1.8426, lr: 0.000050, spent: 3.7 secs\n",
      "epoch: 19, test auc: 0.835337\n",
      "epoch: 19, train loss: 1.8200, lr: 0.000050, spent: 3.9 secs\n",
      "epoch: 20, test auc: 0.838942\n",
      "epoch: 20, train loss: 1.8352, lr: 0.000050, spent: 4.3 secs\n",
      "epoch: 21, test auc: 0.855769\n",
      "epoch: 21, train loss: 1.7334, lr: 0.000050, spent: 4.4 secs\n",
      "epoch: 22, test auc: 0.862981\n",
      "epoch: 22, train loss: 1.6806, lr: 0.000050, spent: 4.6 secs\n",
      "epoch: 23, test auc: 0.862981\n",
      "epoch: 23, train loss: 1.6727, lr: 0.000050, spent: 4.8 secs\n",
      "epoch: 24, test auc: 0.865385\n",
      "epoch: 24, train loss: 1.6112, lr: 0.000050, spent: 5.0 secs\n",
      "epoch: 25, test auc: 0.855769\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 25, train loss: 1.6110, lr: 0.000050, spent: 5.1 secs\n",
      "epoch: 26, test auc: 0.861779\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 26, train loss: 1.6287, lr: 0.000050, spent: 5.3 secs\n",
      "epoch: 27, test auc: 0.871394\n",
      "epoch: 27, train loss: 1.5796, lr: 0.000050, spent: 5.5 secs\n",
      "epoch: 28, test auc: 0.875000\n",
      "epoch: 28, train loss: 1.5522, lr: 0.000050, spent: 5.9 secs\n",
      "epoch: 29, test auc: 0.871394\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 29, train loss: 1.5339, lr: 0.000050, spent: 6.0 secs\n",
      "epoch: 30, test auc: 0.861779\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 30, train loss: 1.5048, lr: 0.000050, spent: 6.2 secs\n",
      "epoch: 31, test auc: 0.858173\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 31, train loss: 1.4805, lr: 0.000050, spent: 6.3 secs\n",
      "epoch: 32, test auc: 0.862981\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 32, train loss: 1.4540, lr: 0.000050, spent: 6.4 secs\n",
      "epoch: 33, test auc: 0.867788\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 33, train loss: 1.4355, lr: 0.000050, spent: 6.6 secs\n",
      "epoch: 34, test auc: 0.881010\n",
      "epoch: 34, train loss: 1.4043, lr: 0.000050, spent: 6.8 secs\n",
      "epoch: 35, test auc: 0.887019\n",
      "epoch: 35, train loss: 1.4152, lr: 0.000050, spent: 7.1 secs\n",
      "epoch: 36, test auc: 0.887019\n",
      "epoch: 36, train loss: 1.4244, lr: 0.000050, spent: 7.3 secs\n",
      "epoch: 37, test auc: 0.888221\n",
      "epoch: 37, train loss: 1.4081, lr: 0.000050, spent: 7.5 secs\n",
      "epoch: 38, test auc: 0.891827\n",
      "epoch: 38, train loss: 1.3737, lr: 0.000050, spent: 7.7 secs\n",
      "epoch: 39, test auc: 0.891827\n",
      "epoch: 39, train loss: 1.3737, lr: 0.000050, spent: 7.9 secs\n",
      "epoch: 40, test auc: 0.895433\n",
      "epoch: 40, train loss: 1.3591, lr: 0.000050, spent: 8.0 secs\n",
      "epoch: 41, test auc: 0.897837\n",
      "epoch: 41, train loss: 1.3633, lr: 0.000050, spent: 8.2 secs\n",
      "epoch: 42, test auc: 0.896635\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 42, train loss: 1.4204, lr: 0.000050, spent: 8.6 secs\n",
      "epoch: 43, test auc: 0.894231\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 43, train loss: 1.3234, lr: 0.000050, spent: 8.7 secs\n",
      "epoch: 44, test auc: 0.897837\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 44, train loss: 1.3262, lr: 0.000050, spent: 8.8 secs\n",
      "epoch: 45, test auc: 0.897837\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 45, train loss: 1.3101, lr: 0.000050, spent: 9.0 secs\n",
      "epoch: 46, test auc: 0.895433\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 46, train loss: 1.2605, lr: 0.000050, spent: 9.1 secs\n",
      "epoch: 47, test auc: 0.905048\n",
      "epoch: 47, train loss: 1.3237, lr: 0.000050, spent: 9.3 secs\n",
      "epoch: 48, test auc: 0.902644\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 48, train loss: 1.2897, lr: 0.000050, spent: 9.5 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:23:06.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:23:06.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:23:06.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 10.0 secs.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49, test auc: 0.900240\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 49, train loss: 1.2881, lr: 0.000050, spent: 9.8 secs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255a2f62ad9b45598eaaeaf62f7c79d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.485577\n",
      "epoch: 0, train loss: 2.8908, lr: 0.000050, spent: 0.2 secs\n",
      "epoch: 1, test auc: 0.716346\n",
      "epoch: 1, train loss: 2.7376, lr: 0.000050, spent: 0.4 secs\n",
      "epoch: 2, test auc: 0.752404\n",
      "epoch: 2, train loss: 2.6579, lr: 0.000050, spent: 0.6 secs\n",
      "epoch: 3, test auc: 0.771635\n",
      "epoch: 3, train loss: 2.5582, lr: 0.000050, spent: 0.7 secs\n",
      "epoch: 4, test auc: 0.770433\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 4, train loss: 2.4776, lr: 0.000050, spent: 0.9 secs\n",
      "epoch: 5, test auc: 0.764423\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 5, train loss: 2.4104, lr: 0.000050, spent: 1.0 secs\n",
      "epoch: 6, test auc: 0.764423\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 6, train loss: 2.3325, lr: 0.000050, spent: 1.4 secs\n",
      "epoch: 7, test auc: 0.756010\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 7, train loss: 2.2584, lr: 0.000050, spent: 1.5 secs\n",
      "epoch: 8, test auc: 0.757212\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 8, train loss: 2.1949, lr: 0.000050, spent: 1.7 secs\n",
      "epoch: 9, test auc: 0.756010\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 9, train loss: 2.1585, lr: 0.000050, spent: 1.8 secs\n",
      "epoch: 10, test auc: 0.762019\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 10, train loss: 2.1207, lr: 0.000050, spent: 1.9 secs\n",
      "epoch: 11, test auc: 0.770433\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 11, train loss: 2.1468, lr: 0.000050, spent: 2.1 secs\n",
      "epoch: 12, test auc: 0.765625\n",
      "EarlyStopping counter: 9 out of 15\n",
      "epoch: 12, train loss: 2.0333, lr: 0.000050, spent: 2.2 secs\n",
      "epoch: 13, test auc: 0.765625\n",
      "EarlyStopping counter: 10 out of 15\n",
      "epoch: 13, train loss: 2.0199, lr: 0.000050, spent: 2.6 secs\n",
      "epoch: 14, test auc: 0.774038\n",
      "epoch: 14, train loss: 1.9625, lr: 0.000050, spent: 2.8 secs\n",
      "epoch: 15, test auc: 0.774038\n",
      "epoch: 15, train loss: 1.9654, lr: 0.000050, spent: 3.0 secs\n",
      "epoch: 16, test auc: 0.777644\n",
      "epoch: 16, train loss: 1.8966, lr: 0.000050, spent: 3.1 secs\n",
      "epoch: 17, test auc: 0.788462\n",
      "epoch: 17, train loss: 1.8366, lr: 0.000050, spent: 3.3 secs\n",
      "epoch: 18, test auc: 0.792067\n",
      "epoch: 18, train loss: 1.8149, lr: 0.000050, spent: 3.5 secs\n",
      "epoch: 19, test auc: 0.795673\n",
      "epoch: 19, train loss: 1.8174, lr: 0.000050, spent: 3.7 secs\n",
      "epoch: 20, test auc: 0.805288\n",
      "epoch: 20, train loss: 1.7435, lr: 0.000050, spent: 4.0 secs\n",
      "epoch: 21, test auc: 0.806490\n",
      "epoch: 21, train loss: 1.7423, lr: 0.000050, spent: 4.2 secs\n",
      "epoch: 22, test auc: 0.807692\n",
      "epoch: 22, train loss: 1.7021, lr: 0.000050, spent: 4.4 secs\n",
      "epoch: 23, test auc: 0.812500\n",
      "epoch: 23, train loss: 1.6712, lr: 0.000050, spent: 4.6 secs\n",
      "epoch: 24, test auc: 0.813702\n",
      "epoch: 24, train loss: 1.6095, lr: 0.000050, spent: 4.8 secs\n",
      "epoch: 25, test auc: 0.818510\n",
      "epoch: 25, train loss: 1.5680, lr: 0.000050, spent: 5.0 secs\n",
      "epoch: 26, test auc: 0.824519\n",
      "epoch: 26, train loss: 1.6261, lr: 0.000050, spent: 5.1 secs\n",
      "epoch: 27, test auc: 0.829327\n",
      "epoch: 27, train loss: 1.5742, lr: 0.000050, spent: 5.5 secs\n",
      "epoch: 28, test auc: 0.834135\n",
      "epoch: 28, train loss: 1.5377, lr: 0.000050, spent: 5.7 secs\n",
      "epoch: 29, test auc: 0.843750\n",
      "epoch: 29, train loss: 1.4902, lr: 0.000050, spent: 5.9 secs\n",
      "epoch: 30, test auc: 0.843750\n",
      "epoch: 30, train loss: 1.4983, lr: 0.000050, spent: 6.1 secs\n",
      "epoch: 31, test auc: 0.837740\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 31, train loss: 1.5155, lr: 0.000050, spent: 6.2 secs\n",
      "epoch: 32, test auc: 0.842548\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 32, train loss: 1.5016, lr: 0.000050, spent: 6.4 secs\n",
      "epoch: 33, test auc: 0.842548\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 33, train loss: 1.4615, lr: 0.000050, spent: 6.5 secs\n",
      "epoch: 34, test auc: 0.841346\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 34, train loss: 1.4136, lr: 0.000050, spent: 6.8 secs\n",
      "epoch: 35, test auc: 0.841346\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 35, train loss: 1.4090, lr: 0.000050, spent: 7.0 secs\n",
      "epoch: 36, test auc: 0.846154\n",
      "epoch: 36, train loss: 1.3922, lr: 0.000050, spent: 7.2 secs\n",
      "epoch: 37, test auc: 0.847356\n",
      "epoch: 37, train loss: 1.3952, lr: 0.000050, spent: 7.4 secs\n",
      "epoch: 38, test auc: 0.853365\n",
      "epoch: 38, train loss: 1.3409, lr: 0.000050, spent: 7.5 secs\n",
      "epoch: 39, test auc: 0.853365\n",
      "epoch: 39, train loss: 1.4041, lr: 0.000050, spent: 7.7 secs\n",
      "epoch: 40, test auc: 0.856971\n",
      "epoch: 40, train loss: 1.3044, lr: 0.000050, spent: 7.9 secs\n",
      "epoch: 41, test auc: 0.859375\n",
      "epoch: 41, train loss: 1.3082, lr: 0.000050, spent: 8.3 secs\n",
      "epoch: 42, test auc: 0.861779\n",
      "epoch: 42, train loss: 1.2711, lr: 0.000050, spent: 8.5 secs\n",
      "epoch: 43, test auc: 0.862981\n",
      "epoch: 43, train loss: 1.2595, lr: 0.000050, spent: 8.7 secs\n",
      "epoch: 44, test auc: 0.867788\n",
      "epoch: 44, train loss: 1.2174, lr: 0.000050, spent: 8.8 secs\n",
      "epoch: 45, test auc: 0.868990\n",
      "epoch: 45, train loss: 1.2646, lr: 0.000050, spent: 9.0 secs\n",
      "epoch: 46, test auc: 0.864183\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 46, train loss: 1.2248, lr: 0.000050, spent: 9.2 secs\n",
      "epoch: 47, test auc: 0.864183\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 47, train loss: 1.2077, lr: 0.000050, spent: 9.3 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:23:16.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 48, test auc: 0.873798\n",
      "epoch: 48, train loss: 1.2100, lr: 0.000050, spent: 9.7 secs\n",
      "epoch: 49, test auc: 0.870192\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 49, train loss: 1.2440, lr: 0.000050, spent: 9.8 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:23:16.924\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:23:17.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 10.0 secs.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24798aa877af410f8f645884d4090f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.448317\n",
      "epoch: 0, train loss: 2.9109, lr: 0.000050, spent: 0.2 secs\n",
      "epoch: 1, test auc: 0.677885\n",
      "epoch: 1, train loss: 2.7827, lr: 0.000050, spent: 0.4 secs\n",
      "epoch: 2, test auc: 0.748798\n",
      "epoch: 2, train loss: 2.7541, lr: 0.000050, spent: 0.5 secs\n",
      "epoch: 3, test auc: 0.752404\n",
      "epoch: 3, train loss: 2.7131, lr: 0.000050, spent: 0.7 secs\n",
      "epoch: 4, test auc: 0.763221\n",
      "epoch: 4, train loss: 2.7080, lr: 0.000050, spent: 0.9 secs\n",
      "epoch: 5, test auc: 0.766827\n",
      "epoch: 5, train loss: 2.6741, lr: 0.000050, spent: 1.3 secs\n",
      "epoch: 6, test auc: 0.768029\n",
      "epoch: 6, train loss: 2.6495, lr: 0.000050, spent: 1.5 secs\n",
      "epoch: 7, test auc: 0.769231\n",
      "epoch: 7, train loss: 2.6413, lr: 0.000050, spent: 1.7 secs\n",
      "epoch: 8, test auc: 0.759615\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 8, train loss: 2.6159, lr: 0.000050, spent: 1.8 secs\n",
      "epoch: 9, test auc: 0.758413\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 9, train loss: 2.5736, lr: 0.000050, spent: 1.9 secs\n",
      "epoch: 10, test auc: 0.763221\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 10, train loss: 2.5350, lr: 0.000050, spent: 2.1 secs\n",
      "epoch: 11, test auc: 0.760817\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 11, train loss: 2.4948, lr: 0.000050, spent: 2.2 secs\n",
      "epoch: 12, test auc: 0.751202\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 12, train loss: 2.4413, lr: 0.000050, spent: 2.6 secs\n",
      "epoch: 13, test auc: 0.764423\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 13, train loss: 2.3617, lr: 0.000050, spent: 2.7 secs\n",
      "epoch: 14, test auc: 0.764423\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 14, train loss: 2.3180, lr: 0.000050, spent: 2.9 secs\n",
      "epoch: 15, test auc: 0.737981\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 15, train loss: 2.2441, lr: 0.000050, spent: 3.0 secs\n",
      "epoch: 16, test auc: 0.771635\n",
      "epoch: 16, train loss: 2.1981, lr: 0.000050, spent: 3.2 secs\n",
      "epoch: 17, test auc: 0.775240\n",
      "epoch: 17, train loss: 2.1175, lr: 0.000050, spent: 3.4 secs\n",
      "epoch: 18, test auc: 0.756010\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 18, train loss: 2.0969, lr: 0.000050, spent: 3.5 secs\n",
      "epoch: 19, test auc: 0.747596\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 19, train loss: 2.0129, lr: 0.000050, spent: 3.9 secs\n",
      "epoch: 20, test auc: 0.760817\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 20, train loss: 2.0028, lr: 0.000050, spent: 4.0 secs\n",
      "epoch: 21, test auc: 0.768029\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 21, train loss: 1.9186, lr: 0.000050, spent: 4.2 secs\n",
      "epoch: 22, test auc: 0.798077\n",
      "epoch: 22, train loss: 1.8519, lr: 0.000050, spent: 4.4 secs\n",
      "epoch: 23, test auc: 0.806490\n",
      "epoch: 23, train loss: 1.8290, lr: 0.000050, spent: 4.5 secs\n",
      "epoch: 24, test auc: 0.813702\n",
      "epoch: 24, train loss: 1.7633, lr: 0.000050, spent: 4.7 secs\n",
      "epoch: 25, test auc: 0.818510\n",
      "epoch: 25, train loss: 1.6627, lr: 0.000050, spent: 4.9 secs\n",
      "epoch: 26, test auc: 0.823317\n",
      "epoch: 26, train loss: 1.6399, lr: 0.000050, spent: 5.3 secs\n",
      "epoch: 27, test auc: 0.830529\n",
      "epoch: 27, train loss: 1.6263, lr: 0.000050, spent: 5.5 secs\n",
      "epoch: 28, test auc: 0.812500\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 28, train loss: 1.5664, lr: 0.000050, spent: 5.6 secs\n",
      "epoch: 29, test auc: 0.824519\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 29, train loss: 1.5372, lr: 0.000050, spent: 5.8 secs\n",
      "epoch: 30, test auc: 0.829327\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 30, train loss: 1.5426, lr: 0.000050, spent: 5.9 secs\n",
      "epoch: 31, test auc: 0.829327\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 31, train loss: 1.5263, lr: 0.000050, spent: 6.1 secs\n",
      "epoch: 32, test auc: 0.832933\n",
      "epoch: 32, train loss: 1.5186, lr: 0.000050, spent: 6.2 secs\n",
      "epoch: 33, test auc: 0.830529\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 33, train loss: 1.4800, lr: 0.000050, spent: 6.6 secs\n",
      "epoch: 34, test auc: 0.829327\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 34, train loss: 1.4590, lr: 0.000050, spent: 6.7 secs\n",
      "epoch: 35, test auc: 0.830529\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 35, train loss: 1.4387, lr: 0.000050, spent: 6.9 secs\n",
      "epoch: 36, test auc: 0.836538\n",
      "epoch: 36, train loss: 1.4183, lr: 0.000050, spent: 7.0 secs\n",
      "epoch: 37, test auc: 0.835337\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 37, train loss: 1.4546, lr: 0.000050, spent: 7.2 secs\n",
      "epoch: 38, test auc: 0.841346\n",
      "epoch: 38, train loss: 1.3761, lr: 0.000050, spent: 7.4 secs\n",
      "epoch: 39, test auc: 0.840144\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 39, train loss: 1.4212, lr: 0.000050, spent: 7.5 secs\n",
      "epoch: 40, test auc: 0.836538\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 40, train loss: 1.3367, lr: 0.000050, spent: 7.9 secs\n",
      "epoch: 41, test auc: 0.842548\n",
      "epoch: 41, train loss: 1.3619, lr: 0.000050, spent: 8.0 secs\n",
      "epoch: 42, test auc: 0.837740\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 42, train loss: 1.3417, lr: 0.000050, spent: 8.2 secs\n",
      "epoch: 43, test auc: 0.820913\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 43, train loss: 1.3927, lr: 0.000050, spent: 8.3 secs\n",
      "epoch: 44, test auc: 0.817308\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 44, train loss: 1.4112, lr: 0.000050, spent: 8.5 secs\n",
      "epoch: 45, test auc: 0.806490\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 45, train loss: 1.3418, lr: 0.000050, spent: 8.6 secs\n",
      "epoch: 46, test auc: 0.818510\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 46, train loss: 1.3931, lr: 0.000050, spent: 8.8 secs\n",
      "epoch: 47, test auc: 0.848558\n",
      "epoch: 47, train loss: 1.2983, lr: 0.000050, spent: 9.1 secs\n",
      "epoch: 48, test auc: 0.844952\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 48, train loss: 1.3672, lr: 0.000050, spent: 9.3 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:23:26.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:23:26.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49, test auc: 0.838942\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 49, train loss: 1.3946, lr: 0.000050, spent: 9.4 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:23:26.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 9.7 secs.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8072578e39c5453babc723abfb7f5336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.496394\n",
      "epoch: 0, train loss: 2.7856, lr: 0.000050, spent: 0.2 secs\n",
      "epoch: 1, test auc: 0.612981\n",
      "epoch: 1, train loss: 2.7110, lr: 0.000050, spent: 0.4 secs\n",
      "epoch: 2, test auc: 0.663462\n",
      "epoch: 2, train loss: 2.7106, lr: 0.000050, spent: 0.6 secs\n",
      "epoch: 3, test auc: 0.697115\n",
      "epoch: 3, train loss: 2.6453, lr: 0.000050, spent: 0.7 secs\n",
      "epoch: 4, test auc: 0.721154\n",
      "epoch: 4, train loss: 2.6272, lr: 0.000050, spent: 1.1 secs\n",
      "epoch: 5, test auc: 0.737981\n",
      "epoch: 5, train loss: 2.5319, lr: 0.000050, spent: 1.3 secs\n",
      "epoch: 6, test auc: 0.741587\n",
      "epoch: 6, train loss: 2.4875, lr: 0.000050, spent: 1.5 secs\n",
      "epoch: 7, test auc: 0.740385\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 7, train loss: 2.3755, lr: 0.000050, spent: 1.6 secs\n",
      "epoch: 8, test auc: 0.750000\n",
      "epoch: 8, train loss: 2.3438, lr: 0.000050, spent: 1.8 secs\n",
      "epoch: 9, test auc: 0.758413\n",
      "epoch: 9, train loss: 2.2578, lr: 0.000050, spent: 2.0 secs\n",
      "epoch: 10, test auc: 0.782452\n",
      "epoch: 10, train loss: 2.2567, lr: 0.000050, spent: 2.2 secs\n",
      "epoch: 11, test auc: 0.793269\n",
      "epoch: 11, train loss: 2.1509, lr: 0.000050, spent: 2.5 secs\n",
      "epoch: 12, test auc: 0.795673\n",
      "epoch: 12, train loss: 2.1661, lr: 0.000050, spent: 2.7 secs\n",
      "epoch: 13, test auc: 0.808894\n",
      "epoch: 13, train loss: 2.0722, lr: 0.000050, spent: 2.9 secs\n",
      "epoch: 14, test auc: 0.817308\n",
      "epoch: 14, train loss: 2.0573, lr: 0.000050, spent: 3.1 secs\n",
      "epoch: 15, test auc: 0.829327\n",
      "epoch: 15, train loss: 2.0031, lr: 0.000050, spent: 3.3 secs\n",
      "epoch: 16, test auc: 0.844952\n",
      "epoch: 16, train loss: 1.9759, lr: 0.000050, spent: 3.4 secs\n",
      "epoch: 17, test auc: 0.848558\n",
      "epoch: 17, train loss: 1.9336, lr: 0.000050, spent: 3.6 secs\n",
      "epoch: 18, test auc: 0.854567\n",
      "epoch: 18, train loss: 1.9247, lr: 0.000050, spent: 4.0 secs\n",
      "epoch: 19, test auc: 0.868990\n",
      "epoch: 19, train loss: 1.8884, lr: 0.000050, spent: 4.2 secs\n",
      "epoch: 20, test auc: 0.876202\n",
      "epoch: 20, train loss: 1.9038, lr: 0.000050, spent: 4.4 secs\n",
      "epoch: 21, test auc: 0.879808\n",
      "epoch: 21, train loss: 1.8470, lr: 0.000050, spent: 4.5 secs\n",
      "epoch: 22, test auc: 0.884615\n",
      "epoch: 22, train loss: 1.7726, lr: 0.000050, spent: 4.7 secs\n",
      "epoch: 23, test auc: 0.887019\n",
      "epoch: 23, train loss: 1.7250, lr: 0.000050, spent: 4.9 secs\n",
      "epoch: 24, test auc: 0.890625\n",
      "epoch: 24, train loss: 1.7346, lr: 0.000050, spent: 5.1 secs\n",
      "epoch: 25, test auc: 0.887019\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 25, train loss: 1.7075, lr: 0.000050, spent: 5.4 secs\n",
      "epoch: 26, test auc: 0.890625\n",
      "epoch: 26, train loss: 1.7078, lr: 0.000050, spent: 5.6 secs\n",
      "epoch: 27, test auc: 0.894231\n",
      "epoch: 27, train loss: 1.6939, lr: 0.000050, spent: 5.8 secs\n",
      "epoch: 28, test auc: 0.893029\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 28, train loss: 1.6834, lr: 0.000050, spent: 6.0 secs\n",
      "epoch: 29, test auc: 0.894231\n",
      "epoch: 29, train loss: 1.6684, lr: 0.000050, spent: 6.1 secs\n",
      "epoch: 30, test auc: 0.895433\n",
      "epoch: 30, train loss: 1.6279, lr: 0.000050, spent: 6.3 secs\n",
      "epoch: 31, test auc: 0.899038\n",
      "epoch: 31, train loss: 1.6488, lr: 0.000050, spent: 6.5 secs\n",
      "epoch: 32, test auc: 0.900240\n",
      "epoch: 32, train loss: 1.6030, lr: 0.000050, spent: 6.9 secs\n",
      "epoch: 33, test auc: 0.895433\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 33, train loss: 1.5863, lr: 0.000050, spent: 7.0 secs\n",
      "epoch: 34, test auc: 0.893029\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 34, train loss: 1.5505, lr: 0.000050, spent: 7.2 secs\n",
      "epoch: 35, test auc: 0.895433\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 35, train loss: 1.5183, lr: 0.000050, spent: 7.3 secs\n",
      "epoch: 36, test auc: 0.896635\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 36, train loss: 1.5241, lr: 0.000050, spent: 7.4 secs\n",
      "epoch: 37, test auc: 0.900240\n",
      "epoch: 37, train loss: 1.5255, lr: 0.000050, spent: 7.6 secs\n",
      "epoch: 38, test auc: 0.901442\n",
      "epoch: 38, train loss: 1.5093, lr: 0.000050, spent: 7.8 secs\n",
      "epoch: 39, test auc: 0.890625\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 39, train loss: 1.5464, lr: 0.000050, spent: 8.2 secs\n",
      "epoch: 40, test auc: 0.897837\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 40, train loss: 1.4420, lr: 0.000050, spent: 8.3 secs\n",
      "epoch: 41, test auc: 0.899038\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 41, train loss: 1.4300, lr: 0.000050, spent: 8.4 secs\n",
      "epoch: 42, test auc: 0.894231\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 42, train loss: 1.4800, lr: 0.000050, spent: 8.6 secs\n",
      "epoch: 43, test auc: 0.891827\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 43, train loss: 1.4700, lr: 0.000050, spent: 8.7 secs\n",
      "epoch: 44, test auc: 0.896635\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 44, train loss: 1.4099, lr: 0.000050, spent: 8.9 secs\n",
      "epoch: 45, test auc: 0.888221\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 45, train loss: 1.4619, lr: 0.000050, spent: 9.0 secs\n",
      "epoch: 46, test auc: 0.887019\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 46, train loss: 1.4399, lr: 0.000050, spent: 9.2 secs\n",
      "epoch: 47, test auc: 0.895433\n",
      "EarlyStopping counter: 9 out of 15\n",
      "epoch: 47, train loss: 1.4218, lr: 0.000050, spent: 9.5 secs\n",
      "epoch: 48, test auc: 0.884615\n",
      "EarlyStopping counter: 10 out of 15\n",
      "epoch: 48, train loss: 1.3714, lr: 0.000050, spent: 9.6 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:23:36.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:23:36.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:23:37.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 10.0 secs.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49, test auc: 0.884615\n",
      "EarlyStopping counter: 11 out of 15\n",
      "epoch: 49, train loss: 1.4370, lr: 0.000050, spent: 9.8 secs\n"
     ]
    }
   ],
   "source": [
    "## -- Perform Transfer Learning on the first pai of datasets\n",
    "PreTrain1 = cv_transtab_5fold(d_X1, d_y1, cat_cols1, num_cols1, bin_cols1,\n",
    "                              file_name='pretrain_checkpoint1',save_ckpt=True)\n",
    "TransferLearning1 = cv_transtab_5fold(d_X2, d_y2, cat_cols2, num_cols2, bin_cols2,\n",
    "                                      transfer=True, file_name='pretrain_checkpoint1')\n",
    "Baseline1 = cv_transtab_5fold(d_X2, d_y2, cat_cols2, num_cols2, bin_cols2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7fBlAGjJLeg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ec77b7334dcb4414b2ca86245e0cf538",
      "4c84ff89fd864d0a9ff845623592f6dd",
      "4c123d70a9e24e1b8f8cd686eb94747a",
      "05861e89e2da4a8083fc15154899a54a",
      "6d6f191e44f84f8ea39f55796f52fea0",
      "3adac99c18804c9b8cefb2f774094ee5",
      "9794bf31798544c2b4c85cdd30ee1949",
      "50e032fadb024db696b00064e3644bff",
      "d5b33b58164a40e98218e509dd3c0a29",
      "e3f57bbcd1cd4b9cb7095473801424c0",
      "d3b541e7a5f2451bba41cbcac0d30cdb",
      "f6237934be1e48be8334271e24c229c6",
      "95011208ea724c82ad796b4f24dd9023",
      "bfd71944b1c8473a88c918fcc57f54f9",
      "34bdec74c32744b99525526b3e9a5c75",
      "1e83607c97a64052829cfcdee43baacb",
      "d73c8587086a4d6c8c2ac455dba48469",
      "8b5ded4a54a342928f89212bb4c7b485",
      "a7182019975a491cac16a5c4b6b1b574",
      "d4ab515a55bf4110860c1a4ef93a5078",
      "90e49762e5514d5eb9c59ea29d806605",
      "7d39b392dc704930b62a10d7e8afe661",
      "ba9f1ef99baa485d8d62ffe6c1359ed9",
      "f136c802eda245d4b388e030a7259b78",
      "3e19dde146f14f239bf0ebb1b0004cb3",
      "5874e328647b429ba4b1a2cd3c0f5f0d",
      "78345e3bf3394a1e9898f619ea04cc7f",
      "f27dafeca2c444d2a334314698fe9ca4",
      "6cb672ffd46a4a82aa78ec2a73fa2c6c",
      "e624b9d9cbd84814a0996621aa19c371",
      "dcc6fe90c306400d94fcd1a9f1c676cb",
      "62404118ddc74334bb769a08a67ba9b3",
      "a6001ed39cdc46c2a765b41792e557c4",
      "056a682c6ba645e096163e8073aa6c75",
      "6f39b31c908348999b60d62d7c920943",
      "cd617b23ef7d42ba8694edd3331078d3",
      "9ea4c087d56f4073a489fef3d45f4131",
      "05d2ada0dd4a49da859e3f27d0f0f112",
      "a7c1a26dedbb42a2b2eaaa832bc2a452",
      "a23d469c907544d894b29e6c7addb32b",
      "286bf706beb34422b59d0e4618004d0d",
      "f09a5d8e23d24c4db3d386707e378369",
      "3ebee529cc344000b99f5a2602a53dde",
      "f10ec900483046989a6eaa354598df94",
      "da156bf9c9854e14ab9ef734a0aa8cf6",
      "6c807a607b0b45b7bd0dbbf3b12f6917",
      "d8990b9fee284c36b541bd52f5764b15",
      "4d0c012685ae482a8a2e7abe909c0f0a",
      "9ef0b726050948c1b63209dada8aea65",
      "410219d48a17409d92e7c405d65ed733",
      "6ef7a98c2d984c92aca176b66218c489",
      "b34a3cb1121e40bbbc440d5e526afa01",
      "7e99a907c3e5439a9c81de80febf02d6",
      "c5b4184a76de45849d743676264107d2",
      "debbbea3a7354515ac48c9951071ad5f",
      "fa2b97cee1b247a4922933c975923c87",
      "0daf7e1e67374ebfbf136372b2e860e5",
      "02498f5cc93140cfa4a1b41d9ed3aa2e",
      "9cda22f8182648fca6e61bf930738ff4",
      "45d93d3f42f94a13837bc2e3752707e5",
      "43b5dfc727914ce7961ca812eb4e8fed",
      "f1df14d4c1594f9785c01f92c6743aa8",
      "5ef5496ff3174c0e959521a2d5953ea0",
      "2778f8107f5c4dfd8f948e5457e1d2ae",
      "bf22cf5d3d74447f8b22af868a57562f",
      "33bfce7ae2fb4e5ea834ccfbe499cd8d",
      "563eb8d2c49a425dbddf5a2a051c8f59",
      "d28b4b57c7dd4986b90d38bd733512b8",
      "0a33fbc3072242aeb359944faadfb3a1",
      "fb17c275eedf4c079daa352f782979ea",
      "0f5e51231cc243b0968227044f2fb1e5",
      "5665bea546a1473a801a7ef3764c8274",
      "a95b24653ac74dd98316eb3deaeb3529",
      "49613d992f5b4abb8c43ca117bca352b",
      "0ab9960323634677894244ca942de3d6",
      "9a5288edd8d548f6a78824afed7dbc15",
      "8ede4f306b694ee5b45438e6a2df9bbd",
      "4d6e87cda665422aa27418bc5ed358f6",
      "727d9705a5f04b568ff49790736f54ed",
      "89c7fbcd8b344e609562031294badb97",
      "b16e19429a8d41788ea9545fc60c70e9",
      "22ef01e8406f4c6b9620056ffc5226c1",
      "19c5a7993b044314a81a091b5621f866",
      "7a21af6cee394a1c92415b498b0e30c2",
      "5daf80990f68441c8b00e6b9ba29a7bf",
      "d9ea237ad2cc4221b4e09d76ca51ce99",
      "6e24f313e3e8437f80e0371bbce0baa6",
      "cc27c1255348405d9be3033f298588f2",
      "b4a45d536f584f739bb921304151f885",
      "5f63b913c96e420490080753f2e51de4",
      "209de7ee2770482a8ea32935ad639e06",
      "9c34d94af701444285dab973cfb29323",
      "39d938bd77734befb413d974c5aa936c",
      "dbc735496be943cebc62be1a046de8e9",
      "8be14bcc1a754651a55bad5e1e365703",
      "2339155921be446fa021254e29c1eeaa",
      "0f3b93c684ee42f4a1ec7ece755ab134",
      "b9c68a3214e74a86a4c7b51f955e1c8a",
      "ff6bf1ed40434b1d99bfa9a5d7183e02",
      "ae5376eb02e2404abfd87933c0dbbaae",
      "01fab4da0377425e904216610e0494fe",
      "9543206e91e24a2da3f6aef49864b56f",
      "d66d4b7d63c845d0b0f6a6b941c198dc",
      "498e177dae694810af80b5a4ee8585c5",
      "4f630bf5178247d3bc15a0f76c00a913",
      "aa42abdd1f3541d09c8ad457ed4781a5",
      "e6019cb06d444912a17a0cf95da25537",
      "7d96ac94209d4e40afbeea33569f4224",
      "1be59c289ea44f2d9577bfbeb520f62b",
      "f548a6175fa94cc4a4824acbc5bc2897",
      "fbf68e1eda384b6286bc5a53ae4a2103",
      "94124a0870914961804c08bf5fe1ed43",
      "c84a72311a224ffda72038ac8697ae25",
      "6d503dc0ac86425299fa5f4f268766ac",
      "f245b150bd49485d9b72295b04912b4e",
      "1a9f3dc1d689447c9352a6390044d23a",
      "2ea6cd2a4f7a4d23b981e5ee13935a38",
      "0f39cb2884794cb69166267289275f5f",
      "fa0c2a8f841144bfbebee7873e8a0cff",
      "d7f4d4a414124fc6b6f40efff994467c",
      "1902f09b6a784a55a2e13c33971bd1ca",
      "a971d8d29fe7462aa6544ed669f995b9",
      "d647567144684487b1e1b1bb9604c983",
      "8b3ada4d07ad47878ece9d407c714858",
      "1905c2c2351b4dd28daac3a857b5a01d",
      "a741daf11169460194b9531e1cda701b",
      "8564a659bea54abca4b5b3e115a2489b",
      "bb5ac3a0e9804d62ad79a8ddf2a8f941",
      "8d366e5d5e664c7f9c8a8692b83ab970",
      "a5de112d33d748a6ad7dec205381b6f0",
      "4000426d54ab43b294848b15e4bd6cc6",
      "060fbf64c3f4467a84b5e4f2d5caf295",
      "79c1c95fe01d45e1a320830ee71d1ac0",
      "1987d9ddd1c74a29a884b98052663dd7",
      "909de2aea5ea447fbd044d3dc2dde309",
      "c3024b0c4a5548ffb17f7faf1288a0a6",
      "a9b15152c115446d89416d0c5a6fe6e4",
      "d33fe7f8cbb64ef78ef6dce5944dd3fc",
      "ab9386bcaee1474d88fc16524e179646",
      "a9dc1cdf269949bd9da7f44c3ed0b5c9",
      "7f6d092c57784c5aa6c7dbcb1a98b3e1",
      "ad648564dc9248f08250b423d8febf42",
      "bb1f4f2f778949f3a6c9d89fb68c6d04",
      "a1ba7dbed1f7407eb438504078722cca",
      "be92c49afa6442dea027bc5a1b65c066",
      "1c487cb791104b17b1fd58a96f8b88a4",
      "44ae5e058d834fcb897a48e4652be615",
      "dd6993fcbfae40849efa1bca8f36fe46",
      "d997066e7b864ed7be52ac0c1097be61",
      "6fc032e0188e45409a684b926359238e",
      "144f377f43004080bce8f850da50b94c",
      "a96e45a7313640caa356988163780318",
      "3eeb1c5d7d40497d8ae37cb08811291d",
      "abeeb3b60f9d4167bf4e7513bbb9d54b",
      "149b47f6bcda4e1388234ecd744d86d4",
      "4c972522dbe54dc2a8404fd8e21d439e",
      "f3c597bb8c9442f1b70a0f71993262ff",
      "dc346338a4d046d7b728e7514d85c9ac",
      "57d65e93cfce4728af2a4ade76136eff",
      "582a4d31191743c1bc10f4a26d860ca3",
      "e2a013aecb8e4363aa13a899d2bce81d",
      "1b74fd8a19384678a9268b497a775173",
      "ea3728dc840349929f1baf63847daebd",
      "b9f4d7f3c23145c6b02fa7cf3c8ce35f",
      "7b36a382a95841d3a8cc554c6c7d8d92"
     ]
    },
    "id": "b7fBlAGjJLeg",
    "outputId": "1a4c19b0-3896-4a61-da1b-757c007c6d1f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7404646bb14db09894f6f98bd988d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.538149\n",
      "epoch: 0, train loss: 5.1113, lr: 0.000050, spent: 0.3 secs\n",
      "epoch: 1, test auc: 0.579221\n",
      "epoch: 1, train loss: 4.8753, lr: 0.000050, spent: 0.5 secs\n",
      "epoch: 2, test auc: 0.593506\n",
      "epoch: 2, train loss: 4.8490, lr: 0.000050, spent: 0.7 secs\n",
      "epoch: 3, test auc: 0.607792\n",
      "epoch: 3, train loss: 4.7829, lr: 0.000050, spent: 1.0 secs\n",
      "epoch: 4, test auc: 0.613474\n",
      "epoch: 4, train loss: 4.7675, lr: 0.000050, spent: 1.4 secs\n",
      "epoch: 5, test auc: 0.614448\n",
      "epoch: 5, train loss: 4.7383, lr: 0.000050, spent: 1.7 secs\n",
      "epoch: 6, test auc: 0.621266\n",
      "epoch: 6, train loss: 4.7186, lr: 0.000050, spent: 1.9 secs\n",
      "epoch: 7, test auc: 0.626461\n",
      "epoch: 7, train loss: 4.6887, lr: 0.000050, spent: 2.2 secs\n",
      "epoch: 8, test auc: 0.625162\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 8, train loss: 4.6670, lr: 0.000050, spent: 2.4 secs\n",
      "epoch: 9, test auc: 0.628571\n",
      "epoch: 9, train loss: 4.6415, lr: 0.000050, spent: 2.7 secs\n",
      "epoch: 10, test auc: 0.629708\n",
      "epoch: 10, train loss: 4.6106, lr: 0.000050, spent: 2.9 secs\n",
      "epoch: 11, test auc: 0.633117\n",
      "epoch: 11, train loss: 4.5864, lr: 0.000050, spent: 3.1 secs\n",
      "epoch: 12, test auc: 0.633442\n",
      "epoch: 12, train loss: 4.5802, lr: 0.000050, spent: 3.6 secs\n",
      "epoch: 13, test auc: 0.637825\n",
      "epoch: 13, train loss: 4.5455, lr: 0.000050, spent: 3.8 secs\n",
      "epoch: 14, test auc: 0.637500\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 14, train loss: 4.5084, lr: 0.000050, spent: 4.1 secs\n",
      "epoch: 15, test auc: 0.639448\n",
      "epoch: 15, train loss: 4.5043, lr: 0.000050, spent: 4.3 secs\n",
      "epoch: 16, test auc: 0.642370\n",
      "epoch: 16, train loss: 4.4766, lr: 0.000050, spent: 4.6 secs\n",
      "epoch: 17, test auc: 0.650812\n",
      "epoch: 17, train loss: 4.4534, lr: 0.000050, spent: 4.8 secs\n",
      "epoch: 18, test auc: 0.646753\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 18, train loss: 4.4506, lr: 0.000050, spent: 5.2 secs\n",
      "epoch: 19, test auc: 0.644643\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 19, train loss: 4.4003, lr: 0.000050, spent: 5.4 secs\n",
      "epoch: 20, test auc: 0.650812\n",
      "epoch: 20, train loss: 4.3888, lr: 0.000050, spent: 5.7 secs\n",
      "epoch: 21, test auc: 0.653734\n",
      "epoch: 21, train loss: 4.3911, lr: 0.000050, spent: 5.9 secs\n",
      "epoch: 22, test auc: 0.658766\n",
      "epoch: 22, train loss: 4.3637, lr: 0.000050, spent: 6.2 secs\n",
      "epoch: 23, test auc: 0.661526\n",
      "epoch: 23, train loss: 4.3206, lr: 0.000050, spent: 6.4 secs\n",
      "epoch: 24, test auc: 0.662338\n",
      "epoch: 24, train loss: 4.3097, lr: 0.000050, spent: 6.7 secs\n",
      "epoch: 25, test auc: 0.662338\n",
      "epoch: 25, train loss: 4.2743, lr: 0.000050, spent: 7.1 secs\n",
      "epoch: 26, test auc: 0.665584\n",
      "epoch: 26, train loss: 4.2666, lr: 0.000050, spent: 7.4 secs\n",
      "epoch: 27, test auc: 0.675325\n",
      "epoch: 27, train loss: 4.2332, lr: 0.000050, spent: 7.6 secs\n",
      "epoch: 28, test auc: 0.677110\n",
      "epoch: 28, train loss: 4.2114, lr: 0.000050, spent: 7.9 secs\n",
      "epoch: 29, test auc: 0.681331\n",
      "epoch: 29, train loss: 4.2020, lr: 0.000050, spent: 8.1 secs\n",
      "epoch: 30, test auc: 0.679545\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 30, train loss: 4.1924, lr: 0.000050, spent: 8.3 secs\n",
      "epoch: 31, test auc: 0.675649\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 31, train loss: 4.1350, lr: 0.000050, spent: 8.5 secs\n",
      "epoch: 32, test auc: 0.679383\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 32, train loss: 4.1807, lr: 0.000050, spent: 8.8 secs\n",
      "epoch: 33, test auc: 0.671591\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 33, train loss: 4.1339, lr: 0.000050, spent: 9.2 secs\n",
      "epoch: 34, test auc: 0.680519\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 34, train loss: 4.1308, lr: 0.000050, spent: 9.4 secs\n",
      "epoch: 35, test auc: 0.676948\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 35, train loss: 4.1532, lr: 0.000050, spent: 9.6 secs\n",
      "epoch: 36, test auc: 0.676461\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 36, train loss: 4.0876, lr: 0.000050, spent: 9.8 secs\n",
      "epoch: 37, test auc: 0.673701\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 37, train loss: 4.0257, lr: 0.000050, spent: 10.0 secs\n",
      "epoch: 38, test auc: 0.690422\n",
      "epoch: 38, train loss: 4.0730, lr: 0.000050, spent: 10.3 secs\n",
      "epoch: 39, test auc: 0.681006\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 39, train loss: 4.0334, lr: 0.000050, spent: 10.5 secs\n",
      "epoch: 40, test auc: 0.687338\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 40, train loss: 4.0131, lr: 0.000050, spent: 10.9 secs\n",
      "epoch: 41, test auc: 0.692532\n",
      "epoch: 41, train loss: 4.0014, lr: 0.000050, spent: 11.1 secs\n",
      "epoch: 42, test auc: 0.684091\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 42, train loss: 4.0467, lr: 0.000050, spent: 11.4 secs\n",
      "epoch: 43, test auc: 0.697078\n",
      "epoch: 43, train loss: 3.9622, lr: 0.000050, spent: 11.6 secs\n",
      "epoch: 44, test auc: 0.677273\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 44, train loss: 3.9643, lr: 0.000050, spent: 11.8 secs\n",
      "epoch: 45, test auc: 0.693831\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 45, train loss: 3.9417, lr: 0.000050, spent: 12.0 secs\n",
      "epoch: 46, test auc: 0.689610\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 46, train loss: 3.9552, lr: 0.000050, spent: 12.3 secs\n",
      "epoch: 47, test auc: 0.697240\n",
      "epoch: 47, train loss: 3.9142, lr: 0.000050, spent: 12.7 secs\n",
      "epoch: 48, test auc: 0.688312\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 48, train loss: 3.9091, lr: 0.000050, spent: 12.9 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:23:50.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./pretrain_checkpoint2\u001b[0m\n",
      "\u001b[32m2025-10-16 20:23:50.350\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./pretrain_checkpoint2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49, test auc: 0.682305\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 49, train loss: 3.9282, lr: 0.000050, spent: 13.1 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:23:50.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 13.4 secs.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c6b328c81141d285fd14299c6be23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.663636\n",
      "epoch: 0, train loss: 4.8272, lr: 0.000050, spent: 0.3 secs\n",
      "epoch: 1, test auc: 0.653884\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 1, train loss: 4.8037, lr: 0.000050, spent: 0.5 secs\n",
      "epoch: 2, test auc: 0.650248\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 2, train loss: 4.8059, lr: 0.000050, spent: 0.7 secs\n",
      "epoch: 3, test auc: 0.649091\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 3, train loss: 4.7206, lr: 0.000050, spent: 0.9 secs\n",
      "epoch: 4, test auc: 0.660331\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 4, train loss: 4.7093, lr: 0.000050, spent: 1.3 secs\n",
      "epoch: 5, test auc: 0.659835\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 5, train loss: 4.6753, lr: 0.000050, spent: 1.5 secs\n",
      "epoch: 6, test auc: 0.661818\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 6, train loss: 4.6693, lr: 0.000050, spent: 1.7 secs\n",
      "epoch: 7, test auc: 0.661818\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 7, train loss: 4.6474, lr: 0.000050, spent: 2.0 secs\n",
      "epoch: 8, test auc: 0.666281\n",
      "epoch: 8, train loss: 4.6231, lr: 0.000050, spent: 2.2 secs\n",
      "epoch: 9, test auc: 0.664793\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 9, train loss: 4.5908, lr: 0.000050, spent: 2.4 secs\n",
      "epoch: 10, test auc: 0.658843\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 10, train loss: 4.5823, lr: 0.000050, spent: 2.6 secs\n",
      "epoch: 11, test auc: 0.662975\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 11, train loss: 4.5612, lr: 0.000050, spent: 3.0 secs\n",
      "epoch: 12, test auc: 0.662975\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 12, train loss: 4.5329, lr: 0.000050, spent: 3.2 secs\n",
      "epoch: 13, test auc: 0.664793\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 13, train loss: 4.5142, lr: 0.000050, spent: 3.5 secs\n",
      "epoch: 14, test auc: 0.662149\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 14, train loss: 4.5003, lr: 0.000050, spent: 3.7 secs\n",
      "epoch: 15, test auc: 0.663140\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 15, train loss: 4.5064, lr: 0.000050, spent: 3.9 secs\n",
      "epoch: 16, test auc: 0.670744\n",
      "epoch: 16, train loss: 4.5243, lr: 0.000050, spent: 4.1 secs\n",
      "epoch: 17, test auc: 0.669587\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 17, train loss: 4.4294, lr: 0.000050, spent: 4.3 secs\n",
      "epoch: 18, test auc: 0.676860\n",
      "epoch: 18, train loss: 4.4376, lr: 0.000050, spent: 4.8 secs\n",
      "epoch: 19, test auc: 0.680331\n",
      "epoch: 19, train loss: 4.3972, lr: 0.000050, spent: 5.0 secs\n",
      "epoch: 20, test auc: 0.681653\n",
      "epoch: 20, train loss: 4.3779, lr: 0.000050, spent: 5.3 secs\n",
      "epoch: 21, test auc: 0.687603\n",
      "epoch: 21, train loss: 4.3733, lr: 0.000050, spent: 5.5 secs\n",
      "epoch: 22, test auc: 0.690413\n",
      "epoch: 22, train loss: 4.3805, lr: 0.000050, spent: 5.8 secs\n",
      "epoch: 23, test auc: 0.688595\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 23, train loss: 4.4029, lr: 0.000050, spent: 6.0 secs\n",
      "epoch: 24, test auc: 0.688595\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 24, train loss: 4.2917, lr: 0.000050, spent: 6.2 secs\n",
      "epoch: 25, test auc: 0.688760\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 25, train loss: 4.3031, lr: 0.000050, spent: 6.6 secs\n",
      "epoch: 26, test auc: 0.692397\n",
      "epoch: 26, train loss: 4.2901, lr: 0.000050, spent: 6.9 secs\n",
      "epoch: 27, test auc: 0.693223\n",
      "epoch: 27, train loss: 4.3008, lr: 0.000050, spent: 7.1 secs\n",
      "epoch: 28, test auc: 0.697190\n",
      "epoch: 28, train loss: 4.2288, lr: 0.000050, spent: 7.4 secs\n",
      "epoch: 29, test auc: 0.708595\n",
      "epoch: 29, train loss: 4.2264, lr: 0.000050, spent: 7.6 secs\n",
      "epoch: 30, test auc: 0.701818\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 30, train loss: 4.2287, lr: 0.000050, spent: 7.8 secs\n",
      "epoch: 31, test auc: 0.712562\n",
      "epoch: 31, train loss: 4.2009, lr: 0.000050, spent: 8.1 secs\n",
      "epoch: 32, test auc: 0.714050\n",
      "epoch: 32, train loss: 4.1623, lr: 0.000050, spent: 8.5 secs\n",
      "epoch: 33, test auc: 0.713884\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 33, train loss: 4.1463, lr: 0.000050, spent: 8.7 secs\n",
      "epoch: 34, test auc: 0.711074\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 34, train loss: 4.1327, lr: 0.000050, spent: 9.0 secs\n",
      "epoch: 35, test auc: 0.712893\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 35, train loss: 4.1318, lr: 0.000050, spent: 9.2 secs\n",
      "epoch: 36, test auc: 0.709421\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 36, train loss: 4.0928, lr: 0.000050, spent: 9.4 secs\n",
      "epoch: 37, test auc: 0.713388\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 37, train loss: 4.0845, lr: 0.000050, spent: 9.6 secs\n",
      "epoch: 38, test auc: 0.701983\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 38, train loss: 4.0869, lr: 0.000050, spent: 9.8 secs\n",
      "epoch: 39, test auc: 0.709091\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 39, train loss: 4.1210, lr: 0.000050, spent: 10.2 secs\n",
      "epoch: 40, test auc: 0.715868\n",
      "epoch: 40, train loss: 4.0640, lr: 0.000050, spent: 10.5 secs\n",
      "epoch: 41, test auc: 0.712066\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 41, train loss: 4.0434, lr: 0.000050, spent: 10.7 secs\n",
      "epoch: 42, test auc: 0.704793\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 42, train loss: 4.0419, lr: 0.000050, spent: 10.9 secs\n",
      "epoch: 43, test auc: 0.703306\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 43, train loss: 4.0836, lr: 0.000050, spent: 11.1 secs\n",
      "epoch: 44, test auc: 0.711736\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 44, train loss: 4.0141, lr: 0.000050, spent: 11.3 secs\n",
      "epoch: 45, test auc: 0.704298\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 45, train loss: 4.0051, lr: 0.000050, spent: 11.5 secs\n",
      "epoch: 46, test auc: 0.707107\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 46, train loss: 3.9757, lr: 0.000050, spent: 11.7 secs\n",
      "epoch: 47, test auc: 0.712066\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 47, train loss: 3.9839, lr: 0.000050, spent: 12.2 secs\n",
      "epoch: 48, test auc: 0.710248\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 48, train loss: 4.0153, lr: 0.000050, spent: 12.4 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:24:03.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./pretrain_checkpoint2\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:03.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./pretrain_checkpoint2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49, test auc: 0.702810\n",
      "EarlyStopping counter: 9 out of 15\n",
      "epoch: 49, train loss: 3.9702, lr: 0.000050, spent: 12.6 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:24:03.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 12.8 secs.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65dfa51d86be4b048d01e6b6353c414a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.616033\n",
      "epoch: 0, train loss: 4.8654, lr: 0.000050, spent: 0.3 secs\n",
      "epoch: 1, test auc: 0.674380\n",
      "epoch: 1, train loss: 4.8958, lr: 0.000050, spent: 0.5 secs\n",
      "epoch: 2, test auc: 0.719008\n",
      "epoch: 2, train loss: 4.7971, lr: 0.000050, spent: 0.8 secs\n",
      "epoch: 3, test auc: 0.722975\n",
      "epoch: 3, train loss: 4.8167, lr: 0.000050, spent: 1.0 secs\n",
      "epoch: 4, test auc: 0.717521\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 4, train loss: 4.7530, lr: 0.000050, spent: 1.4 secs\n",
      "epoch: 5, test auc: 0.716364\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 5, train loss: 4.7429, lr: 0.000050, spent: 1.6 secs\n",
      "epoch: 6, test auc: 0.712893\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 6, train loss: 4.7161, lr: 0.000050, spent: 1.8 secs\n",
      "epoch: 7, test auc: 0.718843\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 7, train loss: 4.6960, lr: 0.000050, spent: 2.0 secs\n",
      "epoch: 8, test auc: 0.717355\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 8, train loss: 4.6779, lr: 0.000050, spent: 2.3 secs\n",
      "epoch: 9, test auc: 0.709587\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 9, train loss: 4.6733, lr: 0.000050, spent: 2.5 secs\n",
      "epoch: 10, test auc: 0.712893\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 10, train loss: 4.6875, lr: 0.000050, spent: 2.7 secs\n",
      "epoch: 11, test auc: 0.713388\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 11, train loss: 4.6118, lr: 0.000050, spent: 3.1 secs\n",
      "epoch: 12, test auc: 0.714876\n",
      "EarlyStopping counter: 9 out of 15\n",
      "epoch: 12, train loss: 4.6097, lr: 0.000050, spent: 3.3 secs\n",
      "epoch: 13, test auc: 0.718017\n",
      "EarlyStopping counter: 10 out of 15\n",
      "epoch: 13, train loss: 4.5989, lr: 0.000050, spent: 3.5 secs\n",
      "epoch: 14, test auc: 0.722314\n",
      "EarlyStopping counter: 11 out of 15\n",
      "epoch: 14, train loss: 4.5736, lr: 0.000050, spent: 3.7 secs\n",
      "epoch: 15, test auc: 0.725289\n",
      "epoch: 15, train loss: 4.5401, lr: 0.000050, spent: 4.0 secs\n",
      "epoch: 16, test auc: 0.723471\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 16, train loss: 4.5277, lr: 0.000050, spent: 4.2 secs\n",
      "epoch: 17, test auc: 0.730083\n",
      "epoch: 17, train loss: 4.5037, lr: 0.000050, spent: 4.5 secs\n",
      "epoch: 18, test auc: 0.730909\n",
      "epoch: 18, train loss: 4.4881, lr: 0.000050, spent: 4.9 secs\n",
      "epoch: 19, test auc: 0.735041\n",
      "epoch: 19, train loss: 4.4628, lr: 0.000050, spent: 5.1 secs\n",
      "epoch: 20, test auc: 0.740331\n",
      "epoch: 20, train loss: 4.4435, lr: 0.000050, spent: 5.4 secs\n",
      "epoch: 21, test auc: 0.738512\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 21, train loss: 4.4156, lr: 0.000050, spent: 5.6 secs\n",
      "epoch: 22, test auc: 0.743636\n",
      "epoch: 22, train loss: 4.4008, lr: 0.000050, spent: 5.9 secs\n",
      "epoch: 23, test auc: 0.753388\n",
      "epoch: 23, train loss: 4.3806, lr: 0.000050, spent: 6.1 secs\n",
      "epoch: 24, test auc: 0.754380\n",
      "epoch: 24, train loss: 4.3592, lr: 0.000050, spent: 6.4 secs\n",
      "epoch: 25, test auc: 0.757025\n",
      "epoch: 25, train loss: 4.3401, lr: 0.000050, spent: 6.8 secs\n",
      "epoch: 26, test auc: 0.756694\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 26, train loss: 4.3296, lr: 0.000050, spent: 7.0 secs\n",
      "epoch: 27, test auc: 0.756033\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 27, train loss: 4.3170, lr: 0.000050, spent: 7.2 secs\n",
      "epoch: 28, test auc: 0.757355\n",
      "epoch: 28, train loss: 4.2969, lr: 0.000050, spent: 7.5 secs\n",
      "epoch: 29, test auc: 0.754711\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 29, train loss: 4.2639, lr: 0.000050, spent: 7.7 secs\n",
      "epoch: 30, test auc: 0.762314\n",
      "epoch: 30, train loss: 4.2427, lr: 0.000050, spent: 8.0 secs\n",
      "epoch: 31, test auc: 0.769587\n",
      "epoch: 31, train loss: 4.2272, lr: 0.000050, spent: 8.2 secs\n",
      "epoch: 32, test auc: 0.776860\n",
      "epoch: 32, train loss: 4.2087, lr: 0.000050, spent: 8.6 secs\n",
      "epoch: 33, test auc: 0.775702\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 33, train loss: 4.1807, lr: 0.000050, spent: 8.9 secs\n",
      "epoch: 34, test auc: 0.778017\n",
      "epoch: 34, train loss: 4.1781, lr: 0.000050, spent: 9.1 secs\n",
      "epoch: 35, test auc: 0.782645\n",
      "epoch: 35, train loss: 4.1565, lr: 0.000050, spent: 9.4 secs\n",
      "epoch: 36, test auc: 0.780992\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 36, train loss: 4.1424, lr: 0.000050, spent: 9.6 secs\n",
      "epoch: 37, test auc: 0.785950\n",
      "epoch: 37, train loss: 4.1514, lr: 0.000050, spent: 9.8 secs\n",
      "epoch: 38, test auc: 0.785785\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 38, train loss: 4.2042, lr: 0.000050, spent: 10.0 secs\n",
      "epoch: 39, test auc: 0.778182\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 39, train loss: 4.2329, lr: 0.000050, spent: 10.4 secs\n",
      "epoch: 40, test auc: 0.773719\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 40, train loss: 4.1543, lr: 0.000050, spent: 10.7 secs\n",
      "epoch: 41, test auc: 0.780000\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 41, train loss: 4.0931, lr: 0.000050, spent: 10.9 secs\n",
      "epoch: 42, test auc: 0.777355\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 42, train loss: 4.1390, lr: 0.000050, spent: 11.1 secs\n",
      "epoch: 43, test auc: 0.782149\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 43, train loss: 4.0497, lr: 0.000050, spent: 11.3 secs\n",
      "epoch: 44, test auc: 0.777190\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 44, train loss: 4.0677, lr: 0.000050, spent: 11.5 secs\n",
      "epoch: 45, test auc: 0.782810\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 45, train loss: 4.0751, lr: 0.000050, spent: 11.7 secs\n",
      "epoch: 46, test auc: 0.787438\n",
      "epoch: 46, train loss: 4.0765, lr: 0.000050, spent: 12.2 secs\n",
      "epoch: 47, test auc: 0.780331\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 47, train loss: 4.0305, lr: 0.000050, spent: 12.4 secs\n",
      "epoch: 48, test auc: 0.779174\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 48, train loss: 4.0438, lr: 0.000050, spent: 12.6 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:24:16.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./pretrain_checkpoint2\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:16.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./pretrain_checkpoint2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49, test auc: 0.777521\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 49, train loss: 4.0643, lr: 0.000050, spent: 12.8 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:24:16.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 13.1 secs.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ed67b2f3144108aa1fb4928f4aa2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.510417\n",
      "epoch: 0, train loss: 5.1910, lr: 0.000050, spent: 0.3 secs\n",
      "epoch: 1, test auc: 0.641700\n",
      "epoch: 1, train loss: 4.9697, lr: 0.000050, spent: 0.5 secs\n",
      "epoch: 2, test auc: 0.623016\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 2, train loss: 4.8536, lr: 0.000050, spent: 0.7 secs\n",
      "epoch: 3, test auc: 0.620370\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 3, train loss: 4.8207, lr: 0.000050, spent: 1.1 secs\n",
      "epoch: 4, test auc: 0.618882\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 4, train loss: 4.7825, lr: 0.000050, spent: 1.3 secs\n",
      "epoch: 5, test auc: 0.629795\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 5, train loss: 4.7284, lr: 0.000050, spent: 1.5 secs\n",
      "epoch: 6, test auc: 0.638062\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 6, train loss: 4.6944, lr: 0.000050, spent: 1.8 secs\n",
      "epoch: 7, test auc: 0.640046\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 7, train loss: 4.6695, lr: 0.000050, spent: 2.0 secs\n",
      "epoch: 8, test auc: 0.649967\n",
      "epoch: 8, train loss: 4.6463, lr: 0.000050, spent: 2.2 secs\n",
      "epoch: 9, test auc: 0.644841\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 9, train loss: 4.6305, lr: 0.000050, spent: 2.4 secs\n",
      "epoch: 10, test auc: 0.654597\n",
      "epoch: 10, train loss: 4.5889, lr: 0.000050, spent: 2.7 secs\n",
      "epoch: 11, test auc: 0.648148\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 11, train loss: 4.5598, lr: 0.000050, spent: 3.1 secs\n",
      "epoch: 12, test auc: 0.642526\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 12, train loss: 4.5385, lr: 0.000050, spent: 3.3 secs\n",
      "epoch: 13, test auc: 0.638889\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 13, train loss: 4.5150, lr: 0.000050, spent: 3.5 secs\n",
      "epoch: 14, test auc: 0.642030\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 14, train loss: 4.4955, lr: 0.000050, spent: 3.7 secs\n",
      "epoch: 15, test auc: 0.644676\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 15, train loss: 4.4829, lr: 0.000050, spent: 4.0 secs\n",
      "epoch: 16, test auc: 0.645172\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 16, train loss: 4.4428, lr: 0.000050, spent: 4.2 secs\n",
      "epoch: 17, test auc: 0.654266\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 17, train loss: 4.4169, lr: 0.000050, spent: 4.4 secs\n",
      "epoch: 18, test auc: 0.648313\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 18, train loss: 4.3992, lr: 0.000050, spent: 4.8 secs\n",
      "epoch: 19, test auc: 0.645668\n",
      "EarlyStopping counter: 9 out of 15\n",
      "epoch: 19, train loss: 4.3772, lr: 0.000050, spent: 5.0 secs\n",
      "epoch: 20, test auc: 0.648810\n",
      "EarlyStopping counter: 10 out of 15\n",
      "epoch: 20, train loss: 4.3552, lr: 0.000050, spent: 5.2 secs\n",
      "epoch: 21, test auc: 0.650298\n",
      "EarlyStopping counter: 11 out of 15\n",
      "epoch: 21, train loss: 4.3590, lr: 0.000050, spent: 5.4 secs\n",
      "epoch: 22, test auc: 0.664187\n",
      "epoch: 22, train loss: 4.3262, lr: 0.000050, spent: 5.7 secs\n",
      "epoch: 23, test auc: 0.658069\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 23, train loss: 4.3423, lr: 0.000050, spent: 5.9 secs\n",
      "epoch: 24, test auc: 0.668651\n",
      "epoch: 24, train loss: 4.3091, lr: 0.000050, spent: 6.2 secs\n",
      "epoch: 25, test auc: 0.673611\n",
      "epoch: 25, train loss: 4.2824, lr: 0.000050, spent: 6.6 secs\n",
      "epoch: 26, test auc: 0.675926\n",
      "epoch: 26, train loss: 4.2720, lr: 0.000050, spent: 6.9 secs\n",
      "epoch: 27, test auc: 0.686177\n",
      "epoch: 27, train loss: 4.2876, lr: 0.000050, spent: 7.1 secs\n",
      "epoch: 28, test auc: 0.679398\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 28, train loss: 4.2717, lr: 0.000050, spent: 7.3 secs\n",
      "epoch: 29, test auc: 0.690311\n",
      "epoch: 29, train loss: 4.2068, lr: 0.000050, spent: 7.6 secs\n",
      "epoch: 30, test auc: 0.696098\n",
      "epoch: 30, train loss: 4.2099, lr: 0.000050, spent: 7.8 secs\n",
      "epoch: 31, test auc: 0.694114\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 31, train loss: 4.1824, lr: 0.000050, spent: 8.0 secs\n",
      "epoch: 32, test auc: 0.713128\n",
      "epoch: 32, train loss: 4.1614, lr: 0.000050, spent: 8.5 secs\n",
      "epoch: 33, test auc: 0.719081\n",
      "epoch: 33, train loss: 4.1467, lr: 0.000050, spent: 8.7 secs\n",
      "epoch: 34, test auc: 0.724702\n",
      "epoch: 34, train loss: 4.1484, lr: 0.000050, spent: 9.0 secs\n",
      "epoch: 35, test auc: 0.741402\n",
      "epoch: 35, train loss: 4.1297, lr: 0.000050, spent: 9.2 secs\n",
      "epoch: 36, test auc: 0.749504\n",
      "epoch: 36, train loss: 4.0965, lr: 0.000050, spent: 9.5 secs\n",
      "epoch: 37, test auc: 0.753307\n",
      "epoch: 37, train loss: 4.0992, lr: 0.000050, spent: 9.7 secs\n",
      "epoch: 38, test auc: 0.758433\n",
      "epoch: 38, train loss: 4.1395, lr: 0.000050, spent: 10.0 secs\n",
      "epoch: 39, test auc: 0.761409\n",
      "epoch: 39, train loss: 4.1325, lr: 0.000050, spent: 10.4 secs\n",
      "epoch: 40, test auc: 0.758102\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 40, train loss: 4.0605, lr: 0.000050, spent: 10.6 secs\n",
      "epoch: 41, test auc: 0.769180\n",
      "epoch: 41, train loss: 4.0484, lr: 0.000050, spent: 10.9 secs\n",
      "epoch: 42, test auc: 0.761574\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 42, train loss: 4.0635, lr: 0.000050, spent: 11.1 secs\n",
      "epoch: 43, test auc: 0.771825\n",
      "epoch: 43, train loss: 4.0821, lr: 0.000050, spent: 11.3 secs\n",
      "epoch: 44, test auc: 0.774471\n",
      "epoch: 44, train loss: 4.0604, lr: 0.000050, spent: 11.6 secs\n",
      "epoch: 45, test auc: 0.771164\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 45, train loss: 4.0053, lr: 0.000050, spent: 11.8 secs\n",
      "epoch: 46, test auc: 0.770999\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 46, train loss: 3.9960, lr: 0.000050, spent: 12.2 secs\n",
      "epoch: 47, test auc: 0.776455\n",
      "epoch: 47, train loss: 4.0024, lr: 0.000050, spent: 12.5 secs\n",
      "epoch: 48, test auc: 0.762070\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 48, train loss: 4.0209, lr: 0.000050, spent: 12.7 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:24:29.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./pretrain_checkpoint2\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:29.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./pretrain_checkpoint2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49, test auc: 0.763558\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 49, train loss: 4.0611, lr: 0.000050, spent: 12.9 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:24:30.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 13.1 secs.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47f2615584b4744b2a925befbd0adce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.605820\n",
      "epoch: 0, train loss: 4.8167, lr: 0.000050, spent: 0.3 secs\n",
      "epoch: 1, test auc: 0.593750\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 1, train loss: 4.7562, lr: 0.000050, spent: 0.5 secs\n",
      "epoch: 2, test auc: 0.593915\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 2, train loss: 4.7062, lr: 0.000050, spent: 0.7 secs\n",
      "epoch: 3, test auc: 0.588459\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 3, train loss: 4.7017, lr: 0.000050, spent: 1.1 secs\n",
      "epoch: 4, test auc: 0.591435\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 4, train loss: 4.6452, lr: 0.000050, spent: 1.3 secs\n",
      "epoch: 5, test auc: 0.590443\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 5, train loss: 4.6226, lr: 0.000050, spent: 1.5 secs\n",
      "epoch: 6, test auc: 0.591931\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 6, train loss: 4.5934, lr: 0.000050, spent: 1.7 secs\n",
      "epoch: 7, test auc: 0.592758\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 7, train loss: 4.5832, lr: 0.000050, spent: 2.0 secs\n",
      "epoch: 8, test auc: 0.593750\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 8, train loss: 4.5735, lr: 0.000050, spent: 2.2 secs\n",
      "epoch: 9, test auc: 0.596065\n",
      "EarlyStopping counter: 9 out of 15\n",
      "epoch: 9, train loss: 4.5345, lr: 0.000050, spent: 2.4 secs\n",
      "epoch: 10, test auc: 0.594907\n",
      "EarlyStopping counter: 10 out of 15\n",
      "epoch: 10, train loss: 4.5090, lr: 0.000050, spent: 2.8 secs\n",
      "epoch: 11, test auc: 0.594577\n",
      "EarlyStopping counter: 11 out of 15\n",
      "epoch: 11, train loss: 4.5288, lr: 0.000050, spent: 3.0 secs\n",
      "epoch: 12, test auc: 0.594907\n",
      "EarlyStopping counter: 12 out of 15\n",
      "epoch: 12, train loss: 4.4839, lr: 0.000050, spent: 3.2 secs\n",
      "epoch: 13, test auc: 0.598380\n",
      "EarlyStopping counter: 13 out of 15\n",
      "epoch: 13, train loss: 4.4568, lr: 0.000050, spent: 3.4 secs\n",
      "epoch: 14, test auc: 0.598380\n",
      "EarlyStopping counter: 14 out of 15\n",
      "epoch: 14, train loss: 4.4505, lr: 0.000050, spent: 3.6 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:24:34.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./pretrain_checkpoint2\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:34.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./pretrain_checkpoint2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, test auc: 0.601521\n",
      "EarlyStopping counter: 15 out of 15\n",
      "early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:24:34.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 4.1 secs.\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:34.369\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36m_check_column_overlap\u001b[0m:\u001b[36m254\u001b[0m - \u001b[33m\u001b[1mNo cat/num/bin cols specified, will take ALL columns as categorical! Ignore this warning if you specify the `checkpoint` to load the model.\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:34.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m774\u001b[0m - \u001b[1mmissing keys: []\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:34.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m775\u001b[0m - \u001b[1munexpected keys: []\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:34.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m776\u001b[0m - \u001b[1mload model from /home/kutaytire/RL_training/pretrain_checkpoint2\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:34.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mload feature extractor from /home/kutaytire/RL_training/pretrain_checkpoint2/extractor/extractor.json\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1e1696526548039e7f6f85577acb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.802885\n",
      "epoch: 0, train loss: 4.7648, lr: 0.000100, spent: 0.3 secs\n",
      "epoch: 1, test auc: 0.810842\n",
      "epoch: 1, train loss: 4.5930, lr: 0.000100, spent: 0.7 secs\n",
      "epoch: 2, test auc: 0.815153\n",
      "epoch: 2, train loss: 4.3734, lr: 0.000100, spent: 1.0 secs\n",
      "epoch: 3, test auc: 0.816479\n",
      "epoch: 3, train loss: 4.2106, lr: 0.000100, spent: 1.2 secs\n",
      "epoch: 4, test auc: 0.818634\n",
      "epoch: 4, train loss: 3.9683, lr: 0.000100, spent: 1.5 secs\n",
      "epoch: 5, test auc: 0.821950\n",
      "epoch: 5, train loss: 3.7641, lr: 0.000100, spent: 1.7 secs\n",
      "epoch: 6, test auc: 0.854775\n",
      "epoch: 6, train loss: 3.5534, lr: 0.000100, spent: 2.0 secs\n",
      "epoch: 7, test auc: 0.860908\n",
      "epoch: 7, train loss: 3.3435, lr: 0.000100, spent: 2.2 secs\n",
      "epoch: 8, test auc: 0.865219\n",
      "epoch: 8, train loss: 3.0710, lr: 0.000100, spent: 2.6 secs\n",
      "epoch: 9, test auc: 0.871353\n",
      "epoch: 9, train loss: 3.0247, lr: 0.000100, spent: 2.9 secs\n",
      "epoch: 10, test auc: 0.876658\n",
      "epoch: 10, train loss: 2.8208, lr: 0.000100, spent: 3.1 secs\n",
      "epoch: 11, test auc: 0.872016\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 11, train loss: 2.8682, lr: 0.000100, spent: 3.4 secs\n",
      "epoch: 12, test auc: 0.870855\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 12, train loss: 2.8245, lr: 0.000100, spent: 3.6 secs\n",
      "epoch: 13, test auc: 0.874503\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 13, train loss: 2.8057, lr: 0.000100, spent: 3.8 secs\n",
      "epoch: 14, test auc: 0.870192\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch: 14, train loss: 2.7647, lr: 0.000100, spent: 4.0 secs\n",
      "epoch: 15, test auc: 0.869529\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch: 15, train loss: 2.7373, lr: 0.000100, spent: 4.4 secs\n",
      "epoch: 16, test auc: 0.869861\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch: 16, train loss: 2.7989, lr: 0.000100, spent: 4.6 secs\n",
      "epoch: 17, test auc: 0.869363\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch: 17, train loss: 2.7716, lr: 0.000100, spent: 4.8 secs\n",
      "epoch: 18, test auc: 0.865385\n",
      "EarlyStopping counter: 8 out of 10\n",
      "epoch: 18, train loss: 2.7599, lr: 0.000100, spent: 5.0 secs\n",
      "epoch: 19, test auc: 0.868700\n",
      "EarlyStopping counter: 9 out of 10\n",
      "epoch: 19, train loss: 2.7818, lr: 0.000100, spent: 5.3 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:24:39.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:39.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, test auc: 0.863727\n",
      "EarlyStopping counter: 10 out of 10\n",
      "early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:24:40.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 5.7 secs.\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:40.286\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36m_check_column_overlap\u001b[0m:\u001b[36m254\u001b[0m - \u001b[33m\u001b[1mNo cat/num/bin cols specified, will take ALL columns as categorical! Ignore this warning if you specify the `checkpoint` to load the model.\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:40.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m774\u001b[0m - \u001b[1mmissing keys: []\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:40.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m775\u001b[0m - \u001b[1munexpected keys: []\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:40.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m776\u001b[0m - \u001b[1mload model from /home/kutaytire/RL_training/pretrain_checkpoint2\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:40.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mload feature extractor from /home/kutaytire/RL_training/pretrain_checkpoint2/extractor/extractor.json\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f0c930411e43f8bfede73b51a99704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.756963\n",
      "epoch: 0, train loss: 4.7419, lr: 0.000100, spent: 0.3 secs\n",
      "epoch: 1, test auc: 0.756466\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 1, train loss: 4.4536, lr: 0.000100, spent: 0.7 secs\n",
      "epoch: 2, test auc: 0.759781\n",
      "epoch: 2, train loss: 4.2365, lr: 0.000100, spent: 0.9 secs\n",
      "epoch: 3, test auc: 0.771718\n",
      "epoch: 3, train loss: 4.0506, lr: 0.000100, spent: 1.2 secs\n",
      "epoch: 4, test auc: 0.773707\n",
      "epoch: 4, train loss: 3.8150, lr: 0.000100, spent: 1.4 secs\n",
      "epoch: 5, test auc: 0.757626\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 5, train loss: 3.6104, lr: 0.000100, spent: 1.7 secs\n",
      "epoch: 6, test auc: 0.807029\n",
      "epoch: 6, train loss: 3.3627, lr: 0.000100, spent: 1.9 secs\n",
      "epoch: 7, test auc: 0.813992\n",
      "epoch: 7, train loss: 3.1306, lr: 0.000100, spent: 2.1 secs\n",
      "epoch: 8, test auc: 0.819131\n",
      "epoch: 8, train loss: 2.9406, lr: 0.000100, spent: 2.6 secs\n",
      "epoch: 9, test auc: 0.822944\n",
      "epoch: 9, train loss: 2.8209, lr: 0.000100, spent: 2.8 secs\n",
      "epoch: 10, test auc: 0.824934\n",
      "epoch: 10, train loss: 2.8220, lr: 0.000100, spent: 3.1 secs\n",
      "epoch: 11, test auc: 0.826260\n",
      "epoch: 11, train loss: 2.8456, lr: 0.000100, spent: 3.3 secs\n",
      "epoch: 12, test auc: 0.833554\n",
      "epoch: 12, train loss: 2.7800, lr: 0.000100, spent: 3.6 secs\n",
      "epoch: 13, test auc: 0.833886\n",
      "epoch: 13, train loss: 2.7877, lr: 0.000100, spent: 3.8 secs\n",
      "epoch: 14, test auc: 0.834052\n",
      "epoch: 14, train loss: 2.7677, lr: 0.000100, spent: 4.1 secs\n",
      "epoch: 15, test auc: 0.828747\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 15, train loss: 2.7218, lr: 0.000100, spent: 4.3 secs\n",
      "epoch: 16, test auc: 0.826260\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 16, train loss: 2.7120, lr: 0.000100, spent: 4.7 secs\n",
      "epoch: 17, test auc: 0.829576\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 17, train loss: 2.6902, lr: 0.000100, spent: 4.9 secs\n",
      "epoch: 18, test auc: 0.830902\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch: 18, train loss: 2.6698, lr: 0.000100, spent: 5.1 secs\n",
      "epoch: 19, test auc: 0.833554\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch: 19, train loss: 2.6734, lr: 0.000100, spent: 5.4 secs\n",
      "epoch: 20, test auc: 0.833720\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch: 20, train loss: 2.6900, lr: 0.000100, spent: 5.6 secs\n",
      "epoch: 21, test auc: 0.834383\n",
      "epoch: 21, train loss: 2.7334, lr: 0.000100, spent: 5.8 secs\n",
      "epoch: 22, test auc: 0.834715\n",
      "epoch: 22, train loss: 2.6769, lr: 0.000100, spent: 6.1 secs\n",
      "epoch: 23, test auc: 0.830239\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 23, train loss: 2.6885, lr: 0.000100, spent: 6.5 secs\n",
      "epoch: 24, test auc: 0.830405\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 24, train loss: 2.6824, lr: 0.000100, spent: 6.7 secs\n",
      "epoch: 25, test auc: 0.834052\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 25, train loss: 2.6563, lr: 0.000100, spent: 6.9 secs\n",
      "epoch: 26, test auc: 0.836373\n",
      "epoch: 26, train loss: 2.6310, lr: 0.000100, spent: 7.2 secs\n",
      "epoch: 27, test auc: 0.833554\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 27, train loss: 2.6243, lr: 0.000100, spent: 7.4 secs\n",
      "epoch: 28, test auc: 0.834218\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 28, train loss: 2.6519, lr: 0.000100, spent: 7.6 secs\n",
      "epoch: 29, test auc: 0.834549\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 29, train loss: 2.6659, lr: 0.000100, spent: 7.8 secs\n",
      "epoch: 30, test auc: 0.837533\n",
      "epoch: 30, train loss: 2.8181, lr: 0.000100, spent: 8.3 secs\n",
      "epoch: 31, test auc: 0.836538\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 31, train loss: 2.7175, lr: 0.000100, spent: 8.5 secs\n",
      "epoch: 32, test auc: 0.837865\n",
      "epoch: 32, train loss: 2.6629, lr: 0.000100, spent: 8.7 secs\n",
      "epoch: 33, test auc: 0.835544\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 33, train loss: 2.6255, lr: 0.000100, spent: 9.0 secs\n",
      "epoch: 34, test auc: 0.832891\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 34, train loss: 2.6157, lr: 0.000100, spent: 9.2 secs\n",
      "epoch: 35, test auc: 0.833389\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 35, train loss: 2.6354, lr: 0.000100, spent: 9.4 secs\n",
      "epoch: 36, test auc: 0.832228\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch: 36, train loss: 2.6105, lr: 0.000100, spent: 9.6 secs\n",
      "epoch: 37, test auc: 0.833554\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch: 37, train loss: 2.6251, lr: 0.000100, spent: 10.0 secs\n",
      "epoch: 38, test auc: 0.833389\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch: 38, train loss: 2.6205, lr: 0.000100, spent: 10.2 secs\n",
      "epoch: 39, test auc: 0.833223\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch: 39, train loss: 2.6108, lr: 0.000100, spent: 10.4 secs\n",
      "epoch: 40, test auc: 0.835544\n",
      "EarlyStopping counter: 8 out of 10\n",
      "epoch: 40, train loss: 2.6483, lr: 0.000100, spent: 10.6 secs\n",
      "epoch: 41, test auc: 0.833720\n",
      "EarlyStopping counter: 9 out of 10\n",
      "epoch: 41, train loss: 2.6761, lr: 0.000100, spent: 10.9 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:24:51.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:51.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 42, test auc: 0.835710\n",
      "EarlyStopping counter: 10 out of 10\n",
      "early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:24:51.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 11.3 secs.\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:51.824\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36m_check_column_overlap\u001b[0m:\u001b[36m254\u001b[0m - \u001b[33m\u001b[1mNo cat/num/bin cols specified, will take ALL columns as categorical! Ignore this warning if you specify the `checkpoint` to load the model.\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:51.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m774\u001b[0m - \u001b[1mmissing keys: []\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:51.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m775\u001b[0m - \u001b[1munexpected keys: []\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:51.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m776\u001b[0m - \u001b[1mload model from /home/kutaytire/RL_training/pretrain_checkpoint2\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:51.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mload feature extractor from /home/kutaytire/RL_training/pretrain_checkpoint2/extractor/extractor.json\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4b8154bd8f4cafb7e9991526f3869c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.772878\n",
      "epoch: 0, train loss: 4.7138, lr: 0.000100, spent: 0.3 secs\n",
      "epoch: 1, test auc: 0.799403\n",
      "epoch: 1, train loss: 4.4969, lr: 0.000100, spent: 0.7 secs\n",
      "epoch: 2, test auc: 0.804708\n",
      "epoch: 2, train loss: 4.3255, lr: 0.000100, spent: 1.0 secs\n",
      "epoch: 3, test auc: 0.827752\n",
      "epoch: 3, train loss: 4.1290, lr: 0.000100, spent: 1.2 secs\n",
      "epoch: 4, test auc: 0.839191\n",
      "epoch: 4, train loss: 3.9131, lr: 0.000100, spent: 1.5 secs\n",
      "epoch: 5, test auc: 0.848806\n",
      "epoch: 5, train loss: 3.8191, lr: 0.000100, spent: 1.7 secs\n",
      "epoch: 6, test auc: 0.881134\n",
      "epoch: 6, train loss: 3.5419, lr: 0.000100, spent: 2.0 secs\n",
      "epoch: 7, test auc: 0.889423\n",
      "epoch: 7, train loss: 3.3658, lr: 0.000100, spent: 2.2 secs\n",
      "epoch: 8, test auc: 0.893070\n",
      "epoch: 8, train loss: 3.1594, lr: 0.000100, spent: 2.5 secs\n",
      "epoch: 9, test auc: 0.892407\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 9, train loss: 3.0038, lr: 0.000100, spent: 2.9 secs\n",
      "epoch: 10, test auc: 0.893568\n",
      "epoch: 10, train loss: 2.8913, lr: 0.000100, spent: 3.1 secs\n",
      "epoch: 11, test auc: 0.891910\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 11, train loss: 2.8876, lr: 0.000100, spent: 3.3 secs\n",
      "epoch: 12, test auc: 0.897381\n",
      "epoch: 12, train loss: 2.9340, lr: 0.000100, spent: 3.6 secs\n",
      "epoch: 13, test auc: 0.900531\n",
      "epoch: 13, train loss: 2.8541, lr: 0.000100, spent: 3.8 secs\n",
      "epoch: 14, test auc: 0.901028\n",
      "epoch: 14, train loss: 2.9020, lr: 0.000100, spent: 4.1 secs\n",
      "epoch: 15, test auc: 0.900199\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 15, train loss: 2.8561, lr: 0.000100, spent: 4.3 secs\n",
      "epoch: 16, test auc: 0.900365\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 16, train loss: 2.8418, lr: 0.000100, spent: 4.7 secs\n",
      "epoch: 17, test auc: 0.900199\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 17, train loss: 2.8569, lr: 0.000100, spent: 4.9 secs\n",
      "epoch: 18, test auc: 0.898210\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch: 18, train loss: 2.9890, lr: 0.000100, spent: 5.1 secs\n",
      "epoch: 19, test auc: 0.903017\n",
      "epoch: 19, train loss: 2.9692, lr: 0.000100, spent: 5.4 secs\n",
      "epoch: 20, test auc: 0.903349\n",
      "epoch: 20, train loss: 2.8376, lr: 0.000100, spent: 5.6 secs\n",
      "epoch: 21, test auc: 0.902023\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 21, train loss: 2.7920, lr: 0.000100, spent: 5.8 secs\n",
      "epoch: 22, test auc: 0.899204\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 22, train loss: 2.7832, lr: 0.000100, spent: 6.0 secs\n",
      "epoch: 23, test auc: 0.899702\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 23, train loss: 2.7708, lr: 0.000100, spent: 6.5 secs\n",
      "epoch: 24, test auc: 0.900033\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch: 24, train loss: 2.7647, lr: 0.000100, spent: 6.7 secs\n",
      "epoch: 25, test auc: 0.898210\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch: 25, train loss: 2.7751, lr: 0.000100, spent: 6.9 secs\n",
      "epoch: 26, test auc: 0.898375\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch: 26, train loss: 2.7454, lr: 0.000100, spent: 7.1 secs\n",
      "epoch: 27, test auc: 0.897712\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch: 27, train loss: 2.7505, lr: 0.000100, spent: 7.3 secs\n",
      "epoch: 28, test auc: 0.898707\n",
      "EarlyStopping counter: 8 out of 10\n",
      "epoch: 28, train loss: 2.7447, lr: 0.000100, spent: 7.5 secs\n",
      "epoch: 29, test auc: 0.897049\n",
      "EarlyStopping counter: 9 out of 10\n",
      "epoch: 29, train loss: 2.7296, lr: 0.000100, spent: 7.7 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:24:59.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:24:59.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30, test auc: 0.897546\n",
      "EarlyStopping counter: 10 out of 10\n",
      "early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:25:00.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 8.2 secs.\u001b[0m\n",
      "\u001b[32m2025-10-16 20:25:00.229\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36m_check_column_overlap\u001b[0m:\u001b[36m254\u001b[0m - \u001b[33m\u001b[1mNo cat/num/bin cols specified, will take ALL columns as categorical! Ignore this warning if you specify the `checkpoint` to load the model.\u001b[0m\n",
      "\u001b[32m2025-10-16 20:25:00.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m774\u001b[0m - \u001b[1mmissing keys: []\u001b[0m\n",
      "\u001b[32m2025-10-16 20:25:00.302\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m775\u001b[0m - \u001b[1munexpected keys: []\u001b[0m\n",
      "\u001b[32m2025-10-16 20:25:00.302\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m776\u001b[0m - \u001b[1mload model from /home/kutaytire/RL_training/pretrain_checkpoint2\u001b[0m\n",
      "\u001b[32m2025-10-16 20:25:00.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mload feature extractor from /home/kutaytire/RL_training/pretrain_checkpoint2/extractor/extractor.json\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fe10a5b949410cb56b6a3559cf65e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.773585\n",
      "epoch: 0, train loss: 4.6277, lr: 0.000100, spent: 0.5 secs\n",
      "epoch: 1, test auc: 0.777888\n",
      "epoch: 1, train loss: 4.5104, lr: 0.000100, spent: 0.7 secs\n",
      "epoch: 2, test auc: 0.783515\n",
      "epoch: 2, train loss: 4.3565, lr: 0.000100, spent: 0.9 secs\n",
      "epoch: 3, test auc: 0.800397\n",
      "epoch: 3, train loss: 4.0909, lr: 0.000100, spent: 1.2 secs\n",
      "epoch: 4, test auc: 0.814465\n",
      "epoch: 4, train loss: 3.8884, lr: 0.000100, spent: 1.4 secs\n",
      "epoch: 5, test auc: 0.826216\n",
      "epoch: 5, train loss: 3.7096, lr: 0.000100, spent: 1.7 secs\n",
      "epoch: 6, test auc: 0.861470\n",
      "epoch: 6, train loss: 3.4733, lr: 0.000100, spent: 2.2 secs\n",
      "epoch: 7, test auc: 0.867097\n",
      "epoch: 7, train loss: 3.2578, lr: 0.000100, spent: 2.4 secs\n",
      "epoch: 8, test auc: 0.875041\n",
      "epoch: 8, train loss: 2.9962, lr: 0.000100, spent: 2.7 secs\n",
      "epoch: 9, test auc: 0.872724\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 9, train loss: 3.0764, lr: 0.000100, spent: 2.9 secs\n",
      "epoch: 10, test auc: 0.874379\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 10, train loss: 2.9119, lr: 0.000100, spent: 3.1 secs\n",
      "epoch: 11, test auc: 0.876034\n",
      "epoch: 11, train loss: 2.8810, lr: 0.000100, spent: 3.4 secs\n",
      "epoch: 12, test auc: 0.877358\n",
      "epoch: 12, train loss: 2.9407, lr: 0.000100, spent: 3.6 secs\n",
      "epoch: 13, test auc: 0.876696\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 13, train loss: 2.9052, lr: 0.000100, spent: 3.8 secs\n",
      "epoch: 14, test auc: 0.875372\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 14, train loss: 2.8558, lr: 0.000100, spent: 4.3 secs\n",
      "epoch: 15, test auc: 0.875207\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 15, train loss: 3.0029, lr: 0.000100, spent: 4.5 secs\n",
      "epoch: 16, test auc: 0.876862\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch: 16, train loss: 2.9120, lr: 0.000100, spent: 4.7 secs\n",
      "epoch: 17, test auc: 0.875372\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch: 17, train loss: 2.9706, lr: 0.000100, spent: 4.9 secs\n",
      "epoch: 18, test auc: 0.875041\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch: 18, train loss: 2.8853, lr: 0.000100, spent: 5.1 secs\n",
      "epoch: 19, test auc: 0.875703\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch: 19, train loss: 2.8191, lr: 0.000100, spent: 5.3 secs\n",
      "epoch: 20, test auc: 0.876531\n",
      "EarlyStopping counter: 8 out of 10\n",
      "epoch: 20, train loss: 2.8084, lr: 0.000100, spent: 5.5 secs\n",
      "epoch: 21, test auc: 0.878021\n",
      "epoch: 21, train loss: 2.7835, lr: 0.000100, spent: 6.0 secs\n",
      "epoch: 22, test auc: 0.876365\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 22, train loss: 2.8009, lr: 0.000100, spent: 6.2 secs\n",
      "epoch: 23, test auc: 0.874876\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 23, train loss: 2.7829, lr: 0.000100, spent: 6.4 secs\n",
      "epoch: 24, test auc: 0.875372\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 24, train loss: 2.7956, lr: 0.000100, spent: 6.6 secs\n",
      "epoch: 25, test auc: 0.877358\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch: 25, train loss: 2.7751, lr: 0.000100, spent: 6.9 secs\n",
      "epoch: 26, test auc: 0.874048\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch: 26, train loss: 2.7591, lr: 0.000100, spent: 7.1 secs\n",
      "epoch: 27, test auc: 0.877193\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch: 27, train loss: 2.7998, lr: 0.000100, spent: 7.3 secs\n",
      "epoch: 28, test auc: 0.875869\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch: 28, train loss: 2.7450, lr: 0.000100, spent: 7.7 secs\n",
      "epoch: 29, test auc: 0.874876\n",
      "EarlyStopping counter: 8 out of 10\n",
      "epoch: 29, train loss: 2.7836, lr: 0.000100, spent: 7.9 secs\n",
      "epoch: 30, test auc: 0.876531\n",
      "EarlyStopping counter: 9 out of 10\n",
      "epoch: 30, train loss: 2.7886, lr: 0.000100, spent: 8.1 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:25:08.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:25:08.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31, test auc: 0.873717\n",
      "EarlyStopping counter: 10 out of 10\n",
      "early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:25:08.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 8.6 secs.\u001b[0m\n",
      "\u001b[32m2025-10-16 20:25:08.995\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36m_check_column_overlap\u001b[0m:\u001b[36m254\u001b[0m - \u001b[33m\u001b[1mNo cat/num/bin cols specified, will take ALL columns as categorical! Ignore this warning if you specify the `checkpoint` to load the model.\u001b[0m\n",
      "\u001b[32m2025-10-16 20:25:09.071\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m774\u001b[0m - \u001b[1mmissing keys: []\u001b[0m\n",
      "\u001b[32m2025-10-16 20:25:09.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m775\u001b[0m - \u001b[1munexpected keys: []\u001b[0m\n",
      "\u001b[32m2025-10-16 20:25:09.073\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m776\u001b[0m - \u001b[1mload model from /home/kutaytire/RL_training/pretrain_checkpoint2\u001b[0m\n",
      "\u001b[32m2025-10-16 20:25:09.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.modeling_transtab\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mload feature extractor from /home/kutaytire/RL_training/pretrain_checkpoint2/extractor/extractor.json\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca51334724d4b8898d874c8b24aba70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.775641\n",
      "epoch: 0, train loss: 4.7859, lr: 0.000100, spent: 0.3 secs\n",
      "epoch: 1, test auc: 0.789642\n",
      "epoch: 1, train loss: 4.5862, lr: 0.000100, spent: 0.5 secs\n",
      "epoch: 2, test auc: 0.790317\n",
      "epoch: 2, train loss: 4.4125, lr: 0.000100, spent: 0.8 secs\n",
      "epoch: 3, test auc: 0.796559\n",
      "epoch: 3, train loss: 4.2559, lr: 0.000100, spent: 1.2 secs\n",
      "epoch: 4, test auc: 0.819332\n",
      "epoch: 4, train loss: 4.0262, lr: 0.000100, spent: 1.5 secs\n",
      "epoch: 5, test auc: 0.847503\n",
      "epoch: 5, train loss: 3.8962, lr: 0.000100, spent: 1.7 secs\n",
      "epoch: 6, test auc: 0.868084\n",
      "epoch: 6, train loss: 3.6878, lr: 0.000100, spent: 2.0 secs\n",
      "epoch: 7, test auc: 0.896255\n",
      "epoch: 7, train loss: 3.4819, lr: 0.000100, spent: 2.2 secs\n",
      "epoch: 8, test auc: 0.908738\n",
      "epoch: 8, train loss: 3.3264, lr: 0.000100, spent: 2.5 secs\n",
      "epoch: 9, test auc: 0.909413\n",
      "epoch: 9, train loss: 3.0751, lr: 0.000100, spent: 2.7 secs\n",
      "epoch: 10, test auc: 0.905196\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 10, train loss: 2.9931, lr: 0.000100, spent: 3.1 secs\n",
      "epoch: 11, test auc: 0.907895\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 11, train loss: 2.9708, lr: 0.000100, spent: 3.3 secs\n",
      "epoch: 12, test auc: 0.907895\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 12, train loss: 2.9550, lr: 0.000100, spent: 3.6 secs\n",
      "epoch: 13, test auc: 0.909750\n",
      "epoch: 13, train loss: 2.9091, lr: 0.000100, spent: 3.8 secs\n",
      "epoch: 14, test auc: 0.906545\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 14, train loss: 2.8992, lr: 0.000100, spent: 4.0 secs\n",
      "epoch: 15, test auc: 0.907051\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 15, train loss: 2.9972, lr: 0.000100, spent: 4.2 secs\n",
      "epoch: 16, test auc: 0.908232\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 16, train loss: 2.8993, lr: 0.000100, spent: 4.5 secs\n",
      "epoch: 17, test auc: 0.910425\n",
      "epoch: 17, train loss: 2.8811, lr: 0.000100, spent: 4.7 secs\n",
      "epoch: 18, test auc: 0.907557\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch: 18, train loss: 2.8762, lr: 0.000100, spent: 5.1 secs\n",
      "epoch: 19, test auc: 0.905533\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch: 19, train loss: 2.9049, lr: 0.000100, spent: 5.3 secs\n",
      "epoch: 20, test auc: 0.906545\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch: 20, train loss: 2.8494, lr: 0.000100, spent: 5.6 secs\n",
      "epoch: 21, test auc: 0.904521\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch: 21, train loss: 2.8877, lr: 0.000100, spent: 5.8 secs\n",
      "epoch: 22, test auc: 0.904015\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch: 22, train loss: 2.8228, lr: 0.000100, spent: 6.0 secs\n",
      "epoch: 23, test auc: 0.903171\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch: 23, train loss: 2.8332, lr: 0.000100, spent: 6.2 secs\n",
      "epoch: 24, test auc: 0.901147\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch: 24, train loss: 2.8269, lr: 0.000100, spent: 6.4 secs\n",
      "epoch: 25, test auc: 0.900641\n",
      "EarlyStopping counter: 8 out of 10\n",
      "epoch: 25, train loss: 2.8467, lr: 0.000100, spent: 6.8 secs\n",
      "epoch: 26, test auc: 0.898111\n",
      "EarlyStopping counter: 9 out of 10\n",
      "epoch: 26, train loss: 2.8849, lr: 0.000100, spent: 7.0 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:25:16.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:25:16.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27, test auc: 0.896930\n",
      "EarlyStopping counter: 10 out of 10\n",
      "early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:25:16.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 7.5 secs.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5124209b594f2ab46ea5d282ec618f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.683190\n",
      "epoch: 0, train loss: 4.7738, lr: 0.000050, spent: 0.3 secs\n",
      "epoch: 1, test auc: 0.774038\n",
      "epoch: 1, train loss: 4.6882, lr: 0.000050, spent: 0.5 secs\n",
      "epoch: 2, test auc: 0.783156\n",
      "epoch: 2, train loss: 4.6167, lr: 0.000050, spent: 0.8 secs\n",
      "epoch: 3, test auc: 0.793435\n",
      "epoch: 3, train loss: 4.5302, lr: 0.000050, spent: 1.0 secs\n",
      "epoch: 4, test auc: 0.817308\n",
      "epoch: 4, train loss: 4.4277, lr: 0.000050, spent: 1.5 secs\n",
      "epoch: 5, test auc: 0.821618\n",
      "epoch: 5, train loss: 4.3208, lr: 0.000050, spent: 1.7 secs\n",
      "epoch: 6, test auc: 0.837367\n",
      "epoch: 6, train loss: 4.1911, lr: 0.000050, spent: 2.0 secs\n",
      "epoch: 7, test auc: 0.848972\n",
      "epoch: 7, train loss: 4.0434, lr: 0.000050, spent: 2.2 secs\n",
      "epoch: 8, test auc: 0.862401\n",
      "epoch: 8, train loss: 3.8546, lr: 0.000050, spent: 2.4 secs\n",
      "epoch: 9, test auc: 0.863893\n",
      "epoch: 9, train loss: 3.6178, lr: 0.000050, spent: 2.7 secs\n",
      "epoch: 10, test auc: 0.871187\n",
      "epoch: 10, train loss: 3.3727, lr: 0.000050, spent: 2.9 secs\n",
      "epoch: 11, test auc: 0.878481\n",
      "epoch: 11, train loss: 3.1519, lr: 0.000050, spent: 3.4 secs\n",
      "epoch: 12, test auc: 0.877321\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 12, train loss: 2.9997, lr: 0.000050, spent: 3.6 secs\n",
      "epoch: 13, test auc: 0.877487\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 13, train loss: 2.9092, lr: 0.000050, spent: 3.8 secs\n",
      "epoch: 14, test auc: 0.877487\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 14, train loss: 2.8410, lr: 0.000050, spent: 4.0 secs\n",
      "epoch: 15, test auc: 0.877321\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 15, train loss: 2.8846, lr: 0.000050, spent: 4.2 secs\n",
      "epoch: 16, test auc: 0.876824\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 16, train loss: 2.8548, lr: 0.000050, spent: 4.5 secs\n",
      "epoch: 17, test auc: 0.876989\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 17, train loss: 2.8125, lr: 0.000050, spent: 4.7 secs\n",
      "epoch: 18, test auc: 0.875663\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 18, train loss: 2.8476, lr: 0.000050, spent: 5.1 secs\n",
      "epoch: 19, test auc: 0.873176\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 19, train loss: 2.7911, lr: 0.000050, spent: 5.3 secs\n",
      "epoch: 20, test auc: 0.874668\n",
      "EarlyStopping counter: 9 out of 15\n",
      "epoch: 20, train loss: 2.7958, lr: 0.000050, spent: 5.5 secs\n",
      "epoch: 21, test auc: 0.873674\n",
      "EarlyStopping counter: 10 out of 15\n",
      "epoch: 21, train loss: 2.7814, lr: 0.000050, spent: 5.7 secs\n",
      "epoch: 22, test auc: 0.875829\n",
      "EarlyStopping counter: 11 out of 15\n",
      "epoch: 22, train loss: 2.7948, lr: 0.000050, spent: 5.9 secs\n",
      "epoch: 23, test auc: 0.873342\n",
      "EarlyStopping counter: 12 out of 15\n",
      "epoch: 23, train loss: 2.7841, lr: 0.000050, spent: 6.1 secs\n",
      "epoch: 24, test auc: 0.870027\n",
      "EarlyStopping counter: 13 out of 15\n",
      "epoch: 24, train loss: 2.7357, lr: 0.000050, spent: 6.3 secs\n",
      "epoch: 25, test auc: 0.872679\n",
      "EarlyStopping counter: 14 out of 15\n",
      "epoch: 25, train loss: 2.7837, lr: 0.000050, spent: 6.6 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:25:23.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:25:23.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26, test auc: 0.870027\n",
      "EarlyStopping counter: 15 out of 15\n",
      "early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:25:23.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 7.2 secs.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce661d999074e9b87da53d6afd0866e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.761439\n",
      "epoch: 0, train loss: 4.8939, lr: 0.000050, spent: 0.3 secs\n",
      "epoch: 1, test auc: 0.752653\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 1, train loss: 4.7829, lr: 0.000050, spent: 0.5 secs\n",
      "epoch: 2, test auc: 0.745524\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 2, train loss: 4.6576, lr: 0.000050, spent: 0.7 secs\n",
      "epoch: 3, test auc: 0.739887\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 3, train loss: 4.5437, lr: 0.000050, spent: 0.9 secs\n",
      "epoch: 4, test auc: 0.744529\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 4, train loss: 4.4388, lr: 0.000050, spent: 1.1 secs\n",
      "epoch: 5, test auc: 0.746187\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 5, train loss: 4.3175, lr: 0.000050, spent: 1.3 secs\n",
      "epoch: 6, test auc: 0.754310\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 6, train loss: 4.2194, lr: 0.000050, spent: 1.7 secs\n",
      "epoch: 7, test auc: 0.753813\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 7, train loss: 4.0824, lr: 0.000050, spent: 1.9 secs\n",
      "epoch: 8, test auc: 0.754973\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 8, train loss: 3.9406, lr: 0.000050, spent: 2.2 secs\n",
      "epoch: 9, test auc: 0.751824\n",
      "EarlyStopping counter: 9 out of 15\n",
      "epoch: 9, train loss: 3.7487, lr: 0.000050, spent: 2.4 secs\n",
      "epoch: 10, test auc: 0.789788\n",
      "epoch: 10, train loss: 3.5624, lr: 0.000050, spent: 2.6 secs\n",
      "epoch: 11, test auc: 0.809350\n",
      "epoch: 11, train loss: 3.3825, lr: 0.000050, spent: 2.9 secs\n",
      "epoch: 12, test auc: 0.810511\n",
      "epoch: 12, train loss: 3.1713, lr: 0.000050, spent: 3.1 secs\n",
      "epoch: 13, test auc: 0.827089\n",
      "epoch: 13, train loss: 3.1055, lr: 0.000050, spent: 3.6 secs\n",
      "epoch: 14, test auc: 0.828415\n",
      "epoch: 14, train loss: 2.9805, lr: 0.000050, spent: 3.8 secs\n",
      "epoch: 15, test auc: 0.835544\n",
      "epoch: 15, train loss: 2.8852, lr: 0.000050, spent: 4.1 secs\n",
      "epoch: 16, test auc: 0.835875\n",
      "epoch: 16, train loss: 2.8372, lr: 0.000050, spent: 4.3 secs\n",
      "epoch: 17, test auc: 0.830736\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 17, train loss: 2.7752, lr: 0.000050, spent: 4.6 secs\n",
      "epoch: 18, test auc: 0.829244\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 18, train loss: 2.8560, lr: 0.000050, spent: 4.8 secs\n",
      "epoch: 19, test auc: 0.832725\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 19, train loss: 2.7560, lr: 0.000050, spent: 5.0 secs\n",
      "epoch: 20, test auc: 0.828415\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 20, train loss: 2.7734, lr: 0.000050, spent: 5.2 secs\n",
      "epoch: 21, test auc: 0.828415\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 21, train loss: 2.7259, lr: 0.000050, spent: 5.6 secs\n",
      "epoch: 22, test auc: 0.832891\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 22, train loss: 2.6837, lr: 0.000050, spent: 5.8 secs\n",
      "epoch: 23, test auc: 0.834218\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 23, train loss: 2.7496, lr: 0.000050, spent: 6.0 secs\n",
      "epoch: 24, test auc: 0.832394\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 24, train loss: 2.7409, lr: 0.000050, spent: 6.2 secs\n",
      "epoch: 25, test auc: 0.825099\n",
      "EarlyStopping counter: 9 out of 15\n",
      "epoch: 25, train loss: 2.6772, lr: 0.000050, spent: 6.5 secs\n",
      "epoch: 26, test auc: 0.830736\n",
      "EarlyStopping counter: 10 out of 15\n",
      "epoch: 26, train loss: 2.6986, lr: 0.000050, spent: 6.7 secs\n",
      "epoch: 27, test auc: 0.829576\n",
      "EarlyStopping counter: 11 out of 15\n",
      "epoch: 27, train loss: 2.7260, lr: 0.000050, spent: 6.9 secs\n",
      "epoch: 28, test auc: 0.834383\n",
      "EarlyStopping counter: 12 out of 15\n",
      "epoch: 28, train loss: 2.7810, lr: 0.000050, spent: 7.3 secs\n",
      "epoch: 29, test auc: 0.832228\n",
      "EarlyStopping counter: 13 out of 15\n",
      "epoch: 29, train loss: 2.6869, lr: 0.000050, spent: 7.5 secs\n",
      "epoch: 30, test auc: 0.835378\n",
      "EarlyStopping counter: 14 out of 15\n",
      "epoch: 30, train loss: 2.7133, lr: 0.000050, spent: 7.7 secs\n",
      "epoch: 31, test auc: 0.835875\n",
      "epoch: 31, train loss: 2.7144, lr: 0.000050, spent: 8.0 secs\n",
      "epoch: 32, test auc: 0.834549\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 32, train loss: 2.6951, lr: 0.000050, spent: 8.2 secs\n",
      "epoch: 33, test auc: 0.834383\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 33, train loss: 2.6879, lr: 0.000050, spent: 8.4 secs\n",
      "epoch: 34, test auc: 0.831068\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 34, train loss: 2.6830, lr: 0.000050, spent: 8.6 secs\n",
      "epoch: 35, test auc: 0.832394\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 35, train loss: 2.6643, lr: 0.000050, spent: 9.1 secs\n",
      "epoch: 36, test auc: 0.829078\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 36, train loss: 2.6652, lr: 0.000050, spent: 9.3 secs\n",
      "epoch: 37, test auc: 0.831565\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 37, train loss: 2.6678, lr: 0.000050, spent: 9.5 secs\n",
      "epoch: 38, test auc: 0.834052\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 38, train loss: 2.7045, lr: 0.000050, spent: 9.7 secs\n",
      "epoch: 39, test auc: 0.834715\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 39, train loss: 2.7065, lr: 0.000050, spent: 9.9 secs\n",
      "epoch: 40, test auc: 0.830902\n",
      "EarlyStopping counter: 9 out of 15\n",
      "epoch: 40, train loss: 2.6875, lr: 0.000050, spent: 10.1 secs\n",
      "epoch: 41, test auc: 0.833554\n",
      "EarlyStopping counter: 10 out of 15\n",
      "epoch: 41, train loss: 2.6623, lr: 0.000050, spent: 10.4 secs\n",
      "epoch: 42, test auc: 0.834881\n",
      "EarlyStopping counter: 11 out of 15\n",
      "epoch: 42, train loss: 2.6794, lr: 0.000050, spent: 10.8 secs\n",
      "epoch: 43, test auc: 0.834383\n",
      "EarlyStopping counter: 12 out of 15\n",
      "epoch: 43, train loss: 2.6616, lr: 0.000050, spent: 11.0 secs\n",
      "epoch: 44, test auc: 0.831565\n",
      "EarlyStopping counter: 13 out of 15\n",
      "epoch: 44, train loss: 2.6801, lr: 0.000050, spent: 11.2 secs\n",
      "epoch: 45, test auc: 0.833057\n",
      "EarlyStopping counter: 14 out of 15\n",
      "epoch: 45, train loss: 2.7263, lr: 0.000050, spent: 11.4 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:25:35.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:25:35.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46, test auc: 0.831897\n",
      "EarlyStopping counter: 15 out of 15\n",
      "early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:25:36.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 11.9 secs.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c098ec5bf58482a813522c79754a4d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.702089\n",
      "epoch: 0, train loss: 4.8416, lr: 0.000050, spent: 0.3 secs\n",
      "epoch: 1, test auc: 0.806366\n",
      "epoch: 1, train loss: 4.7431, lr: 0.000050, spent: 0.5 secs\n",
      "epoch: 2, test auc: 0.820623\n",
      "epoch: 2, train loss: 4.6606, lr: 0.000050, spent: 0.8 secs\n",
      "epoch: 3, test auc: 0.836704\n",
      "epoch: 3, train loss: 4.5751, lr: 0.000050, spent: 1.2 secs\n",
      "epoch: 4, test auc: 0.836870\n",
      "epoch: 4, train loss: 4.4711, lr: 0.000050, spent: 1.5 secs\n",
      "epoch: 5, test auc: 0.842507\n",
      "epoch: 5, train loss: 4.3606, lr: 0.000050, spent: 1.7 secs\n",
      "epoch: 6, test auc: 0.852619\n",
      "epoch: 6, train loss: 4.2036, lr: 0.000050, spent: 1.9 secs\n",
      "epoch: 7, test auc: 0.877653\n",
      "epoch: 7, train loss: 4.0036, lr: 0.000050, spent: 2.2 secs\n",
      "epoch: 8, test auc: 0.888097\n",
      "epoch: 8, train loss: 3.7528, lr: 0.000050, spent: 2.5 secs\n",
      "epoch: 9, test auc: 0.893402\n",
      "epoch: 9, train loss: 3.5275, lr: 0.000050, spent: 2.7 secs\n",
      "epoch: 10, test auc: 0.893568\n",
      "epoch: 10, train loss: 3.3194, lr: 0.000050, spent: 2.9 secs\n",
      "epoch: 11, test auc: 0.893236\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 11, train loss: 3.1563, lr: 0.000050, spent: 3.4 secs\n",
      "epoch: 12, test auc: 0.896054\n",
      "epoch: 12, train loss: 3.0188, lr: 0.000050, spent: 3.6 secs\n",
      "epoch: 13, test auc: 0.896552\n",
      "epoch: 13, train loss: 2.9761, lr: 0.000050, spent: 3.8 secs\n",
      "epoch: 14, test auc: 0.898375\n",
      "epoch: 14, train loss: 2.9275, lr: 0.000050, spent: 4.1 secs\n",
      "epoch: 15, test auc: 0.896552\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 15, train loss: 2.8988, lr: 0.000050, spent: 4.3 secs\n",
      "epoch: 16, test auc: 0.895557\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 16, train loss: 2.9570, lr: 0.000050, spent: 4.5 secs\n",
      "epoch: 17, test auc: 0.896386\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 17, train loss: 2.8690, lr: 0.000050, spent: 4.9 secs\n",
      "epoch: 18, test auc: 0.895391\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 18, train loss: 2.8708, lr: 0.000050, spent: 5.1 secs\n",
      "epoch: 19, test auc: 0.894397\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 19, train loss: 2.8728, lr: 0.000050, spent: 5.4 secs\n",
      "epoch: 20, test auc: 0.897878\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 20, train loss: 2.8242, lr: 0.000050, spent: 5.6 secs\n",
      "epoch: 21, test auc: 0.897381\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 21, train loss: 2.8534, lr: 0.000050, spent: 5.8 secs\n",
      "epoch: 22, test auc: 0.898873\n",
      "epoch: 22, train loss: 2.8766, lr: 0.000050, spent: 6.0 secs\n",
      "epoch: 23, test auc: 0.895391\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 23, train loss: 2.8227, lr: 0.000050, spent: 6.3 secs\n",
      "epoch: 24, test auc: 0.895391\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 24, train loss: 2.8632, lr: 0.000050, spent: 6.5 secs\n",
      "epoch: 25, test auc: 0.898210\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 25, train loss: 2.8086, lr: 0.000050, spent: 6.9 secs\n",
      "epoch: 26, test auc: 0.898044\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 26, train loss: 2.8102, lr: 0.000050, spent: 7.1 secs\n",
      "epoch: 27, test auc: 0.896054\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 27, train loss: 2.8009, lr: 0.000050, spent: 7.3 secs\n",
      "epoch: 28, test auc: 0.897712\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 28, train loss: 2.8240, lr: 0.000050, spent: 7.5 secs\n",
      "epoch: 29, test auc: 0.894894\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 29, train loss: 2.7950, lr: 0.000050, spent: 7.7 secs\n",
      "epoch: 30, test auc: 0.896220\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 30, train loss: 2.8338, lr: 0.000050, spent: 8.0 secs\n",
      "epoch: 31, test auc: 0.893899\n",
      "EarlyStopping counter: 9 out of 15\n",
      "epoch: 31, train loss: 2.8173, lr: 0.000050, spent: 8.2 secs\n",
      "epoch: 32, test auc: 0.897712\n",
      "EarlyStopping counter: 10 out of 15\n",
      "epoch: 32, train loss: 2.7890, lr: 0.000050, spent: 8.6 secs\n",
      "epoch: 33, test auc: 0.895557\n",
      "EarlyStopping counter: 11 out of 15\n",
      "epoch: 33, train loss: 2.8038, lr: 0.000050, spent: 8.8 secs\n",
      "epoch: 34, test auc: 0.897712\n",
      "EarlyStopping counter: 12 out of 15\n",
      "epoch: 34, train loss: 2.8294, lr: 0.000050, spent: 9.0 secs\n",
      "epoch: 35, test auc: 0.896718\n",
      "EarlyStopping counter: 13 out of 15\n",
      "epoch: 35, train loss: 2.7783, lr: 0.000050, spent: 9.2 secs\n",
      "epoch: 36, test auc: 0.896552\n",
      "EarlyStopping counter: 14 out of 15\n",
      "epoch: 36, train loss: 2.7852, lr: 0.000050, spent: 9.4 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:25:45.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:25:45.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37, test auc: 0.895391\n",
      "EarlyStopping counter: 15 out of 15\n",
      "early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:25:46.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 9.9 secs.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ccbb311ca243f79eda0888b2d7df3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.872890\n",
      "epoch: 0, train loss: 4.8115, lr: 0.000050, spent: 0.3 secs\n",
      "epoch: 1, test auc: 0.863787\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 1, train loss: 4.7619, lr: 0.000050, spent: 0.5 secs\n",
      "epoch: 2, test auc: 0.822410\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 2, train loss: 4.6174, lr: 0.000050, spent: 0.9 secs\n",
      "epoch: 3, test auc: 0.822079\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 3, train loss: 4.5146, lr: 0.000050, spent: 1.1 secs\n",
      "epoch: 4, test auc: 0.819762\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 4, train loss: 4.4108, lr: 0.000050, spent: 1.3 secs\n",
      "epoch: 5, test auc: 0.819596\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 5, train loss: 4.2894, lr: 0.000050, spent: 1.5 secs\n",
      "epoch: 6, test auc: 0.823734\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 6, train loss: 4.1289, lr: 0.000050, spent: 1.7 secs\n",
      "epoch: 7, test auc: 0.830023\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 7, train loss: 3.9752, lr: 0.000050, spent: 1.9 secs\n",
      "epoch: 8, test auc: 0.840947\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 8, train loss: 3.7989, lr: 0.000050, spent: 2.2 secs\n",
      "epoch: 9, test auc: 0.853525\n",
      "EarlyStopping counter: 9 out of 15\n",
      "epoch: 9, train loss: 3.6224, lr: 0.000050, spent: 2.6 secs\n",
      "epoch: 10, test auc: 0.864283\n",
      "EarlyStopping counter: 10 out of 15\n",
      "epoch: 10, train loss: 3.4694, lr: 0.000050, spent: 2.8 secs\n",
      "epoch: 11, test auc: 0.863952\n",
      "EarlyStopping counter: 11 out of 15\n",
      "epoch: 11, train loss: 3.2931, lr: 0.000050, spent: 3.0 secs\n",
      "epoch: 12, test auc: 0.868587\n",
      "EarlyStopping counter: 12 out of 15\n",
      "epoch: 12, train loss: 3.1800, lr: 0.000050, spent: 3.2 secs\n",
      "epoch: 13, test auc: 0.870076\n",
      "EarlyStopping counter: 13 out of 15\n",
      "epoch: 13, train loss: 3.1267, lr: 0.000050, spent: 3.4 secs\n",
      "epoch: 14, test auc: 0.870738\n",
      "EarlyStopping counter: 14 out of 15\n",
      "epoch: 14, train loss: 3.0394, lr: 0.000050, spent: 3.6 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:25:50.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:25:50.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, test auc: 0.871897\n",
      "EarlyStopping counter: 15 out of 15\n",
      "early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:25:50.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 4.1 secs.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836551c450c5426dae4016aab9668189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test auc: 0.562922\n",
      "epoch: 0, train loss: 5.1729, lr: 0.000050, spent: 0.4 secs\n",
      "epoch: 1, test auc: 0.763664\n",
      "epoch: 1, train loss: 4.8613, lr: 0.000050, spent: 0.7 secs\n",
      "epoch: 2, test auc: 0.825911\n",
      "epoch: 2, train loss: 4.7430, lr: 0.000050, spent: 0.9 secs\n",
      "epoch: 3, test auc: 0.838900\n",
      "epoch: 3, train loss: 4.6310, lr: 0.000050, spent: 1.2 secs\n",
      "epoch: 4, test auc: 0.839912\n",
      "epoch: 4, train loss: 4.5405, lr: 0.000050, spent: 1.5 secs\n",
      "epoch: 5, test auc: 0.851552\n",
      "epoch: 5, train loss: 4.4316, lr: 0.000050, spent: 1.7 secs\n",
      "epoch: 6, test auc: 0.854926\n",
      "epoch: 6, train loss: 4.3167, lr: 0.000050, spent: 2.0 secs\n",
      "epoch: 7, test auc: 0.869433\n",
      "epoch: 7, train loss: 4.1655, lr: 0.000050, spent: 2.4 secs\n",
      "epoch: 8, test auc: 0.884278\n",
      "epoch: 8, train loss: 3.9868, lr: 0.000050, spent: 2.6 secs\n",
      "epoch: 9, test auc: 0.894399\n",
      "epoch: 9, train loss: 3.7628, lr: 0.000050, spent: 2.9 secs\n",
      "epoch: 10, test auc: 0.902328\n",
      "epoch: 10, train loss: 3.5238, lr: 0.000050, spent: 3.1 secs\n",
      "epoch: 11, test auc: 0.905702\n",
      "epoch: 11, train loss: 3.3160, lr: 0.000050, spent: 3.4 secs\n",
      "epoch: 12, test auc: 0.905364\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 12, train loss: 3.1651, lr: 0.000050, spent: 3.6 secs\n",
      "epoch: 13, test auc: 0.905702\n",
      "epoch: 13, train loss: 3.0708, lr: 0.000050, spent: 3.8 secs\n",
      "epoch: 14, test auc: 0.904352\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 14, train loss: 3.0111, lr: 0.000050, spent: 4.1 secs\n",
      "epoch: 15, test auc: 0.907557\n",
      "epoch: 15, train loss: 2.9683, lr: 0.000050, spent: 4.5 secs\n",
      "epoch: 16, test auc: 0.904858\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 16, train loss: 2.9622, lr: 0.000050, spent: 4.7 secs\n",
      "epoch: 17, test auc: 0.906377\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 17, train loss: 2.9567, lr: 0.000050, spent: 4.9 secs\n",
      "epoch: 18, test auc: 0.904858\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 18, train loss: 2.9323, lr: 0.000050, spent: 5.2 secs\n",
      "epoch: 19, test auc: 0.907051\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 19, train loss: 2.9173, lr: 0.000050, spent: 5.4 secs\n",
      "epoch: 20, test auc: 0.905364\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 20, train loss: 2.9343, lr: 0.000050, spent: 5.6 secs\n",
      "epoch: 21, test auc: 0.904352\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 21, train loss: 2.9226, lr: 0.000050, spent: 5.8 secs\n",
      "epoch: 22, test auc: 0.903677\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 22, train loss: 2.8993, lr: 0.000050, spent: 6.2 secs\n",
      "epoch: 23, test auc: 0.905870\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 23, train loss: 2.8830, lr: 0.000050, spent: 6.4 secs\n",
      "epoch: 24, test auc: 0.905196\n",
      "EarlyStopping counter: 9 out of 15\n",
      "epoch: 24, train loss: 2.9245, lr: 0.000050, spent: 6.6 secs\n",
      "epoch: 25, test auc: 0.905027\n",
      "EarlyStopping counter: 10 out of 15\n",
      "epoch: 25, train loss: 2.8724, lr: 0.000050, spent: 6.8 secs\n",
      "epoch: 26, test auc: 0.907895\n",
      "epoch: 26, train loss: 2.8843, lr: 0.000050, spent: 7.1 secs\n",
      "epoch: 27, test auc: 0.907557\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 27, train loss: 2.8650, lr: 0.000050, spent: 7.3 secs\n",
      "epoch: 28, test auc: 0.907220\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 28, train loss: 2.8518, lr: 0.000050, spent: 7.5 secs\n",
      "epoch: 29, test auc: 0.908738\n",
      "epoch: 29, train loss: 2.8588, lr: 0.000050, spent: 8.0 secs\n",
      "epoch: 30, test auc: 0.906883\n",
      "EarlyStopping counter: 1 out of 15\n",
      "epoch: 30, train loss: 2.8900, lr: 0.000050, spent: 8.2 secs\n",
      "epoch: 31, test auc: 0.905027\n",
      "EarlyStopping counter: 2 out of 15\n",
      "epoch: 31, train loss: 2.8514, lr: 0.000050, spent: 8.4 secs\n",
      "epoch: 32, test auc: 0.904352\n",
      "EarlyStopping counter: 3 out of 15\n",
      "epoch: 32, train loss: 2.8178, lr: 0.000050, spent: 8.6 secs\n",
      "epoch: 33, test auc: 0.906883\n",
      "EarlyStopping counter: 4 out of 15\n",
      "epoch: 33, train loss: 2.8274, lr: 0.000050, spent: 8.8 secs\n",
      "epoch: 34, test auc: 0.905364\n",
      "EarlyStopping counter: 5 out of 15\n",
      "epoch: 34, train loss: 2.8441, lr: 0.000050, spent: 9.0 secs\n",
      "epoch: 35, test auc: 0.901484\n",
      "EarlyStopping counter: 6 out of 15\n",
      "epoch: 35, train loss: 2.8212, lr: 0.000050, spent: 9.3 secs\n",
      "epoch: 36, test auc: 0.903340\n",
      "EarlyStopping counter: 7 out of 15\n",
      "epoch: 36, train loss: 2.8100, lr: 0.000050, spent: 9.7 secs\n",
      "epoch: 37, test auc: 0.900641\n",
      "EarlyStopping counter: 8 out of 15\n",
      "epoch: 37, train loss: 2.7932, lr: 0.000050, spent: 9.9 secs\n",
      "epoch: 38, test auc: 0.902159\n",
      "EarlyStopping counter: 9 out of 15\n",
      "epoch: 38, train loss: 2.7938, lr: 0.000050, spent: 10.1 secs\n",
      "epoch: 39, test auc: 0.900304\n",
      "EarlyStopping counter: 10 out of 15\n",
      "epoch: 39, train loss: 2.7888, lr: 0.000050, spent: 10.3 secs\n",
      "epoch: 40, test auc: 0.899291\n",
      "EarlyStopping counter: 11 out of 15\n",
      "epoch: 40, train loss: 2.7911, lr: 0.000050, spent: 10.5 secs\n",
      "epoch: 41, test auc: 0.901147\n",
      "EarlyStopping counter: 12 out of 15\n",
      "epoch: 41, train loss: 2.7945, lr: 0.000050, spent: 10.7 secs\n",
      "epoch: 42, test auc: 0.900810\n",
      "EarlyStopping counter: 13 out of 15\n",
      "epoch: 42, train loss: 2.7820, lr: 0.000050, spent: 10.9 secs\n",
      "epoch: 43, test auc: 0.898111\n",
      "EarlyStopping counter: 14 out of 15\n",
      "epoch: 43, train loss: 2.8453, lr: 0.000050, spent: 11.2 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:26:01.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mload best at last from ./ckpt\u001b[0m\n",
      "\u001b[32m2025-10-16 20:26:02.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1msaving model checkpoint to ./ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 44, test auc: 0.893050\n",
      "EarlyStopping counter: 15 out of 15\n",
      "early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-16 20:26:02.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtranstab.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mtraining complete, cost 11.8 secs.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## -- Perform Transfer Learning on the second pair of datasets\n",
    "PreTrain2 = cv_transtab_5fold(d_X3, d_y3, cat_cols3, num_cols3, bin_cols3,\n",
    "                              file_name='pretrain_checkpoint2',save_ckpt=True)\n",
    "TransferLearning2 = cv_transtab_5fold(d_X4, d_y4, cat_cols4, num_cols4, bin_cols4,\n",
    "                                      transfer=True, file_name='pretrain_checkpoint2')\n",
    "Baseline2 = cv_transtab_5fold(d_X4, d_y4, cat_cols4, num_cols4, bin_cols4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "RTi3KBwqLHBw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTi3KBwqLHBw",
    "outputId": "487fc71d-cc59-442f-b5d7-7b88dc2456c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTransfer Learning vs Baseline for First Pair\n",
      "\n",
      " Metric    Transfer (±)    Baseline (±)        Improvement      \n",
      "Accuracy 0.8173 ± 0.0492 0.8039 ± 0.0317 +0.0134  (Δstd +0.0175)\n",
      "F1-score 0.7901 ± 0.0633 0.7826 ± 0.0470 +0.0075  (Δstd +0.0163)\n",
      "     AUC 0.9015 ± 0.0370 0.8931 ± 0.0453 +0.0083  (Δstd -0.0083)\n",
      "\n",
      "\tTransfer Learning vs Baseline for Second Pair\n",
      "\n",
      " Metric    Transfer (±)    Baseline (±)        Improvement      \n",
      "Accuracy 0.8033 ± 0.0278 0.7366 ± 0.1324 +0.0667  (Δstd -0.1045)\n",
      "F1-score 0.8273 ± 0.0242 0.7817 ± 0.0777 +0.0456  (Δstd -0.0536)\n",
      "     AUC 0.8832 ± 0.0282 0.8809 ± 0.0278 +0.0022  (Δstd +0.0004)\n"
     ]
    }
   ],
   "source": [
    "## -- Preview results of TransTab\n",
    "def compare_results(transfer, baseline, title=\"Comparison\"):\n",
    "    import pandas as pd\n",
    "\n",
    "    mean_metrics = [\"acc_mean\", \"f1_mean\", \"auc_mean\"]\n",
    "    std_metrics  = [\"acc_std\",  \"f1_std\",  \"auc_std\"]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"Metric\": [\"Accuracy\", \"F1-score\", \"AUC\"],\n",
    "        \"Transfer_mean\": [transfer[m] for m in mean_metrics],\n",
    "        \"Transfer_std\":  [transfer[m] for m in std_metrics],\n",
    "        \"Baseline_mean\": [baseline[m] for m in mean_metrics],\n",
    "        \"Baseline_std\":  [baseline[m] for m in std_metrics],\n",
    "    })\n",
    "\n",
    "    df[\"Δmean\"] = df[\"Transfer_mean\"] - df[\"Baseline_mean\"]\n",
    "    df[\"Δstd\"]  = df[\"Transfer_std\"]  - df[\"Baseline_std\"]\n",
    "\n",
    "    df[\"Transfer (±)\"] = df[\"Transfer_mean\"].map(\"{:.4f}\".format) + \" ± \" + df[\"Transfer_std\"].map(\"{:.4f}\".format)\n",
    "    df[\"Baseline (±)\"] = df[\"Baseline_mean\"].map(\"{:.4f}\".format) + \" ± \" + df[\"Baseline_std\"].map(\"{:.4f}\".format)\n",
    "    df[\"Improvement\"]  = df[\"Δmean\"].map(\"{:+.4f}\".format) + \"  (Δstd \" + df[\"Δstd\"].map(\"{:+.4f}\".format) + \")\"\n",
    "\n",
    "    with pd.option_context(\"display.precision\", 4, \"display.colheader_justify\", \"center\"):\n",
    "        print(f\"\\n{title}\\n\")\n",
    "        print(df[[\"Metric\", \"Transfer (±)\", \"Baseline (±)\", \"Improvement\"]].to_string(index=False, justify=\"center\"))\n",
    "\n",
    "compare_results(TransferLearning1, Baseline1, title=\"\\tTransfer Learning vs Baseline for First Pair\")\n",
    "compare_results(TransferLearning2, Baseline2, title=\"\\tTransfer Learning vs Baseline for Second Pair\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce73cd0-92ef-4eea-8996-f0cb71a83e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
